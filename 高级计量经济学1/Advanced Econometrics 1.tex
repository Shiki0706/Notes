%!TEX program = xelatex
%!BIB program = biber
\documentclass[cn, 12pt, math=mtpro2, bibstyle=apa, blue, twocol]{elegantbook}
\title{高级计量经济学1}
\subtitle{Advanced Econometrics 1}

\setcounter{tocdepth}{3}
\cover{cover.png}

\addbibresource[location=local]{reference.bib}

\usepackage{cprotect}
\usepackage{array}
\usepackage{bbm}
\usepackage{pgfplots}
\pgfplotsset{compat = newest}
\usetikzlibrary{positioning, arrows.meta}
\usepgfplotslibrary{fillbetween}
\newcommand{\ccr}[1]{\makecell{{\color{#1}\rule{1cm}{1cm}}}}

\definecolor{customcolor}{RGB}{230,230,250}
\colorlet{coverlinecolor}{customcolor}

\newcommand{\F}{\mathscr{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\RH}{\mathbold{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\var}{\text{var}}
\newcommand{\MSE}{\text{MSE}}
\newcommand{\Q}{\mathbold{Q}}
\newcommand{\W}{\mathbold{W}}
\newcommand{\X}{\mathbold{X}}
\newcommand{\Z}{\mathbold{Z}}
\newcommand{\hb}{\hat{\beta}}
\newcommand{\M}{\mathbold{M}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\V}{\mathbold{V}}
\newcommand{\I}{\mathbold{I}}
\newcommand{\Om}{\mathbold{\Omega}}
\newcommand{\limn}{\lim_{n\to\infty}}
\newcommand{\OX}{\overbar{X}}
\newcommand{\BS}{\mathbold{\Sigma}}
\newcommand{\BO}{\mathbold{\Omega}}
\newcommand{\A}{\mathbold{A}}
\newcommand{\G}{\mathbold{G}}
\newcommand{\hth}{\hat{\theta}}


\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\emergencystretch=4em
\begin{document}

\maketitle
\frontmatter

\tableofcontents

\mainmatter
\chapter{条件期望与投影}
计量经济学中最主要的工具是回归方法, 用来估计在给定回归元时, 响应变量的条件期望. 本章主要讨论一般条件下的回归分析, 重点放在条件期望模型和线性投影.
\section{迭代期望定律}
随机变量$X:\Omega\to\R$为定义在概率空间$(\Omega,\F,\PP)$上的$\F$-可测实值函数, 也即对于任意$a\in\R$都有
$$X^{-1}((-\infty,a])\equiv \{\omega\in\Omega: X(\omega)\leq a\}\in\F$$
$X$具有累积分布函数
$$F(x)=\PP[X\leq x],\quad x\in\R$$
并且$X$的期望为
$$\E[X]=\int_\R x\,\text{d}F(x)$$

特别地, 如果$X$为离散型随机变量, 则
$$\E[X]=\sum_{j=1}^{\infty}\tau_j\PP[X=\tau_j]$$
如果$X$为连续型随机变量, 则$F$为绝对连续函数, $X$具有概率密度函数$f$, 则
$$\E[X]=\int_\R x\,\text{d}F(x)=\int_\R xf(x)\,\text{d}x$$
这里的积分为Lebesgue积分. 更一般地, 如果$h:\R\to\R$为Borel可测函数, $Y=h(X)$且$Y\in L^1(\Omega,\F,\PP)$, 那么
$$\displaystyle\E[Y]=\int_\R h(x)\,\text{d}F(x)$$

不严格地说, 条件期望函数\footnote{条件期望的正式定义为: 设$X:\Omega\to\R$为定义在概率空间$(\Omega,\F,\PP)$上的随机变量, 并且$\E|X|<\infty$, 如果$\mathscr{F}_0\subset\F$是$\F$的子$\sigma$-代数, 则$X$在条件$\mathscr{F}_0$下的条件期望为满足以下条件的随机变量$Y$: (i) $Y$是$\mathscr{F}_0$-可测函数, (ii) 对任意的$A\in\mathscr{F}_0$都有$\E[X\mathbbm{1}_A]=\E[Y\mathbbm{1}_A]$.} (Conditional Expectation Function, CEF):
$$m(x)=\E[Y|X=x]$$
是一个关于$x\in\R^K$的函数, 它表示$X=x$时, 随机变量$Y$的期望. 为了估计$m(x)$在$X$处的值, 有时也将CEF视为随机变量$X$的函数, 记作$m(X)$或$\E[Y|X]$, 此时CEF是随机的.

假定二元有序对$(X,Y)$是定义在乘积空间上的连续型随机变量, 即联合分布函数$F_{XY}$绝对连续, 联合概率密度为$f_{XY}$存在, 那么$X$的边缘概率密度为
$$f_X(x)=\int_\R f_{XY}(x,y)\,\text{d}y$$
给定$X=x$时, $Y$的条件概率密度为
\begin{equation}\label{eq1.1}
  f_{Y|X}(y|x)=\frac{f_{XY}(x,y)}{f_X(x)}
\end{equation}
$f_{Y|X}(y|x)$完全描述了$Y$对$x$的依赖关系. 进一步, 可以将CEF表示为
$$m(x)=\E[Y|X=x]=\int_\R yf_{Y|X}(y|x)\,\text{d}y$$
然而事实上, 即使条件密度(\ref{eq1.1})不是良定义的, 只要$\E|Y|<\infty$成立, 任意随机变量$(X,Y)$的CEF就存在, 并且几乎必然唯一, 它的证明具体可参考Durrett (2019)中的Theorem 4.1.2.

\begin{remark}
条件概率分布反映的仅是$Y$与$X$间的预测关系, 而非因果关系, 换言之它无法刻画$X$的变化将如何导致$Y$变化, 而因果关系的刻画需要根据经济理论.

更具体地说, 如果$X$对$Y$具有因果影响, 那么$X$对$Y$也具有预测能力, 然而$X$可以预测$Y$却并不意味着$X$对$Y$之间存在因果关系.
\end{remark}
\begin{example}
假设$(X,Y)$的联合概率密度为$f_{XY}=\text{e}^{-y}$, $0<x<y<\infty$, 则$X$的边际概率密度为
$$f_X(x)=\int_{-\infty}^{\infty}f_{XY}(x,y)\,\text{d}y=\int_{x}^{\infty}\text{e}^{-y}\,\text{d}y=\text{e}^{-x},\quad x\in(0,\infty)$$
那么对于任意给定的$x>0$都有
$$f_{Y|X}(y|x)=\frac{f_{XY}(x,y)}{f_X(x)}=\text{e}^{-(y-x)},\quad y\in(x,\infty)$$
于是可以计算条件期望
\begin{align*}
\E[Y|X=x]&=\int_{-\infty}^{\infty}yf_{Y|X}(y|x)\,\text{d}y \\
&=\int_{x}^{\infty}x\text{e}^{-(y-x)}\,\text{d}y \\
&=-\text{e}^{x}\int_{x}^{\infty}y\,\text{d}y=1+x
\end{align*}
此时CEF $\E[Y|X]=1+X$是$X$的线性函数.
\end{example}
现在介绍在概率论和计量经济学中极其重要的迭代期望定律 (Law of Iterated Expectation, LIE).
\begin{theorem}[简单迭代期望定律]
  如果对任意随机向量$X$都有$\E|Y|<\infty$, 那么
  $$\E[\E[Y|X]]=\E[Y]$$
\end{theorem}
\begin{proof}
  注意到$f_{Y|X}(y|x)f_X(x)=f(x,y)$, 根据Fubini定理可得
  \begin{align*}
  \E[\E[Y|X]]&=\int_{\R^K}\E[Y|X]f_X(x)\,\text{d}x =\int_{\R^K}\left(\int_\R yf_{Y|X}(y|x)\,\text{d}y\right)f_X(x)\,\text{d}x \\
   &=\int_{\R^K}\left(\int_\R yf(x,y)\,\text{d}y\right)\text{d}x =\int_\R y\left(\int_{\R^K}f(x,y)\,\text{d}x\right)\text{d}y=\E[Y]
  \end{align*}
  由此证得定理.
\end{proof}
简单来说, 随机变量$Y$在$X=x$下的条件期望的期望等于$Y$的无条件期望. 若$X$是离散型的, 则
$$\E[\E[Y|X]]=\sum_{j=1}^{\infty}\E[Y|X=x_j]\PP[X=x_j]$$
若$X$是连续型的, 则
$$\E[\E[Y|X]]=\int_{\R^K}\E[Y|X=x]f_X(x)\,\text{d}x$$
其中$f_X$是$X$的概率密度.

\begin{example}
设$\E[Y|X]=X^2$, $Y\sim U[0,1]$, 现在要求$\E[X]$. 尽管无法找到$X$的概率密度, 但是$\E[X]$可用通过LIE得到
$$\E[Y]=\E[\E[Y|X]]=\int_{0}^{1}x^2\,\text{d}x=\frac{1}{3}$$
\end{example}

现在给出一个条件定理, 它是简单LIE的一种扩展.
\begin{theorem}[条件定理]
  对于任意的Borel可测函数$g:\R^K\to\R$, 如果$\E|Y|<\infty$成立, 那么
  $$\E[g(X)Y|X]=g(X)\E[Y|X]$$
  在此基础上若还有$\E|g(Y)|<\infty$, 那么
  \begin{equation}\label{eq1.5}
    \E[g(X)Y]=\E[g(X)\E[Y|X]]
  \end{equation}
\end{theorem}
\begin{proof}
  根据条件期望的定义可知
  \begin{align*}
  \E[g(X)Y|X=x]&=\int_\R g(x)yf_{Y|X}(y|x)\,\text{d}y \\
  &=g(x)\int_{\R}yf_{Y|X}f(y|x)\,\text{d}y=g(x)\E[Y|X=x]
  \end{align*}
  将上式两端取期望并应用简单LIE即可得到(\ref{eq1.5}).
\end{proof}

最后给出更一般的迭代期望定律, 它允许更多的条件变量存在.
\begin{theorem}[迭代期望定律]
  如果对任意随机向量$X_1$和$X_2$都有$\E|Y|<\infty$, 那么
  $$\E[\E[Y|X_1,X_2]|X_1]=\E[Y|X_1]$$
\end{theorem}
\begin{proof}
  首先注意到
  $$f(y|x_1,x_2)f(x_2|x_1)=\frac{f(y,x_1,x_2)}{f(x_1,x_2)}\frac{f(x_1,x_2)}{f(x_1)}=f(y,x_2|x_1)$$
  以及
  $$\E[Y|X_1=x_1,X_2=x_2]=\int_\R yf(y|x_1,x_2)\,\text{d}y$$
  于是
  \begin{align*}
  \E[\E[Y|X_1,X_2]|X=x_1]&=\int_{\R^{K_2}}\E[Y|X_1=x_1,X_2=x_2]f(x_2|x_1)\,\text{d}x_2 \\
  &=\int_{\R^{K_2}}\left(\int_\R yf(y|x_1,x_2)f(x_2|x_1)\,\text{d}y\right)\text{d}x_2 \\
  &=\int_{\R^{K_2}}\left(\int_\R yf(y,x_2|x_1)\,\text{d}y\right)\text{d}x_2=\E[Y|X_1=x_1]
  \end{align*}
  其中$x_2\in\R^{K_2}$.
\end{proof}

条件期望的符号$\E[Y|X]$的严格意义是$\E[Y|\sigma\langle X\rangle]$, 其中$\sigma\langle X\rangle$表示由$X$生成的$\sigma$-代数, 由于$\sigma\langle X_1\rangle\subset\sigma\langle X_1,X_2\rangle$ (也就是说, 由$X_1$, $X_2$生成的$\sigma$-代数包含的信息比$\sigma\langle X_1\rangle$更多), 因此LIE表明: 当只有$\sigma\langle X_1\rangle$可用时, $\E[Y|X_1,X_2]$的期望不可能从$\sigma\langle X_1,X_2\rangle$包含的信息中得到, 最多只能从$\sigma\langle X_1\rangle$中获得.

最后, 我们给出LIE的本质描述: 设$Y:\Omega\to\R$是定义在概率空间$(\Omega,\F,\PP)$上的随机变量, 并且$\E|Y|<\infty$, 若子$\sigma$-代数$\F_1\subset\F_2\subset\mathscr{F}$, 那么
\begin{equation}\label{eq1.12}
  \E[\E[Y|\F_2]|\F_1]=\E[Y|\mathscr{F}_1]
\end{equation}
若$\mathscr{F}_1=\{\emptyset,\Omega\}$, 则(\ref{eq1.12})变为简单LIE的形式: $\E[\E[Y|\mathscr{F}_1]]=\E[Y]$.


\section{CEF误差}
利用$m(X)$所包含的信息去预测$Y$必然存在一定程度上的偏误, 将其定义为CEF误差 (又称回归误差) $e=Y-m(X)$, 也即$Y$与CEF在$X$处取值之差. 根据CEF误差的构造, 可以将其改写为以下形式
\begin{equation}
  Y=m(X)+e
\end{equation}
这表明$e$的性质来源于$(X,Y)$的联合分布, 我们将$e$的性质表述为以下定理.
\begin{theorem}\label{thm:thm1.3}
  设$X$为任意随机向量, $\E|Y|<\infty$, 则以下命题成立:

  (1) $\E[e|X]=0$, 称为零条件均值.

  (2) $\E[e]=0$.

  (3) 如果对于某个$r\ge1$, $\E|Y|^r<\infty$成立, 那么$\E|e|^r<\infty$.

  (4) 对任意使得$\E|h(X)e|<\infty$的可测函数$h$, $\E[h(X)e]=0$成立.
\end{theorem}
\begin{proof}
  (1) $\E[e|X]=\E[(Y-m(X))|X]=\E[Y|X]-\E[m(X)|X]=m(X)-m(X)=0$.

  (2) $\E[e]=\E[\E[e|X]]=\E[0]=0$.

  (3) 根据Minkowski不等式\footnote{对任意$m\times n$矩阵$X$和$Y$, 都有$(\E||X+Y||^p)^{1/p}\leq (\E||X||^p)^{1/p}+(\E||Y||^p)^{1/p}$.}和条件期望不等式\footnote{对于任意$r\geq1$以及随机变量$(Y,X)\in\R\times\R^K$, 如果$\E|Y|<\infty$, 那么$\E|\E[Y|X]|^r\leq \E|Y|^r<\infty$.}可知
  $$(\E|e|^r)^{1/r}\leq (\E|Y|^r)^{1/r}+(\E|m(X)|^r)^{1/r}<2(\E|Y|^r)^{1/r}<\infty$$

  (4) 根据(1)和LIE可知
  $$\E[h(X)e]=\E[\E[h(X)e|X]]=\E[h(X)\E[e|X]]=\E[0]=0$$
\end{proof}
\begin{remark}
等式$\E[e|X]=\E[e]=0$通常称为均值独立, 表明CEF误差$e$在给定$X=x$时的期望为0且不依赖于$X$, 但这并不代表$e$与$X$独立, 这是一个非常严格的条件, 要求对于任意有界Borel可测函数$h_1:\R^K\to\R$, $h_2:\R\to\R$都有$\E[h_1(X)h_2(e)]=\E[h_1(X)]\E[h_2(e)]$.
\end{remark}
\begin{example}
设$e=Xu$, 其中$X$和$u$独立, 且都服从标准正态分布$N(0,1)$, 那么在条件$X$下$e$的分布为$N(0,X^2)$, 此时$\E[e|X]=0$且$e$不与$X$独立.
\end{example}

现在需要进一步测度CEF函数的对$Y$的偏离程度, 考虑使用CEF误差$e$的无条件方差
$$\sigma^2=\text{var}(e)=\E[(e-\E[e])^2]=\E[e^2]$$
称$\sigma^2$为回归方差或回归误差的方差, 其大小反映了: 在$Y$的变化中, 不能被条件期望$\E[Y|X]$所解释的程度. 根据定理\ref{thm:thm1.3}(3), 如果$\E[Y^2]<\infty$, 那么显然有$\sigma^2<\infty$.

回归误差的大小依赖于回归元$X$, 考虑如下两个回归方程
\begin{align*}
Y&=\E[Y|X_1]+e_1 \\
Y&=\E[Y|X_1,X_2]+e_2
\end{align*}
直观来看, $X_1,X_2$包含的信息比$X_1$的多, 故而$\E[Y|X_1,X_2]$能更多地解释$Y$的变化中的线性部分, 因此理应有$\text{var}(e_2)\leq\text{var}(e_1)$. 现在将其表述为定理.

\begin{theorem}
  设$X_1$和$X_2$为随机向量, 如果$\E[Y^2]<\infty$, 那么
  $$\text{var}(Y)\geq \text{var}(Y-\E[Y|X_1])\geq\text{var}(Y-\E[Y|X_1,X_2])$$
\end{theorem}

\begin{proof}
  根据迭代期望定律$\E[Y|X_1]=\E[\E[Y|X_1,X_2]|X_1]$和条件Jensen不等式\footnote{设$g:\R^L\to\R$为下凸函数, 那么对任意使得$\E||Y||<\infty$和$\E|g(Y)|<\infty$的随机向量$(Y,X)\in\R^L\times\R^K$, 总有$g(\E[Y|X])\leq \E[g(Y)|X]$.}可知
  $$(\E[Y|X_1])^2\leq\E[(\E[Y|X_1,X_2])^2|X_1]$$
  两端取无条件期望得到
  $$\E[(\E[Y|X_1])^2]\leq \E[(\E[Y|X_1,X_2])^2]$$
  同样由$\E[Y]=\E[\E[Y|X_1]]$和Jensen不等式\footnote{设$g:\R^L\to\R$为下凸函数, 则对于任意使得$\E||X||<\infty$和$\E|g(X)|<\infty$的随机变量$X\in\R^L$, 总有$g(\E[X])\leq\E[g(X)]$.}可知
  \begin{equation}\label{eq1.2}
    (\E[Y])^2\leq\E[(\E[Y|X_1])^2]\leq \E[(\E[Y|X_1,X_2])^2]
  \end{equation}
  注意到$Y$, $\E[Y|X_1]$和$\E[Y|X_1,X_2]$具有相同的期望$\E[Y]$, 于是(\ref{eq1.2})意味着
  \begin{equation}\label{eq1.3}
    0\leq\text{var}(\E[Y|X_1])\leq\text{var}(\E[Y|X_1,X_2])
  \end{equation}

  定义$e=Y-\E[Y|X]$以及$u=\E[Y|X]-\mu$, 其中$u$和$\mu$满足
  $$Y-\mu=e+u$$
  注意到$\E[e|X]=0$且$u$是$X$的函数, 根据条件定理可知
  $$\E[eu]=\E[u\E[e|X]]=0$$
  又因为$\E[e]=0$, 故而$e$和$u$不相关, 从而
  \begin{equation}\label{eq1.4}
    \text{var}(Y)=\text{var}(e)+\text{var}(u)=\text{var}(Y-\E[Y|X])+\text{var}(\E[Y|X])
  \end{equation}
  将(\ref{eq1.3})应用于(\ref{eq1.4})即可证明定理.
\end{proof}

\section{最优预测量与条件方差}
对于任意给定的随机向量$X$, 现在我们的目标是预测$Y$, 我们可以将$X$任意的预测量记作$g(X)$, 预测误差即为$Y-g(X)$的实现值, 可以$g$的均方误差来衡量其误差大小:
\begin{equation}\label{eq1.6}
 \MSE(g)=\E[(Y-g(X))^2]
\end{equation}
其中$g:\R^K\to\R$为可测函数, 我们定义最优预测量为使得(\ref{eq1.6})最小化的$g(X)$. 下面将要证明, 对于任意$(Y,X)$的联合分布, 最优预测量为CEF $m(X)$.
\begin{theorem}\label{thm:thm1.4}
  设$X$为任意随机向量, 如果$\E[Y^2]<\infty$, 那么对于任意预测量$g(X)$都有
  $$\E[(Y-g(X))^2]\geq\E[(Y-m(X))^2]$$
  其中$m(X)=\E[Y|X]$.
\end{theorem}
\begin{proof}
  注意到对任意预测量$g(X)$都有
  \begin{align*}
  \E[(Y-g(X))^2]&=\E[(e+m(X)-g(X))^2] \\
  &=\E[e^2]+2\E[e(m(X)-g(X))]+\E[(m(X)-g(X))^2] \\
  &=\E[e^2]+\E[(m(X)-g(X))^2] \\
  &\geq \E[e^2]=\E[(Y-m(X))^2]
  \end{align*}
  其中由定理\ref{thm:thm1.3}(4)可知$\E[e(m(X)-g(X))^2]=0$. 当且仅当$g(X)=m(X)$时, 上式大于等于可以取到等号.
\end{proof}

在上面的论述中, 我们已经用条件期望描述了条件分布的“位置”, 但它没有告诉我们条件分布的分散程度, 为了对其进行刻画, 现在需要引入条件方差的概念.
\begin{definition}
如果$\E[W^2]<\infty$, 那么给定条件$X=x$时, $W$的条件方差为
$$\sigma^2(x)=\text{var}(W|X=x)=\E[(W-\E[W|X=x])^2|X=x]$$
类似地, 可以将$\sigma^2(X)=\text{var}[W|X]$视为一个随机变量.
\end{definition}
当CEF误差满足矩条件$\E[e^2]<\infty$时, 我们可以将条件$X=x$下, CEF误差$e$的条件方差定义为
$$\sigma^2(x)=\text{var}(e|X=x)=\E[e^2|X=x]$$
同样可以将$e$的条件方差$\sigma^2(X)=\text{var}(e|X)$视为随机变量.

关于条件方差, 下面介绍一个重要定理, 称为总方差定律(Law of Total Variance, LTV), 也称方差分解公式.
\begin{theorem}
  如果$X$和$W$为定义在相同概率空间上的随机变量, 并且$\E[X^2]<\infty$, 那么
  $$\text{var}(X)=\E[\text{var}(X|W)]+\text{var}(\E[X|W])$$
\end{theorem}
\begin{proof}
   根据$\var(X)=\E[X^2]-(\E[X])^2$及LIE可得
      \begin{align}\label{eq1.7}
      \begin{split}
      \var(\E[X|W])&=\E[(\E[X|W])^2]-(\E[\E[X|W]])^2 \\
      &=\E[(\E[X|W])^2]-(\E[X])^2
      \end{split}
      \end{align}
      又根据$\var(X|W)=\E[X^2|W]-(\E[X|W])^2$可得
      \begin{align}\label{eq1.8}
      \begin{split}
      \E[\var(X|W)]&=\E[\E[X^2|W]]-\E[(\E[X|W])^2]  \\
      &=\E[X^2]-\E[(\E[X|W])^2]
      \end{split}
      \end{align}
      将式(\ref{eq1.7})和(\ref{eq1.8})相加即可证得定理.
\end{proof}
总方差定律将无条件方差分解为了“组内方差”和“组间方差”两个部分. 举例而言, 如果$X$是受教育程度, $W$是年龄, 那么第一项$\E[\var(X|W)]$刻画了所有年龄水平上受教育程度差异的均值, 第二项$\var(\E[X|W])$则为不同年龄分组间平均受教育程度的差异.

从预测的角度看, 总方差定律表明: $\E[Y|X]$变化程度越大, 则它更能够解释$Y$的变化, 因而也能更好地预测$Y$. 由于$\text{var}(Y)$是一个常数, 最优预测量$m(X)$的变化越大, 则均方误差的期望越小. 当$\E[Y|X]=Y$时, 均方误差为0, CEF可以完全刻画$Y$的变化. 事实上, 利用LTV可以证明
$$\var(Y)=\var(\E[Y|X])+\var(Y-\E[Y|X])$$

由于回归误差$e$具有零条件均值, 所以它的无条件误差方差等于期望条件方差, 也即
$$\sigma^2=\E[e^2]=\E[\E[e^2|X]]=\E[\sigma^2(X)]$$
也即无条件误差方差相当于是“平均的”条件误差方差.

给定条件方差后, 我们可以定义一个调整误差
\begin{equation}\label{eq1.9}
  u=\frac{e}{\sigma(X)}
\end{equation}
由于$\sigma^2(X)$是$X$的函数, 可以计算出
$$\E[u|X]=\E\left[\left.\frac{e}{\sigma(X)}\right|X\right]=\frac{1}{\sigma(X)}\E[e|X]=0$$
以及
$$\var(u|X)=\E[u^2|X]=\E\left[\left.\frac{e^2}{\sigma^2(X)}\right|X\right]=\frac{1}{\sigma^2(X)}\E[e^2|X]=1$$
也即$u$在给定$X$下的条件期望为0, 条件方差为1. 注意到(\ref{eq1.9})可以改写为
$$e=\sigma(X)u$$
将其代入到CEF方程中得到
$$Y=m(X)+\sigma(X)u$$
称为均值-方差形式的CEF方程.

最后我们给出同方差和异方差的概念, 这在后面推导参数估计量的渐近性质时具有重要作用.
\begin{definition}
CEF误差$e$是同方差的, 如果$\sigma^2(x)=\sigma^2$不依赖于$x$. 若$\sigma^2(x)$依赖于$x$, 则称$e$是异方差的.
\end{definition}
\begin{remark}
这里定义的同方差和异方差都是条件方差意义下的, 而非无条件方差. 显然, 无条件方差$\sigma^2$是一个固定的常数, 它独立于回归元$X$, 也就无所谓同方差还是异方差了.
\end{remark}
在现实的经济社会中, 同方差是几乎难以达成的苛刻条件, 而异方差才是对回归模型的一个更标准和更稳健的假设. 关于同方差和异方差的图示, 具体可以参考Wooldridge (2019)中的Figure 2.8与Figure 2.9.

造成异方差的其中一个原因在于, 如果$\E[Y|X]$, 那么$\var(Y|X)$和更高阶的条件矩也很有可能依赖于$X$. 倘若我们考虑现实的因素, 则还有其他可能的解释, 例如在OCED国家中, 政府规模大小和GDP波动率具有显著的负相关关系 (Fat\'{a}s and Mihov, 2001).

\section{回归导数}
现在的问题是如何解释在CEF $m(x)=\E[Y|X=x]$中, 回归元$x$的边际变化如何导致响应变量$Y$的条件期望变化. 通常而言, 如果要考虑单个回归元$X_1$边际变化, 此时应该保持其它因素不变. 当$X_1$具有连续分布时, 定义$X_2,\cdots,X_k$不变时$x_1$变化的边际效应为
$$\frac{\partial}{\partial x_1}m(x_1,x_2,\cdots,x_k)$$
若$X_1$是离散型的, 则$X_1$的边际效应为离差形式. 例如, 若$X_1$为二值变量, 则它在其它条件不变时对CEF的边际效应为
$$m(1,x_2,\cdots,x_k)-m(0,x_2,\cdots,x_k)$$
我们可以用如下记号表述边际效应
$$\nabla_1m(x)=\begin{cases}
                 \displaystyle\frac{\partial}{\partial x_1}m(x_1,x_2,\cdots,x_k), &\text{如果}\,X_1\,\text{为连续型} \\
                 m(1,x_2,\cdots,x_k)-m(0,x_2,\cdots,x_k), & \text{如果}\,X_1\,\text{为二值变量}
               \end{cases}$$
将$K-1$个解释变量的边际效应写成列向量的形式, 则关于$X$的回归导数为
$$\nabla m(x)=\begin{bmatrix}
           \nabla_1m(x) \\
           \nabla_2m(x) \\
           \vdots \\
           \nabla_km(x)
         \end{bmatrix}$$
当$x$中的所有元素都为连续型随机变量时, 回归导数可以简化为$\displaystyle\nabla m(x)=\partial m(x)/\partial x$.

当我们计算某个解释变量对CEF的边际效应时, 需要保持其它因素不变, 这通常称为Ceteris Paribus概念. 然而实际上, 我们只能控制那些被包括进了$\E[Y|X]$中的其它因素不变, 无法做到保持其它所有一切的因素不变, 因此回归导数依赖于解释变量的选取.

另一方面, 回归导数$\nabla m(x)$刻画的是$Y$的条件期望的变化, 而非$Y$的具体值的变化. 只有当$e$不受回归元$X$影响时, $\nabla m(x)$才是$Y$的真实值的变化.

\section{线性CEF}
CEF中极其特殊的一种情况是线性CEF, 也即$m(x)=\E[Y|X=x]$线性于$x$, 此时
$$m(x)=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_{k}x_{k}$$
定义记号$X=[1,X_1,X_2,\cdots,X_{k}]'$, 则线性CEF为
\begin{equation}\label{eq1.10}
  m(x)=x'\beta
\end{equation}
其中$\beta=[\beta_0,\beta_1,\cdots,\beta_k]'$为$K\times1$参数向量. 将线性CEF代入$Y=m(X)+e$中可得
\begin{align*}
Y&=X'\beta+e \\
\E[e|X]&=0
\end{align*}
称为$Y$对$X$的线性回归模型 (Linear Regression Model, LRM).


在线性CEF中, 回归导数可以简化为参数向量, 也即$\nabla m(x)=\beta$, 这是线性CEF一个颇具吸引力的特点. 回归系数$\beta_j$可以解释为保持其它条件不变时, $X_j$变化对CEF带来的边际效应.
\begin{example}\label{eg3}
考虑如下工资方程
$$\text{wage}=\beta_0+\beta_1\text{educ}+\beta_2\text{exp}+e$$
其中$\text{wage}$为工资, educ为受教育程度, exp为工作经验. 那么$\beta_1$可以粗略地解释为: 在工作经验相同的情况下, 每增加一年教育水平, 条件期望$\E[\text{wage}|\text{educ}, \text{exp}]$会提高$\beta_1$\footnote{更确切地说, 当其它解释变量固定不变时, 对于一个很小的$\Delta x_j$, 条件期望的变化为$\displaystyle\Delta \E[Y|x]\approx\frac{\partial m(x)}{\partial x_j}\Delta x_j$.}.
\end{example}

不仅如此, 在线性CEF框架下, 解释变量的非线性影响也能被捕捉到. 假设我们有两个解释变量$X_1$与$X_2$, CEF为如下的二次型
\begin{equation}\label{eq1.11}
  m(x_1,x_2)=\beta_1x_1+\beta_2x_2+\beta_3x_1^3+\beta_4x_2^2+\beta_5x_1x_2+\beta_6
\end{equation}
显然, 回归元$[x_1,x_2]$线性于$\beta=[\beta_1,\cdots,\beta_6]'$, 因此我们也称(\ref{eq1.11})为线性CEF. 然而, 如果分别对$x_1$和$x_2$求偏导
\begin{align*}
\frac{\partial}{\partial x_1}m(x_1,x_2)&=\beta_1+2\beta_3x_1+\beta_5x_2\\
\frac{\partial}{\partial x_2}m(x_1,x_2)&=\beta_2+2\beta_4x_2+\beta_5x_1
\end{align*}
此时回归导数不再是简单的回归系数, 而是关于$(x_1,x_2)$的函数, 因此很难单独解释回归系数的意义.

我们称 (\ref{eq1.11})中的回归系数$\beta_5$为交互效应. 如果$\beta_5>0$, 那么关于$x_1$的回归导数随着$x_2$的增加而增加, 关于$x_2$的回归导数也随着$x_1$的增加而增加. 类似地, 如果$\beta_5<0$, 那么关于$x_1$的回归导数随着$x_2$的增加而减少, 关于$x_2$的回归导数同样随着$x_1$的增加而减少.

如果回归元包含虚拟变量, 那么CEF也能写成是回归元的线性函数, 现在我们考虑将例\ref{eg3}中的工资方程进行拓展.

\begin{example}考虑如下回归模型
\begin{equation}\label{eq1.13}
  \text{wage}=\beta_0+\beta_1\text{educ}+\delta_0\text{female}+e
\end{equation}
其中$\text{wage}$为时薪, $\text{educ}$为受教育程度, $\text{female}$为性别, 它是一个虚拟变量:
$$\text{female}=\begin{cases}
                  1, & \text{性别为女} \\
                  0, & \text{性别为男}
                \end{cases}$$
在零条件均值$\E[e|\text{female}, \text{educ}]=0$的假定下, 回归系数$\delta_0$为
$$\delta_0=\E[\text{wage}|\text{female}=1, \text{educ}]-\E[\text{wage}|\text{feamle}=0,\text{educ}]$$
其含义为: 在相同的受教育水平上, 女性与男性的时薪差异. 此外, 我们还可以在工资方程(\ref{eq1.13})中加入交互项得到
$$\text{wage}=\beta_0+\beta_1\text{educ}+\delta_0\text{female}+\delta_1\text{female}\cdot\text{educ}+e$$
于是
\begin{align*}
\E[\text{wage}|\text{female}=0, \text{educ}]&=\beta_0+\beta_1\text{educ} \\
\E[\text{wage}|\text{female}=1, \text{educ}]&=(\beta_0+\delta_0)+(\beta_1+\delta_1)\text{educ}
\end{align*}
此时$\delta_1$衡量的是女性与男性教育回报差异.
\end{example}
\section{最优线性预测量}
定理\ref{thm:thm1.4}表明在所有关于$X$的可测函数$g$中, CEF $m(X)=\E[Y|X]$是对$Y$最优的预测量, 然而它的形式通常是未知的. 现在我们考虑线性CEF的情形, 尽管它无法捕捉到高维信息和包括所有的交互项, 但它毕竟没有复杂到我们无法对其进行分析.

在进行分析前, 先给出一些正则条件.
\begin{proposition}\label{pro:pro1.1}
设$Y$为随机变量, $X$为$K$维随机向量, 那么:

(1) $\E[Y^2]<\infty$.

(2) $\E||X||^2<\infty$, 其中$||\cdot||$表示范数\footnote{范数$||x||=(x'x)^{1/2}$表示向量$x$的Euclid模长.}.

(3) $\mathbold{Q}_{XX}=\E[XX']$为$K\times K$维正定矩阵.
\end{proposition}
以上假设的第(1)点和第(2)点表明$Y$和$X$具有有限的期望, 方差及协方差, 而第(3)点是技术性假设, 它也等价于$\mathbold{Q}_{XX}=\E[XX']$是可逆矩阵, 表明线性投影系数$\beta$是可识别的 (identified)\footnote{一个参数是可识别的, 如果它能被可观测变量的概率分布唯一确定.}.

\begin{definition}
给定条件$X$时, $Y$的最优线性预测量为
$$\mathscr{P}[Y|X]=X'\beta$$
其中$\beta$使得均方误差
\begin{equation}\label{eq1.14}
  S(\beta)=\E[(Y-X'\beta)^2]
\end{equation}
最小化, 也即
\begin{equation}\label{eq1.15}
  \beta=\arg\min_{b\in\R^K}\,S(b)
\end{equation}
称$\beta$为线性投影系数, 也称最优最小二乘近似系数.
\end{definition}
下面将给出关于线性投影投影模型的显式解和其它性质, 但如果CEF $m(X)$不是线性的, 那么我们通常无法找出$\beta$的显式解.
\begin{theorem}\label{thm:thm1.5}
  在假设\ref{pro:pro1.1}下, 以下结论成立:

  (1) 矩$\E[XX']$和$\E[XY]$存在且它们的元素均有限.

  (2) 线性投影系数(\ref{eq1.15})存在且唯一, 等于
  $$\beta=\E[XX']^{-1}\E[XY]$$

  (3) 投影误差$e=Y-X'\beta$存在, 并且$\E[e^2]<\infty$, $\E[Xe]=0$. 如果$X$包含常数项, 那么$\E[e]=0$.

  (4) 若$r\geq2$, $\E|Y|^r<\infty$且$\E||X||^r<\infty$, 那么$\E|e|^r<\infty$.
\end{theorem}
\begin{proof}
  (1) 根据假设\ref{pro:pro1.1}(1), (2), 以及期望不等式和范数的性质可知
  $$||\E[XX']|\leq\E||XX'||=\E||X||^2<\infty|$$
  再由Cauchy-Schwarz不等式得到
  $$||\E[XY]||\leq\E||XY||\leq(\E||X||^2)^{1/2}(\E[Y^2])^{1/2}<\infty$$
  这表明$\E[XX']$和$\E[XY]$都是良定义的, 因此假设\ref{pro:pro1.1}(3)有意义.

  (2) 将均方误差(\ref{eq1.14})展开, 它可以表示为关于$\beta$的二次型:
  $$S(\beta)=\E[Y^2]-2\beta'\E[XY]+\beta'\E[XX']\beta$$
  根据矩阵微分求得一阶条件 (First Order Condition, FOC):
  $$\frac{\partial}{\partial\beta}S(\beta)=-2\E[XY]+2\E[XX']\beta=0$$
  也即
  \begin{equation}\label{eq1.16}
    \Q_{XY}=\Q_{XX}\beta
  \end{equation}
  其中$\Q_{XY}=\E[XY]$为$K\times1$向量. 由于$\Q_{XX}$是可逆的, 于是
  \begin{equation}\label{eq1.17}
    \beta=\Q_{XX}^{-1}\Q_{XY}=\E[XX']^{-1}\E[XY]
  \end{equation}
  最后由二阶条件
  $$\frac{\partial}{\partial\beta\partial\beta'}S(\beta)=2XX'$$
  可知(\ref{eq1.17})为全局最小化最优解.

  (3) 将$e=Y-X'\beta$代入到$\E[e^2]$中得到
  \begin{align*}
  \E[e^2]&=\E[Y^2]-2\E[YX']\beta+\beta'\E[XX']\beta \\
  &=\E[Y^2]-\E[YX'](\E[XX'])^{-1}\E[XY]\\
  &\leq\E[Y^2]<\infty
  \end{align*}
  另一方面, 根据期望不等式和Cauchy-Schwarz不等式可知
  $$||\E[Xe]||\leq\E||Xe||\leq(\E||X||^2)^{1/2}(\E[e^2])^{1/2}<\infty$$
  于是$\E[Xe]$存在, 它等于
  \begin{align*}
  \E[Xe]&=\E[XY]-\E[XX'\beta] \\
  &=\E[XY]-\E[XX'](\E[XX'])^{-1}\E[XY]=0
  \end{align*}
  将$X=[X_1,X_2,\cdots,X_k]$中的第一项设置为1, 根据$\E[Xe]=0$即可知$\E[e]=0$.

  (4) 根据Minkowski不等式可知
  \begin{align*}
  (\E|e|^r)^{1/r}&=(\E|Y-X'\beta|^r)^{1/r}\\
  &\leq(\E|Y|^r)^{1/r}+(\E|X'\beta|^r)^{1/r}\\
  &\leq(\E|Y|^r)^{1/r}+(\E||X||^r)^{1/r}||\beta||<\infty
  \end{align*}

\end{proof}
\begin{remark}
在1.2节中, 我们根据CEF的定义$\E[Y|X]=m(X)$设置了CEF误差$e=Y-\E[Y|X]$, 并证明了$\E[e]=0$. 而在这里, 我们仅需要假设\ref{pro:pro1.1}就可以定义线性回归模型
$$Y=X'\beta+e$$
并且投影误差$e$满足$\E[Xe]=0$, 而无需用到线性CEF $m(X)=X'\beta$这一更强的假设.

事实上, 根据定理\ref{thm:thm1.3}(4)可知CEF误差满足$\E[Xe]=0$, 从而线性CEF是最优线性投影, 然而投影误差并不意味着$\E[e|X]=0$, 因此最优线性投影不一定是线性CEF. 倘若CEF不是线性的, 那我们用线性投影来近似CEF将会产生很大误差.

举例而言, 我们假设数据生成过程 (Data Generating Process, DGP)为$Y=X+X^2$, 其中$X\sim N(0,1)$, 此时CEF $m(x)=x+x^2$并不会在预测$Y$时产生误差. 现在考虑$Y$在$X$和常数项上的线性投影$Y=X'\beta+e$, 因为$X\sim N(0,1)$, 故而$X$与$X^2\sim \chi^2(1)$不相关, 根据定理\ref{thm:thm1.5}(2)可知线性投影系数为
$$\beta=\begin{bmatrix}
          \E[X^2] & \E[X^3] \\
          \E[X^3] & \E[X^4]
        \end{bmatrix}^{-1}\begin{bmatrix}
                            \E[XY] \\
                            \E[X^2Y]
                          \end{bmatrix}=\begin{bmatrix}
                     1 & 0 \\
                     0 & 2
                   \end{bmatrix}^{-1}\begin{bmatrix}
                                       1 \\
                                       2
                                     \end{bmatrix}=\begin{bmatrix}
                                                     1 \\
                                                     1
                                                   \end{bmatrix}$$
从而线性投影为$\mathscr{P}[Y|X]=1+X$, 投影误差为$e=X^2-1$, 显然它不是CEF误差.
\end{remark}
在CEF模型中, 我们定义了误差方差$\sigma^2=\E[e^2]$, 利用$e=Y-X'\beta$可以将其改写为
\begin{align*}
\sigma^2&=\E[Y^2]-2\E[YX']\beta+\beta'\E[XX']\beta \\
&=Q_{YY}-2\Q_{YX}\Q_{XX}^{-1}\Q_{XY}+\Q_{YX}\Q_{XX}^{-1}\Q_{XX}\Q_{XX}^{-1}\Q_{XY}\\
&=Q_{YY}-\Q_{YX}\Q_{XX}^{-1}\Q_{XY}\equiv Q_{YY\cdot X}
\end{align*}
它揭示了$Q_{YY\cdot X}=Q_{YY}-\Q_{YX}\Q_{XX}^{-1}\Q_{XY}$等于$Y$在$X$上的线性投影误差的方差.

有时候我们会将回归元中的常数项分离出来, 而将线性模型写为以下形式
\begin{equation}\label{eq1.18}
  Y=X'\beta+\alpha+e
\end{equation}
其中$\alpha$为截距项, $X$不包括任何常量. 对上式两端取期望得
$$\E[Y]=\E[X'\beta]+\E[\alpha]+\E[e]$$
由于$\E[e]=0$, 故而$\mu_Y=\mu_X'\beta+\alpha$, 其中$\mu_Y=\E[Y]$而$\mu_X=\E[X]$. 将$\alpha=\mu_Y-\mu_X'\beta$代入到(\ref{eq1.18})可得
$$Y-\mu_Y=(X-\mu_X)'\beta+e$$
如果假设\ref{pro:pro1.1}成立, 那么根据定理\ref{thm:thm1.5}可知线性投影系数为
\begin{align*}
\beta=[\var(X)]^{-1}\text{cov}(X,Y)
\end{align*}
其中协方差矩阵$\text{cov}(X,Y)=\E[(X-\E[X])(Y-\E[Y])']$, 以及$\var(X)=\text{cov}(X,X)=\E[(X-\E[X])(X-\E[X])']$.

更多关于线性投影的内容可以参考Wooldridge (2010)的第二章.
\section{分块回归与遗漏变量偏误}
现在将回归元$X$分割为
\begin{equation}\label{eq1.22}
X=\begin{bmatrix}
      X_1 \\
      X_2
    \end{bmatrix}
\end{equation}
再将回归系数分割为$\beta=[\beta_1',\beta_2']'$, 于是$Y$对$X$的线性投影为
\begin{align}
\begin{split}
Y&=X_1'\beta_1+X_2'\beta_2+e \\
\E[Xe]&=0
\end{split}
\label{eq1.20}
\end{align}
现在问题是如何推导出子向量$\beta_1$和$\beta_2$.

首先分割$\Q_{XX}$为
$$\Q_{XX}=\begin{bmatrix}
            \Q_{11} & \Q_{12} \\
            \Q_{21} & \Q_{22}
          \end{bmatrix}=\begin{bmatrix}
                          \E[X_1X_1'] & \E[X_1X_2'] \\
                          \E[X_2X_1'] & \E[X_2X_2']
                        \end{bmatrix}$$
类似可以分割$\Q_{XY}$为
$$\Q_{XY}=\begin{bmatrix}
            \Q_{1Y} \\
            \Q_{2Y}
          \end{bmatrix}=\begin{bmatrix}
                          \E[X_1Y] \\
                          \E[X_2Y]
                        \end{bmatrix}$$
根据分块矩阵求逆公式得
\begin{align}
  \Q_{XX}^{-1}&=\begin{bmatrix}
                 \Q_{11} & \Q_{12} \\
                 \Q_{21} & \Q_{22}
               \end{bmatrix}^{-1}\equiv\begin{bmatrix}
                                         \Q^{11} & \Q^{12} \\
                                         \Q^{21} & \Q^{22}
                                       \end{bmatrix} \nonumber \\
                                       &=\begin{bmatrix}
                                                       \Q_{11\cdot 2}^{-1} & -\Q_{11\cdot2}^{-1}\Q_{12}\Q_{22}^{-1} \\
                                                       -\Q_{22\cdot1}^{-1}\Q_{21}\Q_{11}^{-1} & \Q_{22\cdot1}^{-1}
                                                     \end{bmatrix} \label{eq1.19}
\end{align}
其中$\Q_{11\cdot2}\equiv\Q_{11}-\Q_{12}\Q_{22}^{-1}\Q_{21}$, $\Q_{22\cdot1}\equiv\Q_{22}-\Q_{21}\Q_{11}^{-1}\Q_{12}$. 因此
\begin{align}
\beta&=\begin{bmatrix}
\Q_{11\cdot 2}^{-1} & -\Q_{11\cdot2}^{-1}\Q_{12}\Q_{22}^{-1} \\
-\Q_{22\cdot1}^{-1}\Q_{21}\Q_{11}^{-1} & \Q_{22\cdot1}^{-1}
\end{bmatrix}\begin{bmatrix}
\Q_{1Y} \\
\Q_{2Y}
\end{bmatrix} \nonumber \\
&=\begin{bmatrix}
    \Q_{11\cdot2}^{-1}(\Q_{1Y}-\Q_{12}\Q_{22}^{-1}\Q_{2Y}) \\
    \Q_{22\cdot1}^{-1}(\Q_{2Y}-\Q_{21}\Q_{11}^{-1}\Q_{1Y})
  \end{bmatrix}=\begin{bmatrix}
                  \Q_{11\cdot2}^{-1}\Q_{1Y\cdot2} \\
                  \Q_{22\cdot1}^{-1}\Q_{2Y\cdot1}
                \end{bmatrix} \label{eq1.24}
\end{align}
因此$\beta_1=\Q_{11\cdot2}^{-1}\Q_{1Y\cdot2}$, $\beta_2=\Q_{22\cdot1}^{-1}\Q_{2Y\cdot1}$.

特别地, 如果$\text{dim}\,(X_1)=1$, 也即$\beta_1\in\R$, 此时可以将(\ref{eq1.20})改写为
\begin{equation}\label{eq1.21}
  Y=X_1\beta_1+X_2'\beta_2+e
\end{equation}
再来考虑$X_1$对$X_2$的线性投影
\begin{align*}
X_1&=X_2'\gamma_2+u_1 \\
\E[X_2u_1]&=0
\end{align*}
于是$\gamma_2=\Q_{22}^{-1}\Q_{21}$, $\E[u_1^2]=\Q_{11}-\Q_{12}\Q_{22}^{-1}\Q_{21}$, 以及
\begin{align*}
\E[u_1Y]&=\E[(X_1-\gamma_2'X_2)Y]=\E[X_1Y]-\gamma_2'\E[X_2Y] \\
&=\Q_{1Y}-\Q_{12}\Q_{22}^{-1}\Q_{2Y}=\Q_{1Y\cdot2}
\end{align*}
因此
$$\beta_1=\Q_{11\cdot2}^{-1}\Q_{1Y\cdot2}=\frac{\E[u_1Y]}{\E[u_1^2]}$$
也即$\beta_1$为$Y$对$u_1$回归得到的回归系数.

注意到$u_1$是$X_1$对$X_2$投影得到的误差, 我们可以认为它是$X_1$的变化中无法被$X_2$线性解释的部分, 因此$Y$对$u$回归就是想用这些部分来线性解释$Y$的变化, 于是$\beta_1$可以很自然地理解为过滤掉其它回归元对$X_1$和$Y$的影响后, $X_1$对$Y$的线性影响.

同样考虑回归元被分割为(\ref{eq1.22})那样的形式, 但假设$X_2$无法被观测到, 因此我们只考虑$Y$在$X_1$上的投影:
\begin{align}
Y&=X_1'\gamma_1+u \label{eq1.23} \\
\E[X_1u]&=0 \nonumber
\end{align}
计算线性投影系数$\gamma_1$得到
\begin{align*}
\gamma_1&=(\E[X_1X_1'])^{-1}\E[X_1Y] \\
&=(\E[X_1X_1'])^{-1}\E[X_1(X_1'\beta_1+X_2'\beta_2+e)] \\
&=\beta_1+(\E[X_1X_1'])^{-1}\E[X_1X_2']\beta_2\equiv \beta_1+\Gamma_{12}\beta_2
\end{align*}
其中$\Gamma_{12}=\Q_{11}^{-1}\Q_{12}$是将$X_2$投影到$X_1$上得到的系数矩阵.

显然$\gamma_1=\beta_1+\Gamma_{12}\beta_2\neq\beta_1$, 除非$\beta_2=0$或$\Gamma_{12}=0$, $\gamma_1$和$\beta_1$之间的差$\Gamma_{12}\beta_2$称为遗漏变量偏误 (Omitted Variable Bias), 此时模型设定错误, 导致内生性 (endogeneity)问题. 显然, 如果系数$\beta_2>0$, 若$X_2$对$X_1$有正 (负)向影响, 我们得到的回归系数$\gamma$将会高 (低)估$\beta$. 类似可以分析$\beta_2<0$时的情况.

为了避免遗漏变量偏误, 最好将所有潜在的相关变量纳入到回归模型中, 但显然这极不可能, 我们只能尽可能将合适的变量纳入到回归模型中以减轻遗漏变量偏误的影响.

%\section{因果效应}
%最后我们简要介绍Rubin (1974)提出的反事实框架, 也即Rubin因果模型 (Rubin Causal Model, RCM).

%设$Y$是潜在结果变量, $D$为二值处置变量, $X$为可观测的协变量, $U$为$l\times1$维不可观测的随机因素, 那么我们可以用以下模型来描述处置变量对结果变量的影响
%\begin{equation}\label{eq1.24}
% Y=h(D,X,U)
%\end{equation}
%其中$h$为Borel可测函数, 通常也用符号$Y(0)=h(0,X,U)$和$Y(1)=h(1,X,U)$来分别表示非处置和处置情形下的潜在结果, 那么$D$对$Y$的%因果影响为
%$$C(X,U)=h(1,X,U)-h(0,X,U)$$
%表示$X$和$U$保持不变时, 由处置变量引起的$Y$的变化.

%值得注意的是, 处理变量对潜在结果的因果效应一定是针对个体层面而言的, 然而对于任意给定的个体$i$, 我们无法同时观测到$Y_i(0)$和$Y_i(1)$, 而只能观测到实现值
%$$Y_i=\begin{cases}
%      Y_i(0), & D_i=0 \\
%      Y_i(1), & D_i=1
%    \end{cases}$$
%也就是说, 如果我们观测到了$Y_i(1)$, 则无法观测到$Y_i(0)$, 此时称$Y_i(0)$为对应于$Y_i(1)$的反事实结果, 反之也是一样. 因此$C(X,U)$实质上是不可观测的, 这是Holland (1986)提出的因果推断的根本难点.

%\begin{example}[上大学对工资的影响$^1$]\label{eg1}
%假设考上高中和大学就一定能毕业. 我们想知道上大学对收入的影响, 然而我们无法在观测到同一个人上大学毕业后的工资和高中毕业后的工资: 如果$Y_i(1)$表示个体$i$大学毕业后的时薪, 那么$Y_i(0)$就代表该个体如果没上大学所获得的时薪.

%现在我们假定其它一切条件不变的情况下, A类学生高中毕业后的时薪为10美元, 大学毕业后的时薪为20美元; B类学生高中毕业后的时薪为8美元, 大学毕业后的时薪为12美元. 于是读大学对A类学生工资的因果效应为10美元, 而对B类学生的因果效应为4美元.

%倘若我们有$(Y,D,X)$的观测, 我们想借此分析出$D$对$Y$的因果效应, 一个直接想法是用$Y$对$(D,X)$进行回归, 那么可以将$D$的回归系数解释为因果效应吗? 我们将给出一个充分条件来解释.
%\end{example}

%\begin{definition}[条件独立假设 Conditional Independence Assumption, CIA]
%以$X$为条件, 随机变量$D$和不可观测效应$U$在统计上是独立的.
%\end{definition}
%CIA意味着: 给定$(D,X)$时, $U$的条件概率密度只取决于X, 也即:
%$$f(u|D,X)=f(u|X)$$
%%它是将回归系数和条件ACE连接起来的桥梁. 在例\ref{eg1}中, CIA意味着是否上大学不受个体不可观测的特征影响.
%\begin{theorem}
%  在结构方程(\ref{eq1.24})中, 如果CIA成立, 那么$\nabla m(d,x)=\text{ACE}(x)$, 也即关于处置变量的回归导数等于条件ACE.
%\end{theorem}
%\begin{proof}
  %在CIA成立的条件下, 使用$Y$对$(D,X)$回归得到
  %\begin{align*}
 % m(d,x)&=\E[Y|D=d, X=x] \\
 % &=\E[h(d,x,u)|D=d, X=x] \\
 % &=\int h(d,x,u)f(u|x)\,\text{d}u
 % \end{align*}
 % 于是
 % \begin{align*}
 % \nabla m(d,x)&=m(1,x)-m(0,x) \\
 % &=\int h(1,x,u)f(u|x)\,\text{d}u-\int h(0,x,u)f(u|x)\,\text{d}u \\
 % &=\int C(x,u)f(u|x)\,\text{d}x=\text{ACE}(x)
 % \end{align*}
 % 由此证得定理.
%\end{proof}
%\begin{remark}
%实际上, 如果我们只关注ACE, 那么只需满足条件均值独立假设这一更弱的假设即可, 也就是说以$X$为条件时, $D$和$U$是均值独立的.
%\end{remark}
%该定理表明, 在我们控制了协变量后, 只要处置变量和不可观测效应独立, 那么处置变量的回归导数就等于条件ACE. 从1.7节中对遗漏变量偏误的讨论来看, 我们需要纳入足够多合适的协变量到回归方程中, 也是为了尽可能保证CIA成立, 也即使得影响结果变量的不可观测效应独立于处置变量.

%在(\ref{eq1.24})中, 我们定义在条件$X=x$下, $D$对$Y$的条件平均处理效应为
%$$\text{ACE}(x)=\E[C(X,U)|X=x]=\int_{\R^l}C(x,u)f(u|x)\,\text{d}u$$
%其中$f(u|x)$是给定$X$时, $U$的条件概率密度. $\text{ACE}(x)$表示在具有$X=x$的子总体中, $D$对$Y$的处置效应的平均值. 另一方面, 定义$D$对$Y$的无条件平均处置效应为
%$$\text{ACE}=\E[C(X,U)]=\int_{\R^l}\text{ACE}(x)f(x)\,\text{d}x$$
%其中$f(x)$为$X$的概率密度函数.

%\begin{example}[上大学对工资的影响$^2$]
%现在假定上大学取决于考试成绩, 对于每一个A类学生和B类学生, 如果TA拿高分, 则上大学的概率为$3/4$; 而如果TA拿低分, 则上大学的概率为$1/4$. 这意味着是否上大学与学生是A类还是B类无关, 而是取决于考试成绩.

%在拿高分的学生中, 他们中有$3/4$的工资和A类学生一样, $1/4$的则和B类学生一样; 而在拿低分的学生中, 他们中的$1/4$拿的工资和A类学生一样, $3/4$的则和B类学生一样.

%假设总体中有一半人是高中毕业生, 另一半是大学毕业生. 对于拿高分的学生而言, 条件ACE为$(3/4)\times10+(1/4)\times4=8.5$美元, 而对于拿低分的学生, 条件ACE为$(1/4)\times10+(3/4)\times4=5.5$美元, 从而无条件ACE为平均值$(8.5+5.5)/2=7$美元.

%特别地, 如果我们随机抽取32个样本, 并收集到了如下数据. 此时抽样得到的ACE与真实ACE一致, 均为7美元.

%\begin{table}[htbp!]
%\centering
%\begin{tabular}{lccccc}
%\hline
%        & 8美元 & 10美元 & 12美元 & 20美元 & 均值  \\ \cline{2-6}
%高中毕业+高分 & 1   & 3    & 0    & 0    & 9.5 \\
%大学毕业+高分 & 0   & 0    & 3    & 9    & 18  \\
%高中毕业+低分 & 9   & 3    & 0    & 0    & 8.5 \\
%大学毕业+低分 & 0   & 0    & 3    & 1    & 14  \\ \hline
%\end{tabular}
%\end{table}

%倘若此时我们将A类学生大学毕业后的时薪由20美元改为30美元, 其它条件不变,  那么按照我们给出的条件, 理论计算出的ACE将与随机抽样计算出的ACE不一致, 表明除了考试成绩外, 还有我们未控制的因素也影响了大学入学. 换言之, “高分学生有$3/4$的概率上大学, $1/4$的概率不上大学; 低分学生有$1/4$的概率上大学, $3/4$的概率不上大学”是对真实情况的错误描述.
%\end{example}
%前面已经提到, 无法计算$Y(1)-Y(0)$是因果推断的根本难点, 但这并非不可克服, 解决方法之一是通过随机分组的方法, 使得个体$i$是否上大学通过抛硬币或电脑随机数的方式来决定, 此时我们只需计算样本中处置组和控制组的平均时薪之差, 这称为双重差分法(Difference-in-Difference, DiD). 当然, 随机选择个体上大学不具有可操作性, 因此更合理估计因果效应的方法是后面会提到的断点回归设计 (Regression Discontinuity Design, RDD).


\chapter{经典线性回归模型}
本章主要介绍有限样本下的经典线性回归模型, 也即OLS回归模型, 它是构筑现代计量经济学的基石.
\section{CLRM假设}
设$\{Y_i,X_i'\}_{i=1}^n$是一个样本容量为$n$的随机样本, $Y_i$为一个标量, $X_i=[1,X_{1i},\cdots,X_{ki}]'$为$K\times 1$维列向量. 我们的目的是通过随机样本$\{Y_i,X_i\}_{i=1}^n$对条件期望$\E[Y|X]$建模, 估计未知参数并进行统计推断. 为此我们先给出经典线性回归模型 (Classical Linear Regression Model, CLRM)假设.

\begin{proposition}[线性]\label{pro:pro2.1}
$\{Y_i,X_i'\}_{i=1}^n$是一个可观测的随机样本, 并且
$$Y_i=X_i'\beta+e_i,\quad i=1,2,\cdots,n$$
其中$\beta$为$K\times1$维未知参数向量, $e_i$是不可观测的随机扰动项.
\end{proposition}
当线性CEF是对真实情况的正确设定时, 也即$\E[e_i|X_i]=0$时, 参数$\beta$可以解释为$X_i$对$Y_i$的期望边际效应.

当然, 假设\ref{pro:pro2.1}并不意味着$X_i$和$Y_i$存在因果关系, 它描述的仍然是上一章提到的线性预测关系. 无论统计关系多么强和富有启示性, 都无法确立因果关系, 它必须来源于统计学之外的经济理论.

现在我们用定义以下记号
\begin{align*}
Y&=[Y_1,Y_2,\cdots,Y_n]',\quad n\times1 \\
e&=[e_1,e_2,\cdots,e_n]',\quad n\times1 \\
\mathbold{X}&=[X_1,X_2,\cdots,X_n]',\quad n\times K
\end{align*}
这里$\mathbold{X}$的第$i$行是$K$维列向量$X_i'=[1,X_{1i},\cdots,X_{ki}]$. 通过以上符号, 我们可以将假设\ref{pro:pro2.1}简洁地表示为
$$Y=\mathbold{X}\beta+e$$

\begin{proposition}[严格外生性]\label{pro:pro2.2}
对于一切$ i=1,2,\cdots,n$都有:
$$\E[e_i|\mathbold{X}]=0$$
\end{proposition}
我们可以将该假设简写为$\E[e|\X]=0$, 它意味着线性CEF是对真实情况的正确设定. 根据LIE可知$\E[e_i|X_i]=0$, 以及$\E[e_i]=0$. 不仅如此, 对于任意$1\leq i, j\leq n$, 都有
\begin{align*}
\E[X_j e_i]&=\E[\E[X_j e_i|\mathbold{X}]]=\E[X_j\E[e_i|\mathbold{X}]]=0
\end{align*}
由于$\E[e_i]=0$, 因此假设\ref{pro:pro2.2}意味着每个解释变量和随机扰动项不相关. 显然, 这一假设排除了动态回归模型, 也即$X_i$中包含$Y_i$的滞后项的情形.

特别地, 如果$\{Y_i,X_i\}_{i=1}^n$是i.i.d.随机样本, 那么假设\ref{pro:pro2.2}等价于$\E[e_i|X_i]=0$.

\begin{proposition}[非奇异性]\label{pro:pro2.3}
$K\times K$维方阵$\X'\X=\sum_{i=1}^{n}X_iX_i'$是正定的.
\end{proposition}
以上假设中的(1)排除了$K$个解释变量存在完全多重共线性 (perfect multicolinearity)的可能, 也即排除了至少存在某个解释变量可以表示为其它$K-1$个解释变量的线性组合的情形. $\X'\X$的非奇异性意味着$X$必须是满秩矩阵, 因此解释变量个数$K$不能超过样本容量$n$.

\begin{proposition}[球型扰动项]\label{pro:pro2.4}
(1) 同方差性: $\E[e_i^2|\X]=\sigma^2$, $i=1,2,\cdots,n$.

(2) 无自相关: $\E[e_ie_j|\X]=0$, $1\leq i,j\leq n$, $i\neq j$.
\end{proposition}
我们可以将以上假设简洁地表示为
$$\E[ee'|\X]=\sigma^2\mathbold{I}_n$$
其中$\mathbold{I}_n$是$n\times n$维单位矩阵. 鉴于球型扰动项假设难以成立, 我们使用符号$\mathbold{\Sigma}=\E[ee'|\X]$来表示一般情况下的协方差矩阵.

根据LIE, 假设\ref{pro:pro2.4}表明, 对于任意$1\leq i\leq n$都有$\var(e_i^2)=\sigma^2$, 并且对一切$i\neq j$都有$\E[e_ie_j]=0$.

\begin{remark}
假设\ref{pro:pro2.2}和\ref{pro:pro2.4}无法推出$e_i$和$\X$相互独立, 因为$e_i$的条件高阶矩可能依赖于$\X$.
\end{remark}
\section{普通最小二乘估计}
下面将介绍普通最小二乘 (Ordinary Least Squares, OLS)估计方法, 这也是计量经济学最基本的估计方法.

\begin{definition}
  定义线性回归模型$Y_i=X_i'\beta+e_i$的残差平方和 (Sum of Squared Residuals, SSR):
  $$\text{SSR}(\beta)\equiv (Y-\X\beta)'(Y-\X\beta)$$
  则OLS估计量$\hat{\beta}$是以下最优化问题的解
  $$\hat{\beta}=\arg\min_{\beta\in\R^K}\,\text{SSR}(\beta)$$
\end{definition}

\begin{theorem}[OLS估计量的存在性]
  在假设\ref{pro:pro2.1}和\ref{pro:pro2.3}下, OLS估计量$\hat{\beta}$存在, 且等于
  \begin{equation}\label{eq2.1}
    \hat{\beta}=(\X'\X)^{-1}\X'Y
  \end{equation}
\end{theorem}
\begin{proof}
  先将残差平方和展开得到
  \begin{align*}\text{SSR}(\beta)&=(Y-\X'\beta)'(Y-\X'\beta) \\
  &=Y'Y-\beta'\X'Y-Y'\X\beta+\beta'\X'\X\beta\\
  &=Y'Y-2Y'\X\beta+\beta'\X'\X\beta
  \end{align*}
  其中$\beta'\X'Y=Y'\X\beta$为标量. 进而根据矩阵微分可知FOC为
  $$\frac{\partial \text{SSR}(\beta)}{\partial\beta}=-2\X'Y+2\X'\X\beta=0$$
  因此OLS估计量满足
  $$\X'Y=\X'\X\hat{\beta}$$
  由于$\X'\X$是可逆矩阵, 于是
  $$\hat{\beta}=(\X'\X)^{-1}\X'Y$$
  最后考虑二阶条件
  $$\frac{\partial^2\text{SSR}(\beta)}{\partial\beta\partial\beta'}=2\X'\X$$
  根据$\X'\X$的正定性可知$\hat{\beta}$是全局最小化最优解.
\end{proof}

注意到
$$\sum_{i=1}^{n}X_iX_i'=\X'\X,\quad\sum_{i=1}^{n}X_iY_i=\X'Y$$
因此OLS估计量$\hat{\beta}$除了表示为(\ref{eq2.1}), 还可以将其写为
$$\hat{\beta}=\left(n^{-1}\sum_{i=1}^{n}X_iX_i'\right)^{-1}\left(n^{-1}\sum_{i=1}^{n}X_iY_i\right)$$
这对于分析OLS估计量的渐近性质十分有用.  特别地, 对于一元线性回归$Y_i=\beta_0+\beta_1X_{1i}+e_i$, 其OLS估计量为
$$\hb_1=\frac{\sum_{i=1}^{n}X_iY_i}{\sum_{i=1}^{n}X_i^2},\quad \hb_0=\overbar{Y}-\hat{\beta}_1\overbar{X}$$
其中$\overbar{Y}$和$\overbar{X}$分别是$Y$和$X$的样本均值.


我们称$\hat{Y}_i=X_i'\hb$为观测值$Y_i$的拟合值或预测值, 而$\hat{e}_i=Y_i-\hat{Y}_i$称为$Y_i$的OLS估计残差, 注意到
$$\hat{e}_i=Y_i-\hat{Y}_i=e_i-X_t'(\hb-\beta)$$
下一章将会证明: 随着样本容量$n$增大, $\hat{\beta}$依概率收敛于$\beta$, 从而上式的第二项可以小到忽略不计. 在推导OLS估计量时, 根据FOC还可知
$$(\X'\X)\hat{\beta}=\X'Y\Longleftrightarrow \X'(Y-X\hat{\beta})=\X'\hat{e}=0$$
若$X_i$包含截距项, 那么
$$\hat{e}_1+\hat{e}_2+\cdots+\hat{e}_n=0$$
注意, 这一性质是由最小化问题$\displaystyle\min_{\beta\in\R^K}\,\text{SSR}(\beta)$的FOC得到的, 不论严格外生性假设成立与否, 该正交条件总是成立的.

此外, 我们还可以使用矩估计的方法获得OLS估计量. 考虑线性投影模型
\begin{align}
Y&=X'\beta+e \nonumber\\
\beta&=\E[XX']^{-1}\E[XY] \label{eq3.19} \\
\E[Xe]&=0 \nonumber
\end{align}
既然线性投影系数使得MSE最小化, OLS估计量使得残差平方和最小化, 那么可以在(\ref{eq3.19})中用样本矩替代总体矩得到
$$\hb=\left(\frac{1}{n}\X'\X\right)^{-1}\left(\frac{1}{n}\X'Y\right)=(\X'\X)^{-1}\X'Y$$

\section{投影矩阵与消灭矩阵}
现在我们介绍投影矩阵与消灭矩阵的概念, 它们对于推导统计量的有限样本性质具有重要作用.
\begin{align*}
\mathbold{P}&=\X(\X'\X)^{-1}\X' \\
\mathbold{M}&=\mathbold{I}_n-\mathbold{P}
\end{align*}
它们具有以下良好的代数性质.
\begin{theorem}\label{thm:thm2.8}
  对于投影矩阵$\mathbold{P}$和消灭矩阵$\mathbold{M}$, 以下命题成立:

  (1) $\mathbold{PX}=\X$, $\mathbold{MX}=0$.

  (2) $\mathbold{P}\hat{e}=0$, $\mathbold{M}e=\M Y=\hat{e}$.

  (3) $\mathbold{P}^2=\mathbold{P}$, $\mathbold{M}^2=\mathbold{M}$.

  (4) $\mathbold{P}'=\mathbold{P}$, $\mathbold{M}'=\mathbold{M}$.

  (5) $\text{trace}\,(\mathbold{P})=K$, $\text{trace}\,(\mathbold{M})=n-K$.

  (6) $\text{SSR}(\beta)=e'\mathbold{M}e$.
\end{theorem}
\begin{proof}
  (1) $\mathbold{PX}=\X(\X'\X)^{-1}\X'\X=\X$, $\mathbold{MX}=\X-\X=0$.

  (2) 由$\X'\hat{e}=0$即可推知$\mathbold{P}\hat{e}=0$, 注意到$\hat{Y}=\mathbold{X}\hat{\beta}=\mathbold{P}Y$, 于是
  \begin{align*}
  \hat{e}&=Y-\hat{Y}=(\mathbold{I}_n-\mathbold{P})Y=\mathbold{M}Y \\
  &=\mathbold{M}(\X\beta+e)=\mathbold{MX}\beta+\mathbold{M}e=\mathbold{M}e
  \end{align*}

  (3) $\mathbold{P}^2=\X(\X'\X)^{-1}(\X'\X)(\X'\X)^{-1}\X'=\X(\X'\X)^{-1}\X'=\mathbold{P}$, 并且
  $$\mathbold{M}^2=(\mathbold{I}_n-\mathbold{P})^2=\mathbold{I}_n-2\mathbold{P}+\mathbold{P}^2=\mathbold{I}_n-\mathbold{P}=\mathbold{M}$$

  (4) 结论是显然的.

  (5) 根据矩阵的迹的性质可知$\text{trace}\,(\mathbold{P})=\text{trace}\,[\X(\X'\X)^{-1}\X']=\text{trace}\,[(\X'\X)^{-1}\X'\X]=\text{trace}\,(\mathbold{I}_K)=K$, 自然有$\text{trace}\,(\mathbold{M})=n-K$.

  (6) 根据$\hat{e}=\mathbold{M}e$可知
  $$\text{SSR}(\beta)=\hat{e}'\hat{e}=(\mathbold{M}e)'(\mathbold{M}e)=e'\mathbold{M}'\mathbold{M}e=e'\mathbold{M}e$$
  由此证得定理.
\end{proof}
特别地, 我们可以取$\X=\mathbf{1}_n$, 它是$n\times1$维元素全为1的列向量, 此时$\M=\mathbf{1}_n(\mathbf{1}_n'\mathbf{1}_n)^{-1}\mathbf{1}_n'$, 它可以简化某些求和计算.

\section{杠杆值与留一回归}
有时候单个异常值可能会对整个线性回归产生显著影响, 为了克服这种不利影响, 我们通常会使用留一 (Leave-One-Out, LOO)回归, 为此先介绍杠杆值 (leverage value)的概念.

首先定义回归矩阵$\X$的第$i$个杠杆值为投影矩阵$\mathbold{P}=\X(\X'\X)^{-1}\X'$的第$i$个对角线元素, 也即
$$h_{ii}=X_i'(\X'\X)^{-1}X_i',\quad 1\leq i\leq n$$
它是回归向量$X_i$的归一化长度, 它衡量了第$i$个观测值$X_i$相对于其它观测值的差异程度, $h_{ii}$越大说明$X_i$相对于其它样本值越不同. 我们定义
\begin{equation}\label{eq2.2}
  \overbar{h}=\max_{1\leq i\leq n}\,h_{ii}
\end{equation}
它是所有$h_{ii}$中的最大值, 衡量了总体中回归元的变异程度.

现在我们给出有关杠杆值的定理.
\begin{theorem}\label{thm:thm2.3}
  对于杠杆值$h_{ii}$, 以下命题成立:

  (1) $0\leq h_{ii}\leq1$.

  (2) 如果$X_i$包括截距项, 那么$h_{ii}\geq 1/n$.

  (3) $\sum_{i=1}^{n}h_{ii}=K$.
\end{theorem}
\begin{proof}
  (1) 显然$h_{ii}\geq0$. 定义$n\times1$维列向量$s_i$为
  $$s_i=[0,\cdots,1,\cdots,0]'$$
  它的第$i$个元素为1, 其它元素为0. 现在将$h_{ii}$改写为$h_{ii}=s_i'\mathbold{P}s_i$, 利用二次不等式\footnote{对任意$n\times1$维列向量$b$和$n\times n$维对称矩阵$\mathbold{A}$, $b'\mathbold{A}b\leq||\mathbold{A}||b'b$成立, 这里的$||\mathbold{A}||=[\lambda_{\max}(\mathbold{A}'\mathbold{A})]^{1/2}$, 表示$\mathbold{A}$的谱范数.}可知
  $$h_{ii}=s_i{'}\mathbold{P}s_i\leq s_i{'}s_i \lambda_{\max}(\mathbold{P})=1$$

  (2) 由于回归元$X_i$包含截距项, 故而可以将$X_i$分割为$X_i=[1,Z_i']'$, 不失一般性, 我们可以用$Z_i^\ast=Z_i-\overbar{Z}$替代$Z_i$, 于是
  \begin{align*}
  h_{ii}&=\begin{bmatrix}
             1 & Z_i^\ast
           \end{bmatrix}\begin{bmatrix}
                          n & 0 \\
                          0 & \mathbold{Z}^\ast{'}\mathbold{Z}^\ast
                        \end{bmatrix}^{-1}\begin{bmatrix}
                                            1 \\
                                            Z_i^\ast
                                          \end{bmatrix} \\
  &=\frac{1}{n}+Z_i^\ast{'}(\mathbold{Z}^\ast{'}\mathbold{Z}^\ast)^{-1}Z_i^\ast\geq\frac{1}{n}
  \end{align*}

  (3) 根据杠杆值的定义, 这是显然的.
\end{proof}
进一步, 我们用$\overbar{h}=\displaystyle\max_{1\leq i\leq n}\,h_{ii}$, 也即$h_{ii}$的最大值来衡量总体的变异程度, 如果一个回归设计是平衡的, 那么每一个$h_{ii}$大致应该相等, 在完全平衡的情况下, 根据定理\ref{thm:thm2.3}(3)可知$h_{ii}=\overbar{h}=k/n$.

反之, 若回归设计是不平衡的, 则不同的$h_{ii}$之间具有很强的差异性, 最极端的情况就是$\overbar{h}=1$: 举例而言, 回归元中包括虚拟变量, 并且该虚拟变量仅对唯一的一个观测值取1, 而对其它观测值都取0.

现在来看LOO回归, 它是利用$Y$对缺失第$i$个观测值的原样本进行的回归, 此时OLS估计量为
\begin{align*}
\hat{\beta}_{-i}&=\left(\sum_{j\neq i}X_jX_j'\right)^{-1}\left(\sum_{j\neq i}X_jY_j\right) \\
&=(\X'\X-X_iX_i')^{-1}(\X'Y-X_iY_i) \\
&=(\X_{-i}'\X_{-i})^{-1}\X_{-i}'Y_{-i}
\end{align*}
定义留一回归的预测值为$\tilde{Y}_i=X_i'\hat{\beta}_{-i}$, 预测误差为$\tilde{e}_i=Y_i-\tilde{Y}_i$, 有时它可以替代残差$\hat{e}_i$来作为$e_i$的估计量, 并且具有更好的性质. 由于$\hat{\beta}_{-i}$以及$\tilde{e}_i$的计算较为复杂, 下面给出一个简化其计算的定理.
\begin{theorem}
  LOO估计量和预测误差分别等价于
  \begin{equation}\label{eq2.3}
    \hat{\beta}_{-i}=\hb-(\X'\X)^{-1}X_i\tilde{e}_i
  \end{equation}
  以及
  \begin{equation}\label{eq2.4}
    \tilde{e}_i=(1-h_{ii})^{-1}\hat{e}_i
  \end{equation}
  其中$h_{ii}$为$\X$的第$i$个杠杆值.
\end{theorem}
\begin{proof}
  将LOO估计量写为
  $$(\X_{-i}'\X_{-i})^{-1}\X_{-i}'Y_{-i}$$
  上式左乘$(\X'\X)^{-1}(\X'\X-X_iX_i')$得到
  $$\hb_{-i}-(\X'\X)^{-1}X_iX_i'\hb_{-i}=(\X'\X)^{-1}(\X'Y-X_iY_i)=\hb-(\X'\X)^{-1}X_iY_i$$
  也即
  $$\hb_{-i}=\hb-(\X'\X)^{-1}X_i(Y_i-X_i'\hb_{-i})=\hb-(\X'\X)^{-1}X_i\tilde{e}_i$$
  也即(\ref{eq2.3})成立, 在(\ref{eq2.3})中左乘$X_i'$得到
  $$X_i'\hb_{-i}=X_i'\hb-X_i'(\X'\X)^{-1}X_i\tilde{e}_i=X_i\hb-h_{ii}\tilde{e}_i$$
  因此$\tilde{e}_i=\hat{e}_i+h_{ii}\tilde{e}_i$, 也即(\ref{eq2.4})成立.
\end{proof}

\section{拟合优度}
现在来分析线性回归模型对数据的拟合程度究竟是好是坏, 也即它对$\{Y_i\}$变动的预测能力如何, 为此我们需要设置出一些指标.
\begin{definition}[非中心化$R^2$]
$$R^2_{\text{uc}}=\frac{\hat{Y}'\hat{Y}}{Y'Y}=1-\frac{\hat{e}'\hat{e}}{Y'Y}$$
\end{definition}
它的含义是$\{Y_i\}$的非中心化样本二次型变动可以被预测值$\{\hat{Y}_i\}$的非中心化样本二次型变动所预测的比例, 根据定义可知始终有$0\leq R_{\text{uc}}^2\leq1$.
\begin{definition}[中心化$R^2$]
$$R^2=1-\frac{\sum_{i=1}^{n}\hat{e}_i^2}{\sum_{i=1}^{n}(Y_i-\overbar{Y})^2}$$
其中$\overbar{Y}=n^{-1}\sum_{i=1}^{n}Y_i$是样本均值, $R^2$又称可决系数.
\end{definition}
倘若我们将投影矩阵$\mathbold{P}=\X(\X'\X)^{-1}\X'$中的$\X$设置为$\mathbf{1}_n$, 于是
\begin{align*}
\mathbold{P}&=\mathbf{1}_n(\mathbf{1}_n'\mathbf{1}_n)^{-1}\mathbf{1}_n'=n^{-1}\mathbf{1}_n'\mathbf{1}_n \\
\mathbold{M}&=\mathbold{I}_n-\mathbf{1}_n(\mathbf{1}_n'\mathbf{1}_n)^{-1}\mathbf{1}_n'
\end{align*}
从而
\begin{align*}
\sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})^2&=\hat{Y}'\mathbold{M}\hat{Y},\quad\sum_{i=1}^{n}(Y_i-\overbar{Y})^2=Y'\mathbold{M}Y
\end{align*}
下面我们将给出关于可决系数$R^2$的一些定理.
\begin{theorem}\label{thm:thm2.1}
  如果$X_{i}$中包括截距项$X_{1i}=1$, 那么
  $$\sum_{i=1}^{n}(Y_i-\overbar{Y})^2=\sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})^2+\sum_{i=1}^{n}\hat{e}_i^2$$
\end{theorem}
\begin{proof}
  将$Y_i-\overbar{Y}$进行分解, 于是
  \begin{align*}
  \sum_{i=1}^{n}(Y_i-\overbar{Y})^2&=\sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y}+Y_i-\hat{Y}_i)^2 \\
  &=\sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})^2+\sum_{i=1}^{n}\hat{e}_i^2+2\sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})\hat{e}_i \\
  &=\sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})^2+\sum_{i=1}^{n}\hat{e}_i^2
  \end{align*}
  其中
  \begin{align*}
  \sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})\hat{e}_i&=\hat{\beta}'\sum_{i=1}^{n}X_i\hat{e}_i-\overbar{Y}\sum_{i=1}^{n}\hat{e}_i \\
  &=\hat{\beta}'(\X'\hat{e})-\overbar{Y}\sum_{i=1}^{n}\hat{e}_i=0
  \end{align*}
  由此证得定理.
\end{proof}
由定理\ref{thm:thm2.1}可知
$$R^2=\frac{\hat{Y}'\mathbold{M}_0\hat{Y}}{Y'\mathbold{M}_0Y}$$
由此可知$0\leq R^2\leq1$成立, 其中$\mathbold{M}_0=\mathbold{I}_n-\mathbf{1}_n(\mathbf{1}_n'\mathbf{1}_n)^{-1}\mathbf{1}_n'$. 如果$X_i$不包括截距项, 那么
$$\sum_{i=1}^{n}(Y_i-\overbar{Y})^2\neq \sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})^2+\sum_{i=1}^{n}\hat{e}_i^2$$
在这种情况下, $R^2$可能为负值.

当$X_i$包含截距项时, 中心化$R^2$和非中心化$R_{\text{uc}}^2$有相似的解释, 也即$R^2$测度$\{Y_i\}_{i=1}^n$的样本方差中可被线性拟合值$X_i'\hat{\beta}$所预测的比例.
\begin{theorem}
  设$\hat{\rho}_{Y\hat{Y}}$为$Y_i$和$\hat{Y}_i$间的相关系数, 则$R^2=\hat{\rho}_{Y\hat{Y}}^2$.
\end{theorem}
\begin{proof}
  要证$R^2=\hat{\rho}_{Y\hat{Y}}^2$, 只需证明
  $$\sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})^2=\sum_{i=1}^{n}(\hat{Y}_i-\overbar{Y})(Y_i-\overbar{Y})$$
  也即证明
  $$\hat{Y}'\mathbold{M}_0\hat{Y}=\hat{Y}'\mathbold{M}_0Y$$
  根据$Y=\hat{Y}+\hat{e}$以及$\hat{Y}'e=0$可知
  \begin{align*}
  \text{RHS}&=\hat{Y}'\mathbold{M}_0Y\\
  &=\hat{Y}'\mathbold{M}_0(\hat{Y}+e) \\
  &=\hat{Y}'\mathbold{M}_0\hat{Y}+\hat{Y}'e=\text{LHS}
  \end{align*}
  由此证得定理.
\end{proof}
\begin{theorem}
  随着解释变量个数的增加, $R^2$是不减的.
\end{theorem}
\begin{proof}
  考虑以下两个OLS回归模型
  \begin{align*}
  Y&=\X_1\tilde{\beta}+\tilde{e} \\
  Y&=\X_1\hat{\beta}_1+\X_2\hat{\beta}_2+\hat{e}
  \end{align*}
  我们只需证明$\hat{e}'\hat{e}\leq\tilde{e}'\tilde{e}$即可. 注意到
  $$\X_1'\tilde{e}=\X_1'\hat{e}=\X_2'\hat{e}=0$$
  于是
  $$\tilde{e}'\hat{e}=(Y-\X_1\tilde{\beta})'\hat{e}=(\X_1\hat{\beta}_1+\X_2\hb_2+\hat{e})'\hat{e}=\hat{e}'\hat{e}$$
  因此
  $$\hat{e}'\hat{e}-\tilde{e}'\tilde{e}=-(\tilde{e}-\hat{e})'(\tilde{e}-\hat{e})\leq0$$
  由此证得定理.
\end{proof}
这表明, 随着解释变量个数增加, 拟合优度不会变差, 因此若我们添加了一些无关变量, 它对因变量没有真正的预测能力, 而$R^2$不会对这一行为有任何惩罚. 另一方面, 随着解释变量个数增加, 模型的待估参数越多, 其中可能包含一些数据中不太可能再出现的因素, 因此模型对样本外的$Y_i$预测能力越差.

现在我们定义调整$R^2$为
$$\overbar{R}^2=1-\frac{e'e/(n-K)}{(n-1)^{-1}\sum_{i=1}^{n}(Y_i-\overbar{Y})^2}$$
它可以对模型中的冗余变量进行惩罚, 也即添加冗余变量会使得调整$R^2$下降. 可以证明
$$\overbar{R}^2=1-\frac{n-1}{n-K}(1-R^2)$$
因此即使$X_i$中包括截距项, 调整$R^2$也可能为负值.

最后, 我们应该明确无论是$R^2$还是调整$R^2$, 它反映的只是统计上的相关性, 而与因果关系无关. 它只衡量模型拟合程度的好坏, 但是无法衡量回归元对响应变量的解释能力的好坏. 事实上, 对于截面数据和面板数据而言, $R^2$很少有超过0.2的.
\section{OLS的有限样本性质}
\begin{theorem}\label{thm:thm2.2}
  在假设\ref{pro:pro2.1}$-$\ref{pro:pro2.4}成立的条件下, 那么OLS估计量满足:

  (1) 无偏性: $\E[\hat{\beta}|\X]=\beta$, 以及$\E[\hb]=\beta$.

  (2) 方差: $\var(\hb|\X)=\sigma^2(\X'\X)^{-1}$.

  (3) 无相关性: $\text{cov}(\hb,\hat{e}|\X)=0$.
\end{theorem}
\begin{proof}
  (1) 根据$\hat{\beta}=(\X'\X)^{-1}\X'Y$可知$\hat{\beta}-\beta=(\X'\X)^{-1}\X'e$, 于是
  \begin{align*}
  \E[\hb-\beta|\X]&=\E[(\X'\X)^{-1}\X'e|\X] \\
  &=(\X'\X)^{-1}\X'\E[e|\X]=0
  \end{align*}
  也即$\E[\hb|\X]=\beta$, 根据LIE可知无条件期望$\E[\hat{\beta}]=\beta$.

  (2) 由于$\beta$是真实参数, 故而$\var(\hat{\beta}|\X)=\text{var}(\hat{\beta}-\beta|\X)$, 于是
  \begin{align*}
  \var(\hb|\X)&=(\X'\X)^{-1}\X'\var(e|\X)\X(\X'\X)^{-1}=(\X'\X)^{-1}\X'\mathbold{\Sigma}\X(\X'\X)^{-1}
  \end{align*}
  在假设\ref{pro:pro2.4}成立的条件时有$\mathbold{\Sigma}=\sigma^2\mathbold{I}_n$, 此时
  \begin{align*}
  \var(\hb|\X)&=\sigma^2(\X'\X)^{-1}\X'\mathbold{I}_n\X(\X'\X)^{-1} \\
  &=\sigma^2(\X'\X)^{-1}
  \end{align*}

  (3) 首先有$\E[\hat{e}|\X]=\M\E[e|\X]=0$, 再根据协方差的定义可知
  \begin{align*}
  \text{cov}(\hb,\hat{e}|\X)&=\E\left[\left.\{\hb-\E[\hb|\X]\}\{\hat{e}-\E[\hat{e}|\X]\}'\right|\X\right] \\
  &=\E[(\hb-\beta)\hat{e}'|\X] \\
  &=\E[(\X'\X)^{-1}\X'ee'\M|\X] \\
  &=(\X'\X)^{-1}\X'\E[ee'|\X]\mathbold{M}=0
  \end{align*}
  其中$\E[ee'|\X]=\sigma^2\mathbold{I}_n$和$\mathbold{M}'\X=0$保证了上式成立.
\end{proof}
现在介绍Gauss-Markov定理, 它表明在$\beta$的所有线性无偏估计量中, OLS估计量是最优无偏估计量 (Best Linear Unbiased Estimator, BLUE).
\begin{theorem}[Gauss-Markov定理]\label{thm:thm2.7}
  在假设\ref{pro:pro2.1}$-$\ref{pro:pro2.4}下, 如果$\tilde{\beta}$是$\tilde{\beta}$的线性无偏估计量, 那么$\var(\hb|\X)\geq\sigma^2(\X'\X)^{-1}$.
\end{theorem}
\begin{proof}
  设$\tilde{\beta}=\mathbold{A}Y$为$\beta$的任意线性估计量, 由无偏性可知
  $$\E[\tilde{\beta}|\X]=\mathbold{A}'\X\beta+\mathbold{A}'\E[e|\X]=\mathbold{A}'\X\beta=\beta$$
  因此必有$\mathbold{A}'\X=\mathbold{I}_K$.

  另一方面
  \begin{align*}
  \tilde{\beta}=\mathbold{A}'Y=\mathbold{A}'(\X\beta+e)=\beta+\mathbold{A}'e
  \end{align*}
  从而$\tilde{\beta}$的条件方差为
  $$\var(\tilde{\beta}|\X)=\E[(\tilde{\beta}-\beta)(\tilde{\beta}-\beta)'|\X]=\sigma^2\mathbold{A}'\mathbold{A}$$
  根据定理\ref{thm:thm2.2}(2)和$\mathbold{A}'\X=\mathbold{I}_K$可知
  \begin{align*}
  \var(\tilde{\beta}|\X)-\var(\hb|\X)&=\sigma^2[\mathbold{A}'\mathbold{A}-(\X'\X)^{-1}] \\
  &=\sigma^2\mathbold{A}'[\mathbold{I}_K-\X(\X'\X)^{-1}\X']\mathbold{A} \\
  &=\sigma^2(\mathbold{MA})'(\mathbold{MA})
  \end{align*}
  它是一个半正定矩阵, 定理得证.
\end{proof}
\begin{remark}
Hansen (2022b)提出了一个现代Gauss-Markov定理, 去掉了$\hat{\beta}$为线性估计量这一限制, 证明OLS估计量为BUE. 实际上, 该定理与经典Gauss-Markov定理并无本质差别. Portnoy (2022)的结论表明, 估计量的无偏性意味着它是线性的, 因此BUE和BLUE对于OLS而言是一回事.
\end{remark}

\begin{theorem}\label{thm:thm2.4}
  定义残差方差估计量
  $$s^2=\hat{e}'\hat{e}/(n-K)=\frac{1}{n-K}\sum_{i=1}^{n}\hat{e}_i^2$$
  那么在假设\ref{pro:pro2.1}$-$\ref{pro:pro2.4}下, $\E[s^2|\X]=\sigma^2$, 其中$\sigma^2=\E[e_i^2]$.
\end{theorem}
\begin{proof}
  根据$\hat{e}'\hat{e}=e'\mathbold{M}e$, 以及$\text{trace}\,(\mathbold{AB})=\text{trace}\,(\mathbold{BA})$可知
  \begin{align*}
  \E[\hat{e}'\hat{e}|\X]&=\E[e'\mathbold{M}e|\X]=\E[\text{trace}\,(e'\M e)|\X] \\
  &=\E[\text{trace}\,(ee'\M)|\X]=\text{trace}\,(\E[ee'|\X]\M) \\
  &=\sigma^2\text{trace}\,(\M)=\sigma^2(n-K)
  \end{align*}
  因此
  $$\E[s^2|\X]=\frac{1}{n-K}\E[\hat{e}'\hat{e}|\X]=\sigma^2$$
  由此证得定理.
\end{proof}
\section{OLS的协方差矩阵估计}
为了进行后续章节的统计推断, 我们需要得到条件方差$\var(\hb|\X)$的估计量. 首先根据$\hat{e}=\M e$与$\M=\mathbold{I}_n-\X(\X'\X)^{-1}\X'$得到
$$\var(\hat{e}|\X)=\var(\M e|\X)=\M\mathbold{\Sigma}\M$$
在同方差条件下
$$\var(\hat{e}|\X)=\sigma^2\M$$
并且
$$\var(\hat{e}_i|\X)=\E[\hat{e}_i|\X]=(1-h_{ii})\sigma^2$$

进一步, 我们定义$\M^\ast$为对角矩阵, 其第$i$个对角元素为$(1-h_{ii})^{-1}$. 再定义标准化残差
$$\overbar{e}_i=(1-h_{ii})^{-1/2}\hat{e}_i$$
用向量可以表示为
$$\overbar{e}=[\overbar{e}_1,\overbar{e}_2,\cdots,\overbar{e}_n]=\M^{\ast 1/2}\M e$$
在同方差条件下
\begin{align*}
\var(\overbar{e}|\X)&=\sigma^2\M^{\ast 1/2}\M\M^{\ast 1/2} \\
\var(\overbar{e}_i|\X)&=\sigma^2
\end{align*}
因此残差方差估计量$s^2$可以写为
$$s^2=\frac{1}{n}\sum_{i=1}^{n}\overbar{e}_i^2=\frac{1}{n}\sum_{i=1}^{n}(1-h_{ii})^{-1}\hat{e}_i^2$$

根据定理\ref{thm:thm2.4}, 我们可以得到OLS估计量协方差矩阵$\var(\hb|\X)$的估计量
$$\hat{\mathbold{V}}_{\text{OLS}}=s^2(\X'\X)^{-1}$$
并且它是无偏的, 因为
$$\E[\hat{\mathbold{V}}_{\text{OLS}}|\X]=\E[s^2|\X](\X'\X)^{-1}=\sigma^2(\X'\X)^{-1}$$

倘若同方差假设不成立, 那么$\mathbold{\Sigma}\neq \sigma^2\mathbold{I}_n$, 而是等于
$$\mathbold{\Sigma}=\E[ee'|\X]=\text{diag}\,\{\sigma_1^2,\sigma_2^2,\cdots,\sigma_n^2\}$$
并且
$$\var(\hb|\X)=(\X'\X)^{-1}(\X'\mathbold{\Sigma}\X)(\X'\X)^{-1}$$
由于$\E[e_i^2|\X]=\sigma_i^2$, 因此$\mathbold{V}_{\text{OLS}}$的理想无偏估计量为
$$\hat{\mathbold{V}}_{\text{OLS}}^\text{ideal}=(\X'\X)^{-1}\left(\sum_{i=1}^{n}X_iX_i'e_i^2\right)(\X'\X)^{-1}$$
然而$e_i$不可观测, 我们无法得到$\hat{\mathbold{V}}_{\text{OLS}}^\text{ideal}$.

自然而然地, 我们考虑使用残差$\hat{e}_i$替代$e_i$, 由此可以得到$\mathbold{V}_{\text{OLS}}$的一个估计量
$$\hat{\mathbold{V}}_{\text{OLS}}^\text{HC0}=(\X'\X)^{-1}\left(\sum_{i=1}^{n}X_iX_i'\hat{e}_i^2\right)(\X'\X)^{-1}$$
然而该统计量未经过自由度调整, 一个更好的估计量是
\begin{align*}
\hat{\mathbold{V}}_{\text{OLS}}^\text{HC1}&=\frac{n}{n-K}(\X'\X)^{-1}\left(\sum_{i=1}^{n}X_iX_i'\hat{e}_i^2\right)(\X'\X)^{-1}
\end{align*}
另外, 我们可以用$\overbar{e}_i$和$\tilde{e}_i$分别替换$\hat{\mathbold{V}}_{\text{OLS}}^\text{ideal}$中的$e_i$, 由此得到形式类似的估计量$\hat{\mathbold{V}}_{\text{OLS}}^\text{HC2}$和$\hat{\mathbold{V}}_{\text{OLS}}^\text{HC3}$, 也即
\begin{align*}
\hat{\mathbold{V}}_{\text{OLS}}^\text{HC2}&=(\X'\X)^{-1}\left(\sum_{i=1}^{n}X_iX_i'\overbar{e}_i^2\right)(\X'\X)^{-1} \\
\hat{\mathbold{V}}_{\text{OLS}}^\text{HC3}&=(\X'\X)^{-1}\left(\sum_{i=1}^{n}X_iX_i'\tilde{e}_i^2\right)(\X'\X)^{-1}
\end{align*}
以上估计量均称为异方差一致性稳健协方差矩阵估计量.

\section{分块回归}
首先将$\X$分割为$\X=[\X_1,\X_2]$, 再将$\beta$分割为$\beta=[\beta_1',\beta_2']'$, 于是回归模型可以写作
\begin{equation}\label{eq2.17}
  Y=\X_1\beta_1+\X_2\beta_2+e
\end{equation}
$\beta$的OLS估计量可以由$Y$对$\X=[\X_1,\X_2]$的回归获得, 此时
\begin{equation}
  Y=\X\hb+e=\X_1\hb_1+\X_2\hb_2+e
\end{equation}
为了得到$\hb=[\hb_1',\hb_2']'$的表达式, 考虑对(\ref{eq1.24})进行矩估计, 由此得到
$$\hb=\begin{bmatrix}
        \hb_1 \\
        \hb_2
      \end{bmatrix}=\begin{bmatrix}
                      \hat{\Q}_{11\cdot2}^{-1}\hat{\Q}_{1Y\cdot2} \\
                      \hat{\Q}_{22\cdot1}^{-1}\hat{\Q}_{2Y\cdot1}
                    \end{bmatrix}$$
其中
\begin{align*}
\hat{\Q}_{11\cdot2}&=\hat{\Q}_{11}-\hat{\Q}_{12}\hat{\Q}_{22}^{-1}\hat{\Q}_{21} \\
&=\frac{1}{n}\X_1'\X_1-\frac{1}{n}\X_1'\X_2\left(\frac{1}{n}\X_2'\X_2\right)^{-1}\frac{1}{n}\X_2'\X_1 \\
&=\frac{1}{n}\X_1'\mathbold{M}_2\X_1
\end{align*}
以及$\mathbold{M}_2=\mathbold{I}_n-\X_2(\X_2'\X_2)^{-1}\X_2'$, 同理可得
$\displaystyle\hat{\Q}_{1Y\cdot2}=\frac{1}{n}\X_1\mathbold{M}_2Y$. 另一方面
\begin{align*}
\hat{\Q}_{22\cdot1}&=\frac{1}{n}\X_2'\mathbold{M}_1\X_2 \\
\hat{\Q}_{2Y\cdot1}&=\frac{1}{n}\X_2'\mathbold{M}_1Y \\
\mathbold{M}_1&=\mathbold{I}_n-\X_1(\X_1'\X_1)^{-1}\X_1'
\end{align*}
因此可得OLS估计量
\begin{align}
\hb_1&=(\X_1'\M_2\X_1)^{-1}\X_1'\M_2Y \label{eq2.15} \\
\hb_2&=(\X_2'\M_1\X_2)^{-1}\X_2'\M_1Y \label{eq2.16}
\end{align}

下面我们介绍著名的FWL定理, 它由Frisch, Waugh和Lovell提出, 该定理给出了计算$\hb_1$和$\hb_2$的另一种方法. 考虑OLS估计量(\ref{eq2.16}), 注意到$\M_1$为对称幂等矩阵, 于是
\begin{align*}
\hb_2&=(\X_2'\M_1\X_2)^{-1}\X_2'\M_1Y \\
&=(\X_2'\M_1'\M_1\X_2)^{-1}\X_2'\M_1'\M_1Y \\
&=(\tilde{\X}_2'\tilde{\X}_2)^{-1}\tilde{\X}_2'\tilde{e}_1
\end{align*}
其中$\tilde{\X}_2=\M_1\X_2$, 以及$\tilde{e}_1=\M_1Y$. 根据定理\ref{thm:thm2.8}(2)可知, $\tilde{\X}_2$是$\X_2$对$\X_1$回归获得的残差, $\tilde{e}_1$是$Y$对$\X_1$回归的残差.

\begin{theorem}[FWL定理]
  在回归模型(\ref{eq2.17})中, $\beta_2$的OLS估计量可由以下步骤获得:

  STEP 1: 使用$\X_2$对$\X_1$回归, 获得残差$\tilde{e}_1$;

  STEP 2: 使用$Y$对$\X_1$回归, 获得残差$\tilde{\X}_2$;

  STEP 3: 使用$\tilde{\X}_2$对$\tilde{e}_1$回归, 获得OLS估计量$\hb_2$及残差$\hat{e}$.
\end{theorem}
\begin{remark}
类似地, $\X_1$的回归系数$\hb_1$也可以通过$\tilde{e}_2$对$\tilde{\X}_1$回归得到, 这里的$\tilde{e}_2$是$\X_1$对$\X_2$回归的残差, 而$\tilde{\X}_1$是$Y$对$\X_2$回归的残差.
\end{remark}

FWL定理表明, $\X_2$的回归系数$\hb_2$表示的是“过滤掉$\X_1$影响的$\X_2$”对“过滤掉$\X_1$影响的$Y$”的作用, $\hb_1$也可以做类似解释.

\section{正态假设下的参数检验}
为了检验关于OLS估计量的相关假设, 我们需要构建OLS估计量在有限样本下的参数检验统计量, 在此之前还需要假定随机扰动项服从条件正态分布.
\begin{proposition}[正态扰动项]\label{pro:pro2.5}
$e|\X\sim N(0,\sigma^2\mathbold{I}_n)$.
\end{proposition}

\begin{remark}
事实上, 假设\ref{pro:pro2.5}可以推出假设\ref{pro:pro2.2}和\ref{pro:pro2.4}成立, 因此这是个很强的假定.
\end{remark}

不仅如此, 我们还需要知道$\hat{\beta}$和$s^2$的抽样分布, 这就要求先得到$s^2$的统计性质, 为此先介绍一个引理.
\begin{lemma}\label{lem:lem2.1}
设$m\times1$维随机向量$v\sim N(0,\mathbold{I}_m)$, $\mathbold{A}$是一个$m\times m$维非随机对称幂等矩阵, $\text{rank}\,(\mathbold{A})=q\leq m$, 那么二次型$v'\mathbold{A}v\sim \chi^2_q$.
\end{lemma}
\begin{proof}
  因为$\mathbold{A}$是实对称阵, 故而存在正交矩阵$\mathbold{Q}$, 使得$\mathbold{Q}'\mathbold{AQ}=\mathbold{\Lambda}$, 其中$\mathbold{\Lambda}$为对角矩阵, 其对角元素均为$\mathbold{A}$的特征值. 由于$\mathbold{A}$又是幂等矩阵, 故而其特征值只要0和1, 因为$\text{rank}\,(\mathbold{A})=q$, 不妨设$\mathbold{\Lambda}$对角线上的前$q$个元素为1, 其余元素为0.

  由于$\mathbold{Q}$是正交矩阵, 其各行各列都是单位向量且两两正交, 故而
  $$p=\mathbold{Q}'v\sim N(0,\mathbold{I}_n)$$
  因此二次型
  $$v'\mathbold{A}v=p'\mathbold{\Lambda}p\sim\chi^2_q$$
  由此证得引理.
\end{proof}
\begin{theorem}\label{thm:thm2.5}
  设$s^2$为$\sigma^2$的残差方差估计量, 那么在假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和\ref{pro:pro2.5}下:

  (1) $\displaystyle \left.\frac{(n-K)s^2}{\sigma^2}\right|\X=\left.\frac{\hat{e}'\hat{e}}{\sigma^2}\right|\X\sim\chi_{n-K}^2$.

  (2) 在给定$\X$时, $s^2$和$\hb$独立.
\end{theorem}
\begin{proof}
  (1) 根据$\hat{e}'\hat{e}=e\M e$可知
  $$\frac{\hat{e}'\hat{e}}{\sigma^2}=\frac{e\M e}{\sigma^2}=\left(\frac{e}{\sigma}\right)'\M\left(\frac{e}{\sigma}\right)$$
  在假设\ref{pro:pro2.5}下, $\displaystyle \left.\frac{e}{\sigma}\right|\X\sim N(0,\mathbold{I}_n)$, 又因为$\M$是一个秩为$n-K$的幂等矩阵, 根据引理\ref{lem:lem2.1}可知(1)成立.

  (2) 注意到$s^2=\hat{e}'\hat{e}/(n-K)$, 只需证明$\hat{e}$与$\hb$相互独立即可. 由于
  \begin{equation}\label{eq2.5}
  \begin{bmatrix}
      \hat{e} \\
      \hb-\beta
    \end{bmatrix}=\begin{bmatrix}
                    \M \\
                    (\X'\X)^{-1}\X'
                  \end{bmatrix}e
  \end{equation}
  根据$e|\X\sim N(0,\sigma^2\mathbold{I}_n)$, 上式在给定$\X$的条件下也服从正态分布, 由于联合正态分布的不相关意味着独立性, 根据定理\ref{thm:thm2.2}(3)可知$\hat{e}$与$\hat{\beta}$相互独立.
\end{proof}

现在来构建参数假设检验, 考虑如下线性假设
$$\mathbold{R}\beta=r$$
其中$\mathbold{R}$为$J\times K$维选择矩阵, $r$是$J\times1$维列向量, $J$是参数的限制条件数目.

\begin{example}
考虑线性回归模型
$$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+\beta_4x_4+e$$
我们想检验原假设$\HH_0: \beta_2=\beta_3+1$, $\beta_4=0$, 那么
$$\mathbold{R}=\begin{bmatrix}
                 0 & 1 & -1 & 0 \\
                 0 & 0 & 0 & 1
               \end{bmatrix},\quad r=\begin{bmatrix}
                                       1 \\
                                       0
                                     \end{bmatrix}$$
\end{example}
\begin{lemma}
在假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和\ref{pro:pro2.5}下, 对于任意非随机$J\times K$维矩阵$\mathbold{R}$都有
$$\mathbold{R}(\hb-\beta)|\X\sim N(0,\sigma^2\mathbold{R}(\X'\X)^{-1}\mathbold{R}')$$
\end{lemma}
\begin{proof}
  在给定$\X$的条件下, $\hb-\beta$是$e$的线性组合, 因此也服从正态分布. 又因为
  $$\E[\RH(\hb-\beta)|\X]=\RH\E[\hb-\beta|\X]=0$$
  以及
  $$\var[\RH(\hb-\beta)|\X]=\RH'\var(\hb|\X)\RH'=\sigma^2\RH(\X'\X)^{-1}\RH'$$
  因此$\mathbold{R}(\hb-\beta)|\X\sim N(0,\sigma^2\mathbold{R}(\X'\X)^{-1}\mathbold{R}')$.
\end{proof}
\begin{corollary}\label{cor:cor2.1}
在假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和\ref{pro:pro2.5}下, 当原假设$\HH_0:\RH\beta=r$成立时有
$$(\RH\hb-r)|\X\sim N(0,\sigma^2\mathbold{R}(\X'\X)^{-1}\mathbold{R}')$$
\end{corollary}
\subsection{\emph{T}检验}
我们先考虑$J=1$的情况, 也即单个约束条件的情况.
\begin{theorem}[$T$统计量的分布]\label{thm:thm3.8}
  在假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和\ref{pro:pro2.5}下, 若$J=1$, 如果原假设$\HH_0:\RH\beta=r$成立, 那么$T$统计量
  \begin{equation}\label{eq2.6}
    T=\frac{\RH\hb-r}{\sqrt{s^2\RH(\X'\X)^{-1}\RH'}}\sim t_{n-K}
  \end{equation}
\end{theorem}
\begin{proof}
  根据推论\ref{cor:cor2.1}, 在原假设$\HH_0:\RH\beta=r$成立的情况下
  $$(\RH\hb-r)|\X\sim N(0,\sigma^2\mathbold{R}(\X'\X)^{-1}\mathbold{R}')$$
  从而在给定$\X$时有
  $$\frac{\RH\hb-r}{\sqrt{\sigma^2\RH(\X'\X)^{-1}\RH'}}\sim N(0,1)$$
  将(\ref{eq2.7})进行等价变换得
  $$T=\frac{\RH\hb-r}{\sqrt{s^2\RH(\X'\X)^{-1}\RH'}}=\frac{\displaystyle\frac{\RH\hb-r}{\sqrt{\sigma^2\RH(\X'\X)^{-1}\RH'}}}{\displaystyle \sqrt{ \frac{(n-K)s^2}{\sigma^2}/(n-K)}}$$
  根据定理\ref{thm:thm2.5}, 由$t$分布的定义可知
  $$T\sim \frac{N(0,1)}{\sqrt{\chi^2_{n-K}/(n-K)}}\sim t_{n-K}$$
\end{proof}
\begin{remark}
事实上, 定理\ref{thm:thm3.8}是一个相当一般的形式, $T$检验并不局限于检验单个系数, 它的本质实际上是单个约束条件下的检验. 如果考虑特殊的原假设$\HH_0: \hat{\beta}_j=\beta_j$, 那么对应的$T$统计量为
$$T=\frac{\hat{\beta}_j-\beta_j}{\sqrt{s^2(\X'\X)_{jj}^{-1}}}$$
这里的$\beta_j$是我们预设的想要检验的真实参数值, 上式的分母称为估计量$\hb_j$的标准误, 其中$(\X'\X)_{jj}^{-1}$是矩阵$(\X'\X)^{-1}$主对角线上的第$(j,j)$个元素.
\end{remark}

现在来看$T$检验的步骤. 给定显著性水平$\alpha\in(0,1)$, $|T|>C_{T_{n-K},\frac{\alpha}{2}}$, 则拒绝原假设$\HH_0:\RH\beta=r$. 其中, $C_{T_{n-K},\frac{\alpha}{2}}$是$T_{n-K}$分布在$\alpha/2$水平上的右侧临界值. 用概率的形式可以表述为
$$\PP[|T_{n-K}|>C_{T_{n-K},\frac{\alpha}{2}}]=\alpha$$
反之若$|T|\leq C_{T_{n-K},\frac{\alpha}{2}}$, 则不拒绝原假设$\HH_0:\RH\beta=r$. 注意, 我们通常不说“接受原假设$\HH_0$”, 因为即使$T$检验统计量不超过对应的临界值, 它也并非是支持$\HH_0$为真的证据, 只是说没有充分的证据拒绝$\HH_0$为真.

当我们在检验$\HH_0$时, 样本的有限性意味着它包含的总体信息也是有限的, 因此可能存在两种错误: I类错误为原假设为真但被拒绝, 显著性水平$\alpha$就是犯第I类错误的概率, 也即
$$\PP[|T|>C_{t_{n-K},\frac{\alpha}{2}}|\HH_0]=\alpha$$
如果$\alpha$显著性水平的功效函数满足
$$\PP[|T|>C_{t_{n-K},\frac{\alpha}{2}}|\HH_1]<1$$
则存在$\HH_0$为假时被接受的可能性, 称为第II类错误. 其中$\HH_1$为$\HH_0$的备择假设.

理想的情形是同时最小化I类错误和II类错误, 然而对于有限样本而言, 这几乎是不可能完成的. 在实际应用中, 我们通常事先设定I类错误的水平, 即显著性水平$\alpha$, 通常将它选为$10\%$, $5\%$或$1\%$.

注意, 如果模型中存在近似多重共线性, 则随着样本容量$n$增大, $\var(\hb|\X)$并不趋近于0, $T$统计量不显著的可能性增大, 因此若原假设$\HH_0$为假, 则我们可能会接受它. 换言之, 近似多重共线性会影响$T$检验的II类错误.

除了使用基于$T$检验统计量的方法外, 我们还可以使用基于p值的判断法则, p值是可以拒绝原假设$\HH_0$的最小显著性水平$\alpha$, 并且Stata之类的软件可以直接给出p值. 举例而言, 如果$p=0.11$, 那么我们可以在$\alpha>0.11$的水平下拒绝$\HH_0$, 但是无法在$\alpha<0.11$的水平下拒绝$\HH_0$. 相较于统计量的临界值, p值不仅告诉人们是否应该在某一显著性水平下拒绝$\HH_0$, 而且还能告诉人们拒绝或不拒绝的程度有多大.

\subsection{\emph{F}检验}
现在来考虑线性约束个数$J>1$的情况, 为了构造合适的检验统计量, 这里先给出另一个引理.
\begin{lemma}\label{lem:lem2.2}
设$q\times 1$随机向量$Z\sim N(0,\mathbold{V})$, 其中$\mathbold{V}=\var(Z)$是一个$q\times q$维可逆对称的协方差矩阵, 则$Z'\mathbold{V}^{-1}Z\sim \chi^2_q$.
\end{lemma}
\begin{proof}
  因为$\V$的对称正定的, 因此存在可逆对称矩阵$\V^{1/2}$使得
  \begin{align*}
  \V^{1/2}\V^{1/2}&=\V \\
  \V^{-1/2}\V^{-1/2}&=\V^{-1}
  \end{align*}
  现在定义随机变量$Y=\V^{-1/2}Z$, 则$\E[Y]=0$, 以及
  \begin{align*}
  \var(Y)&=\E[YY']=\V^{-1/2}\E[ZZ']\V^{-1/2} \\
  &=\V^{-1/2}\V\V^{-1/2}=\mathbold{I}_q
  \end{align*}
  从而$Y\sim N(0,\mathbold{I}_q)$, $Y_1,Y_2,\cdots,Y_q$为相互独立的标准正态分布, 于是
  $$Z'\V^{-1}Z=Y'Y=\sum_{i=1}^{q}Y_i^2\sim \chi_q^2$$
  由此证得引理.
\end{proof}
\begin{theorem}[$F$统计量的分布]
  在假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和\ref{pro:pro2.5}下, 若$J>1$, 如果原假设$\HH_0:\RH\beta=r$成立, 那么$F$统计量
  \begin{equation}\label{eq2.7}
    F=\frac{(\RH\hb-r)'[\RH(\X'\X)^{-1}\RH']^{-1}(\RH\hb-r)/J}{s^2}\sim F_{J,n-K}
  \end{equation}
\end{theorem}
\begin{proof}
   根据推论\ref{cor:cor2.1}, 在原假设$\HH_0:\RH\beta=r$成立的情况下
  $$(\RH\hb-r)|\X\sim N(0,\sigma^2\mathbold{R}(\X'\X)^{-1}\mathbold{R}')$$
  由引理\ref{lem:lem2.2}可知
  $$\frac{(\RH\hb-r)'[\RH(\X'\X)^{-1}\RH']^{-1}(\RH\hb-r)}{\sigma^2}\sim\chi_J^2$$
  将(\ref{eq2.7})等价变换为
  $$F=\frac{\displaystyle \frac{(\RH\hb-r)'[\RH(\X'\X)^{-1}\RH']^{-1}(\RH\hb-r)}{\sigma^2}/J}{\displaystyle\frac{(n-K)s^2}{\sigma^2}/(n-K)}$$
  根据定理\ref{thm:thm2.5}得到
  $$F\sim\frac{\chi^2_J/J}{\chi^2_{n-K}/(n-K)}\sim F_{J,n-K}$$
  由此证得定理.

\end{proof}
\begin{remark}
当$J=1$时, $F_{1,n-K}=t_{1,n-K}$, 因此$T$检验和$F$检验是等价的.
\end{remark}
$F$检验的步骤同$T$检验类似但略有不同, $T$检验属于双边检验, 在我们预设了显著性水平$\alpha$后, 计算的临界值以$\alpha/2$为基准. 而$F$检验为单边检验, 首先计算出$F_{J,n-K}$在$\alpha$分位点对应的临界值$C_{F_{J,n-K},\alpha}$, 也即
$$\PP[F>C_{F_{J,n-K},\alpha}]=\alpha$$
如果$F$的值大于临界值$C_{F_{J,n-K},\alpha}$, 那么在显著性水平$\alpha$下拒绝原假设$\HH_0$, 否则不拒绝它.

由于按照定义计算$F$统计量比较麻烦, 我们在本节的最后部分给出一个更方便的方法来计算它.
\begin{theorem}
  给定假设\ref{pro:pro2.1}和\ref{pro:pro2.3}, 令$\hat{e}'\hat{e}$为以下无约束回归模型的残差平方和
  $$Y=\X\beta+e$$
  再令$\tilde{e}'\tilde{e}$为以下有约束回归模型的残差平方和
  \begin{align*}
  Y&=\X\beta+e \\
  \text{s.t. }\RH\beta&=r
  \end{align*}
  这里$\tilde{e}=Y-\X\tilde{\beta}$, 而$\tilde{\beta}$是有约束回归模型的OLS估计量. 那么$F$统计量可以写为
  $$F=\frac{(\tilde{e}'\tilde{e}-\hat{e}'\hat{e})/J}{\hat{e}'\hat{e}/(n-K)}$$
\end{theorem}
\begin{proof}
  设$\tilde{\beta}$是原假设$\HH_0:\RH\beta=r$成立时有约束模型的OLS估计量, 即
  $$\tilde{\beta}=\arg\min_{\beta\in\R^K}\,(Y-\X\beta)'(Y-\X\beta)$$
  现在构建Lagrange函数
  $$L(\beta,\lambda)=(Y-\X\beta)'(Y-\X\beta)-2\lambda'(r-\RH\beta)$$
  其中$\lambda$为$J\times1$维Lagrange乘子向量. 根据约束最优化的FOC可知
  \begin{align*}
  \frac{\partial L(\tilde{\beta},\tilde{\lambda})}{\partial\beta}&=-2\X'(Y-\X\tilde{\beta})+2\RH'\tilde{\lambda}=0 \\
  \frac{\partial L(\tilde{\beta},\tilde{\lambda})}{\partial\lambda}&=r-\RH\tilde{\beta}=0
  \end{align*}
  由于无约束模型的OLS估计量为$\hb=(\X'\X)^{-1}\X'Y$, 于是
  \begin{align*}
  \hat{\beta}-\tilde{\beta}&=(\X'\X)^{-1}\RH'\tilde{\lambda} \\
  \RH(\X'\X)^{-1}\RH'\tilde{\lambda}&=\RH(\hb-\tilde{\beta})
  \end{align*}
  因此Lagrange乘子可以表示为
  $$\tilde{\lambda}=[\RH(\X'\X)^{-1}\RH']^{-1}(\RH\hb-r)$$

  现在将其代入到$\hb-\tilde{\beta}$的表达式中得到
  $$\hb-\tilde{\beta}=(\X'\X)^{-1}\RH'[\RH(\X'\X)^{-1}\RH']^{-1}(\RH\hb-r)$$
  根据定义, 有约束回归模型的残差为
  $$\tilde{e}=Y-\X\tilde{\beta}=\hat{e}+\X(\hb-\tilde{\beta})$$
  从而
  $$\tilde{e}'\tilde{e}-\hat{e}'\hat{e}=(\RH\hb-r)'[\RH(\X'\X)^{-1}\RH']^{-1}(\RH\hb-r)$$
  根据$F$统计量的定义得到
  \begin{align*}
  F&=\frac{(\RH\hb-r)'[\RH(\X'\X)^{-1}\RH']^{-1}(\RH\hb-r)/J}{s^2} \\
  &=\frac{(\tilde{e}'\tilde{e}-\hat{e}'\hat{e})/J}{\hat{e}'\hat{e}/(n-K)}
  \end{align*}
  由此证得定理.
  \end{proof}
\begin{corollary}\label{cor:cor2.2}
对于原假设$\HH_0: \beta_1=\beta_2=\cdots=\beta_k=0$, $F$统计量为
$$F=\frac{R^2/(K-1)}{(1-R^2)/(n-K)}$$
其中$R^2$为无约束回归模型的可决系数.
\end{corollary}
\begin{proof}
  在$\HH_0$成立的条件下, $\tilde{\beta}=(\overbar{Y},0,\cdots,0)'$, 从而
  $$\tilde{e}'\tilde{e}=(Y-\mathbf{1}_n\overbar{Y})'(Y-\mathbf{1}_n\overbar{Y})$$
  根据$R^2$的定义可知
  $$R^2=1-\frac{\hat{e}'\hat{e}}{\tilde{e}'\tilde{e}}$$
  于是
  $$F=\frac{(\tilde{e}'\tilde{e}-\hat{e}'\hat{e})/(K-1)}{\hat{e}'\hat{e}/(n-K-1)}=\frac{R^2/(K-1)}{(1-R^2)/(n-K)}$$
\end{proof}

\section{广义最小二乘估计}
考虑如下矩阵形式的线性回归模型
\begin{equation}\label{eq2.13}
  Y=\X\beta+e
\end{equation}
随机扰动项之间存在\textbf{已知形式}的相关性和异方差
\begin{align}
\begin{split}
\E[e|\X]&=0 \\
\var(e|\X)&=\sigma^2\mathbold{\Sigma}
\end{split}
\label{eq2.12}
\end{align}
这里的$\mathbold{\Sigma}$是一个$n\times n$维对称的正定矩阵, 并且它是关于数据矩阵$\X$的函数, $\sigma^2$仍是一个未知的常数. 现在来研究OLS估计量的统计性质.

\begin{theorem}
  在假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和条件(\ref{eq2.12})下, OLS估计量具有以下性质.

  (1) 无偏性: $\E[\hb|\X]=\beta$.

  (2) 方差: $\var(\hb|\X)=\sigma^2(\X'\X)^{-1}\X'\mathbold{\Sigma}\X(\X'\X)^{-1}$.

  (3) 相关性: $\text{cov}(\hb,\hat{e}|\X)\neq0$.
\end{theorem}
\begin{proof}
  (1) 由$\hb-\beta=(\X'\X)^{-1}\X'e$可知
  \begin{align*}
  \E[(\hb-\beta)|\X]&=(\X'\X)^{-1}\X'\E[e|\X]=0
  \end{align*}

  (2) 类似之前的证明可得
  \begin{align*}
  \var(\hb|\X)&=\E[(\hb-\beta)(\hb-\beta)'|\X] \\
  &=\E[(\X'\X)^{-1}\X'ee'\X(\X'\X)^{-1}|\X] \\
  &=(\X'\X)^{-1}\X'\E[ee'|\X]\X(\X'\X)^{-1} \\
  &=\sigma^2(\X'\X)^{-1}\X'\mathbold{\Sigma}\X(\X'\X)^{-1}
  \end{align*}

  (3) 因为$\X'\mathbold{\Sigma}\mathbold{M}\neq0$, 于是
  \begin{align*}
  \text{cov}(\hb,\hat{e}|\X)&=\E[(\hb-\beta)e'|\X] \\
  &=\E[(\X'\X)^{-1}\X'ee'\mathbold{M}|\X] \\
  &=(\X'\X)^{-1}\X'\E[ee'|\X]\mathbold{M} \\
  &=\sigma^2(\X'\X)^{-1}\X'\mathbold{\Sigma}\mathbold{M}\neq0
  \end{align*}
\end{proof}
以上定理表明, 即使随机扰动项存在异方差或自相关, OLS估计量$\hb$仍是无偏的, 但由于$\hb$的方差不再具有简单形式的$\sigma^2(\X'\X)^{-1}$, 因此之前提到的基于简单形式方差的$T$检验和$F$检验均无效. 此外, 即使使用正确的方差$\sigma^2(\X'\X)^{-1}\X'\mathbold{\Sigma}\X(\X'\X)^{-1}$, $T$检验和$F$检验仍无效, 因为$\hb$和$\hat{e}$存在条件相关性. 另一方面, OLS估计量此时显然不再是BLUE.

为了解决上述问题, 现在提出一种新的估计方法, 称为广义最小二乘 (Generalized Least Squares, GLS)估计.

首先我们知道, 对于任意对称正定矩阵$\mathbold{\Sigma}$, 总存在矩阵$\mathbold{\Sigma}^{-\frac{1}{2}}$, 使得$\mathbold{\Sigma}^{-1}=\mathbold{\Sigma}^{-\frac{1}{2}}\mathbold{\Sigma}^{-\frac{1}{2}}$. 现在将$\mathbold{\Sigma}^{-\frac{1}{2}}$左乘回归方程(\ref{eq2.13})得到
$$\tilde{Y}=\tilde{\X}\beta+\tilde{e}$$
其中$\tilde{Y}=\mathbold{\Sigma}^{-\frac{1}{2}}Y$, $\tilde{\X}=\mathbold{\Sigma}^{-\frac{1}{2}}\X$, 以及$\tilde{e}=\mathbold{\Sigma}^{-\frac{1}{2}}e$. 对上式进行OLS回归即可得到所谓的GLS估计量
\begin{align*}
\tilde{\beta}&=(\tilde{\X}{'}\tilde{\X})^{-1}\tilde{\X}{'}\tilde{Y} \\
&=(\X'\mathbold{\Sigma}^{-1}\X)^{-1}\X'\mathbold{\Sigma}^{-1}Y
\end{align*}
根据之前的做法, 容易得到以下定理.

\begin{theorem}\label{thm:thm2.6}
  在假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和条件(\ref{eq2.12})成立的情况下有

  (1) $\E[\tilde{\beta}|\X]=\beta$.

  (2) $\var(\tilde{\beta}|\X)=\sigma^2(\X'\mathbold{\Sigma}^{-1}\X)^{-1}$.

  (3) $\text{cov}(\tilde{\beta},\tilde{e}|\X)=0$, 其中$\tilde{e}=\tilde{Y}-\tilde{\X}\tilde{\beta}$.

  (4) $\tilde{\beta}$为最佳线性无偏估计量 (BLUE).

  (5) $\E[\tilde{s}^2|\X]=\sigma^2$, 其中$\tilde{s}^2=\hat{e}^{\ast}{'}\hat{e}^{\ast}/(n-K)$.
\end{theorem}
\begin{proof}
  变换后的模型$\tilde{Y}=\tilde{\X}\beta+\tilde{e}$满足CLRM假设\ref{pro:pro2.1}$-$\ref{pro:pro2.4}, 并且GLS估计量$\tilde{\beta}$又是$\tilde{Y}=\tilde{\X}\beta+\tilde{e}$的OLS估计量, 因此根据定理\ref{thm:thm2.2}, 定理\ref{thm:thm2.7} (Gauss-Markov定理)以及定理\ref{thm:thm2.4}即可证得结论成立.
\end{proof}
仅根据定理\ref{thm:thm2.6}还无法构造新的检验统计量, 如果将条件(\ref{eq2.12})强化为
\begin{equation}\label{eq2.14}
  e|\X\sim N(0,\sigma^2\mathbold{\Sigma})
\end{equation}
那么在假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和条件(\ref{eq2.14})下有
\begin{align*}
\tilde{T}&=\frac{\RH\tilde{\beta}-r}{\sqrt{\tilde{s}^2\RH(\X'\mathbold{\Sigma}^{-1}\X)^{-1}\RH'}}\sim t_{n-K} \\
\tilde{F}&=\frac{(\RH\tilde{\beta}-r)'[\RH(\X'\mathbold{\Sigma}^{-1}\X)^{-1}\RH']^{-1}(\RH\tilde{\beta}-r)/J}{\tilde{s}^2}\sim F_{J,n-K}
\end{align*}
可以看出, 只要是在有限样本条件下, 无论是OLS还是GLS, 在构造检验统计量的时候都必须假设正态随机扰动项. 不仅如此, 使用GLS估计必须已知误差项的协方差矩阵的具体形式, 这在实际应用中难以满足, 因此GLS的理论意义远高于实践意义.

在一些情形下, 我们可以用估计量$\hat{\mathbold{\Sigma}}$来替代协方差矩阵$\mathbold{\Sigma}$, 估计量$\hat{\BS}$既可以是参数的也可以是非参数的, 由此可以得到以下可行广义最小二乘估计 (Feasible Generalized Least Squares, FGLS)
$$\tilde{\beta}_F=(\X'\hat{\mathbold{\Sigma}}^{-1}\X)^{-1}\X'\hat{\mathbold{\Sigma}}^{-1}Y$$
FGLS的困难在于$\mathbold{\Sigma}$包含太多参数, 可能多达$n(n+1)/2$个, 然而样本容量大小只有$n$. 因此如果不对$\mathbold{\Sigma}$的形式施加一定限制, 那么无法用这$n$个数据一致估计出$\mathbold{\Sigma}$. 例如, 可假设误差项仅存在异方差而不存在自相关, 也即
\begin{align*}
\E[ee'|\X]=\text{diag}\,\{\sigma^2(X_1),\sigma^2(X_2),\cdots,\sigma^2(X_n)\}
\end{align*}
此时得到的FGLS估计量$\tilde{\beta}_F$的有限样本分布与方差形式$\mathbold{\Sigma}$已知时得到的GLS估计量$\tilde{\beta}$的有限样本分布不同, 这是因为$\hat{\mathbold{\Sigma}}$估计量的抽样误差会对估计量$\tilde{\beta}_F$造成一定影响.

然而在一定正则条件下, 可以证明GLS估计量和FGLS估计量具有相同的渐近性质, 具体见第五章的相关讨论.

\section{聚类样本}
重新回到假设\ref{pro:pro2.1}, 它要求个体$i$与个体$j$的决策互不影响. 然而, 当样本内的个体存在某些联系时, 该假设不成立, 例如: 这些个体间的关系是邻居, 同班同学等.

下面这个例子由 Duflo et al. (2011)给出. 在2005年, Kenya的140座小学获得了一笔资金招募更多的一年级教师以降低班级规模, 其中的121座学校原本仅有一个一年级班级, 在招募新教师后, 将这些学校的一年级班级分为两个组别, 其中一个组别由新教师进行授课. 在随机选择的60座学校中, 学生进入哪个组别取决于TA的初始测验成绩, 我们称其为Tracking学校; 而在剩余的61座学校中, 学生随机进入两个组别的班级.

现在考虑以下模型
$$\text{TestScore}_{ig}=\alpha+\gamma\text{Tracking}_g+X_{ig}'\beta+e_{ig}$$
其中$\text{TestScore}_{ig}$是学校$g$中的学生$i$在18个月后的测验成绩, $\text{Tracking}_g$是虚拟变量, 当学校$g$为Tracking学校时取值为1, $X_{ig}$是一系列控制变量.

我们的目的是估计参数$\gamma$, 困难在于学生的测试成绩可能受到同校其他学生的影响, 假设\ref{pro:pro2.1}不成立, 因而难以在经典线性回归模型的框架中进行分析. 一个更合理的假设是, 这种相关性不会存在于不同学校中.

现在我们将样本表示为$\{Y_{ig},X_{ig}\}$, 其中$g=1,2,\cdots,G$表示聚类指标, $i=1,2,\cdots,n_g$为第$g$个聚类中的个体, 观测值总计为$n=\sum_{g=1}^{G}n_g$. 在Duflo et al. (2011)的研究中, $G=121$, 每个学校的样本学生数量从19到62不等, 观测值总共为$n=5795$.

一般地, 聚类样本中的线性回归模型为
$$Y_{ig}=X_{ig}'\beta+e_{ig}$$
也可以更紧凑地表示为
\begin{equation}\label{eq2.8}
  Y_g=\X_g\beta+e_g
\end{equation}
其中
\begin{align*}
Y_g&=(Y_{1g},Y_{2g},\cdots,Y_{n_gg})' \\
\X_g&=(X_{1g},X_{2g},\cdots,X_{n_gg})' \\
e_g&=(e_{1g},e_{2g},\cdots,e_{n_gg})'
\end{align*}
而全样本的回归模型仍为$Y=\X\beta+e$.

根据以上的表示法, 可以写出OLS估计量
\begin{align}
\hb&=\left(\sum_{g=1}^{G}\sum_{i=1}^{n_g}X_{ig}X_{ig}'\right)^{-1}\left(\sum_{g=1}^{G}\sum_{i=1}^{n_g}X_{ig}Y_{ig}\right) \nonumber \\
&=\left(\sum_{g=1}^{G}\X_g'\X_g\right)^{-1}\left(\sum_{g=1}^{G}\X_g'Y_g\right) \label{eq2.9} \\
&=(\X'\X)^{-1}\X'Y \nonumber
\end{align}
为了得到OLS估计量的统计性质, 我们给出以下假设.
\begin{proposition}\label{pro:pro2.6}
聚类样本$(Y_g,\X_g)$在聚类$g$之间是相互独立的.
\end{proposition}
进一步, 如果以下矩条件成立
\begin{equation}\label{eq2.10}
  \E[e_g|\X_g]=0
\end{equation}
则线性回归模型是正确识别的, 它要求每个聚类中的不可观测因素与回归元$X_{ig}$无关.
\begin{theorem}
  在假设\ref{pro:pro2.6}和条件(\ref{eq2.10})下, $\E[\hb|\X]=\beta$.
\end{theorem}
\begin{proof}
  根据(\ref{eq2.8})和(\ref{eq2.9})可知
  \begin{align*}
  \hb-\beta=\left(\sum_{g=1}^{G}\X_g'\X_g\right)^{-1}\left(\sum_{g=1}^{G}\X_g'e_g\right)
  \end{align*}
  再由$\E[e_g|\X]=\E[e_g|\X_g]=0$即可证得定理.
\end{proof}
进一步考察$\hb$的协方差矩阵, 记$\mathbold{\Sigma}_g=\E[e_ge_g'|\X_g]$表示第$g$个聚类的$n_g\times n_g$维协方差矩阵, 在假设\ref{pro:pro2.6}下可以得到
\begin{align}
\var\left(\left.\sum_{g=1}^{G}\X_g'e_g\right|\X\right)&=\sum_{g=1}^{G}\var(\X_g'e_g|\X_g) \nonumber \\
&=\sum_{g=1}^{G}\X_g'\E[e_ge_g'|\X_g]\X_g \nonumber \\
&=\sum_{g=1}^{G}\X_g'\mathbold{\Sigma}_g\X_g\equiv\mathbold{\Omega}_n \label{eq2.11}
\end{align}
此时
$$\V_{\text{OLS}}=(\X'\X)^{-1}\mathbold{\Sigma}_n(\X'\X)^{-1}$$
由于聚类内的观测值存在相关性, 因此我们对协方差矩阵的分析需要特别小心. 考虑一种简单的情形: $n_g=N$, $\E[e_{ig}^2|\X_g]=\sigma^2$, 以及$\E[e_{ig}e_{jg}|\X_g]=\sigma^2\rho$, $i\neq j$, 此时
$$\V_{\text{OLS}}=\sigma^2[1+\rho(N-1)](\X'\X)^{-1}$$

Arellano (1987)给出了一般情况下$\mathbold{\Omega}_n$的聚类稳健估计量
\begin{align*}
\hat{\mathbold{\Omega}}_n&=\sum_{g=1}^{G}\X_g'\hat{e}_g\hat{e}_g'\X_g \\
&=\sum_{g=1}^{G}\sum_{i=1}^{n_g}\sum_{j=1}^{n_g}X_{ig}X_{jg}'\hat{e}_{ig}\hat{e}_{jg} \\
&=\sum_{g=1}^{G}\left(\sum_{i=1}^{n_g}X_{ig}\hat{e}_{ig}\right)\left(\sum_{j=1}^{n_g}X_{jg}\hat{e}_{jg}\right)'
\end{align*}
此时协方差矩阵估计量为
$$\hat{\V}_{\text{OLS}}=a_n(\X'\X)^{-1}\hat{\mathbold{\Omega}}_n(\X'\X)^{-1}$$
其中
$$a_n=\left(\frac{n-1}{n-K}\right)\left(\frac{G}{G-1}\right)$$
为有限样本调整参数.

另一方面, 许多聚类理论都设置了$n_g=N$这一条件以简化推导, 但实证分析中很难遇到这种情况. 例如中国西部地区和东部地区的人口数量在省级层面明显不同, 此时聚类稳健推断差不多可以看作是具有极强异质性观测时的异方差稳健推断.

那么什么时候应该使用聚类稳健标准误? Abadie et al. (2023)在最近的研究中给出了答案, 他们提出了因果聚类方差 (Causal Cluster Variance, CCV)修正传统的异方差稳健标准误和聚类稳健标准误. 特别是, 如果样本中的聚类数量占相当部分的总体聚类数量时, CCV将会产生非常显著的修正效果.


\chapter{渐近理论基础}
 正如Wooldridge (2010)所言, 多数估计量并不像OLS估计量那样具有无偏性等性质, 因此我们很难对其进行有限样本分析, 而是将其纳入到大样本框架下得到它的渐近性质.
\section{收敛概念}
确定性序列的收敛和有界性在数学分析中已经学过, 这里给出几个新的符号. 如果$\{a_n\}$的极限为0, 则记为$a_n=\text{o}(1)$; 如果$\{a_n\}$有界, 则记为$a_n=\text{O}(1)$. 更一般地, 如果$n^{-\lambda}a_n\to0$, 则$a_n=o(n^\lambda)$; 如果$\{n^{-\lambda}a_n\}$有界, 则$a_n=\text{O}(n^\lambda)$.


设$\{X_n\}_{n=1}^\infty$是由定义在同一个概率空间$(\Omega,\F,\PP)$上的随机变量构成的序列\footnote{事实上, 依分布收敛可以将$X_n$定义在不同的概率空间$(\Omega_n,\F_n,\PP_n)$上, 但其余的收敛概念只能将$X_n$定义在同一个概率空间上.}, 现在我们给出以下几个收敛的概念.
\begin{definition}
(1) $\{X_n\}$依概率收敛于$X$, 如果对于任意给定的$\varepsilon>0$, 当$n\to\infty$时都有
$$\PP[|X_n-X|<\varepsilon]\to1$$
记作$X_n\xrightarrow{p}X$, $X_n-X=\text{o}_p(1)$或$\text{plim}\,X_n=X$.

(2) $\{X_n\}$是依概率有界的, 如果对于任意给定的$\varepsilon>0$, 总存在$0<\delta<\infty$, 使得当$n\to\infty$时就有
$$\PP[|X_n|>\delta]<\varepsilon$$
记作$X_n=\text{O}_p(1)$.

(3) $\{X_n\}$几乎必然收敛于$X$, 如果存在$A\in\F$, 使得对于一切$\omega\in A$都有$\PP[A]=1$, 并且
$$\PP\left[\lim_{n\to\infty}X_n=X\right]=1$$
记作$X_n\xrightarrow{a.s.}X$.

(4) $\{X_n\}$依$L^p$收敛于$X$, $p>0$, 如果当$n\to\infty$时有
$$\E|X_n-X|^p\to0$$
记作$X_n\xrightarrow{L^p}X$. 当$p=2$时, 称$\{X_n\}$依均方收敛于$X$, 记作$X_n\xrightarrow{q.m.}X$.

(5) $\{X_n\}$依分布收敛于$X$, 如果对于一切$x\in\mathscr{C}(F)$都有
$$\lim_{n\to\infty}F_n(x)=F(x)$$
其中$F_n$和$F$分别为$X_n$和$X$的累积概率分布, $\mathscr{C}(F)$为$F$的一切连续点构成的集合.
\end{definition}
\begin{remark}
更一般地, 如果$\{a_n\}$为非随机序列, 并且$X_n/a_n=\text{o}_p(1)$, 那么$X_n=\text{o}_p(a_n)$. 类似可以定义$\text{O}_p(a_n)$的概念.
\end{remark}
先来看依概率收敛$\text{o}_p(1)$和依概率有界$\text{O}_p(1)$之间的关系.
\begin{theorem}
  设$w_n=\text{o}_p(1)$, $x_n=\text{o}_p(1)$, $y_n=\text{O}_p(1)$, $z_n=\text{O}_p(1)$, 那么

  (1) $w_n+x_n=\text{o}_p(1)$.

  (2) $y_n+z_n=\text{O}_p(1)$.

  (3) $w_n+y_n=\text{O}_p(1)$.

  (4) $w_nx_n=\text{o}_p(1)$.

  (5) $y_nz_n=\text{O}_p(1)$.

  (6) $w_ny_n=\text{o}_p(1)$.
\end{theorem}
\begin{proof}
这里只证明(2), (4)和(6), 其余命题证法类似.

根据定义可知, 当$n\to\infty$时有
\begin{align*}
  &\PP[|w_n+x_n|>2\delta]\leq \PP[|x_n|>\delta]+\PP[|w_n|>\delta]<2\varepsilon \\
  &\PP[w_nx_n>\varepsilon]\leq \PP[|w_n|>\varepsilon]+\PP[|x_n|>1]\to0
\end{align*}
以及
\begin{align*}
\PP[|w_ny_n|>\varepsilon]&\leq \PP[|w_ny_n|>\varepsilon, |y_n|<\delta]+\PP[|w_ny_n|>\varepsilon, |y_n|\geq\delta] \\
&\leq\PP\left[|w_n|>\frac{\varepsilon}{\delta}\right]+\PP[|y_n|\ge\delta]<\varepsilon
\end{align*}
因为$\varepsilon$可以任意小, 因此命题(6)成立.
\end{proof}

现在我们给出几个关于随机收敛的例子.
\begin{example}\label{eg2}
考虑概率空间$([0,1],\mathscr{B}([0,1]),\PP)$, 其中$\mathscr{B}([0,1])$为$[0,1]$上的Borel集构成的集族, $\PP$为$\R$上的Lebesgue测度, $s$在$[0,1]$上均匀分布. 定义随机变量$X(\omega)=\omega$, 以及随机变量序列
$$X_n(\omega)=\begin{cases}
                \omega+\omega^n, & \omega\in[0,1-n^{-1}] \\
                \omega+1, & \omega\in(1-n^{-1},1]
              \end{cases}$$
那么随机变量序列$\{X_n\}$依$L^p$收敛于$X$, 依概率收敛于$X$, 并且几乎必然收敛于$X$. 这是因为当$n\to\infty$时有
\begin{align*}
\E|X_n-X|^p&=\int_{0}^{1-n^{-1}}\omega^{pn}\,\text{d}\omega+\int_{1-n^{-1}}^{1}\text{d}\omega \\
&=\frac{1}{pn+1}\left(1-\frac{1}{n}\right)^{pn+1}+\frac{1}{n}\to0
\end{align*}
并且对于任意$0<\varepsilon<1$都有
$$\PP[|X_n-X|<\varepsilon]=\PP[X_n=\omega+\omega^n]=1-\frac{1}{n}\to1$$
由此分别得出$X_n\xrightarrow{L^p}X$以及$X_n\xrightarrow{p}X$.

最后, 令$\displaystyle A=\left\{\omega\in\Omega: \lim_{n\to\infty}X_n=X\right\}$, 对于一切$\omega\in[0,1)$都有
$$\PP\left[\lim_{n\to\infty}X_n=X\right]=(1-n^{-1})\PP\left[\lim_{n\to\infty}\omega^n=0\right]+n^{-1}\PP\left[\lim_{n\to\infty}1=0\right]=1$$
故而$A=\Omega\backslash\{1\}$, 并且$\PP[A]=1$, 因此$X_n\xrightarrow{a.s.}X$.
\end{example}

\begin{example}
对于一切$n\ge1$, 设随机变量$X_n$的概率分布为

\begin{table}[!htbp]
\centering
\begin{tabular}{lcc}
$X_n$     & $\displaystyle \frac{1}{n}$   & $n$                           \\
$\PP$ & $\displaystyle 1-\frac{1}{n}$ & $\displaystyle \frac{1}{n}$
\end{tabular}
\end{table}

\noindent 那么$\{X_n\}$不依均方收敛于0, 但是依概率收敛于0. 这是因为当$n\to\infty$时
$$\E|X_n|^2=n+\frac{1}{n^2}-\frac{1}{n^3}\to\infty$$
并且
$$\PP[|X_n|>\varepsilon]=\PP[X_n=n]=\frac{1}{n}\to0$$
\end{example}

\begin{example}
设$\{X_n\}$中的元素两两独立, $\PP[X_n=1]=1/n$并且$\PP[X_n=0]=1-1/n$. 对于任意$0<\varepsilon<1/2$, 可以得到
$$\PP[|X_n|>\varepsilon]=\frac{1}{n}$$
因此$\{X_n\}$依概率收敛于0. 由于
$$\sum_{n=1}^{\infty}\PP[X_n=1]=\infty$$
根据Borel-Cantelli第二引理\footnote{设$(\Omega,\F,\PP)$为概率空间, 集合序列$\{A_n\}\subset\F$, 并且它的各个元素两两独立. 如果级数$\sum_{n=1}^{\infty}\PP[A_n]=\infty$, 那么$\PP[A_n\,\text{i.o.}]=1$, 这里的$\text{i.o.}$表示infinitely often, 相当于发生无数次事件$A_n\in\F$. 证明见Athreya and Lahiri (2006) Theorem 7.2.2.}可知$\PP[X_n=1\,\text{i.o.}]=1$, 因此$\{X_n\}$并不是几乎必然收敛于0的.
\end{example}

\begin{example}
设概率空间为$([0,1],\mathscr{B}([0,1]),\PP)$, 各符号含义与例\ref{eg2}中的一致. 考虑随机变量$X_n$, 对一切$\omega\in\Omega$都有
$$X_{2n-1}(\omega)=1-\omega,\quad X_{2n}(\omega)=\omega$$
于是$X_{2n-1}$和$X_{2n}$具有同样的极限分布, 从而$\{X_n\}$依分布收敛于均匀分布$U[0,1]$, 然而它不依概率收敛于任何随机变量.
\end{example}

\begin{example}
设概率空间为$([0,1],\mathscr{B}([0,1]),\PP)$, 各符号含义与例\ref{eg2}中的一致. 定义
$$X_n(\omega)=\begin{cases}
                0, & \omega\in[0,1-n^{-2}] \\
                \text{e}^n, & \omega\in(1-n^{-2},1]
              \end{cases}$$
可以得到
$$\limn\E|X_n|^p=\limn\frac{\text{e}^{np}}{n^2}=\infty$$
以及
$$\PP\left[\limn X_n(\omega)=0\right]=1$$
因此$\{X_n\}$不依$L^p$收敛于0, 但几乎必然收敛于0.

再考虑$Y(\omega)=\omega$, 以及
$$Y_n(\omega)=\begin{cases}
                1, & \omega\in \displaystyle\left[\frac{i}{2^k},\frac{i+1}{2^k}\right],\,i=n-2^k,\,1\leq i\leq 2^k\\
                \omega, & \mbox{otherwise}.
              \end{cases}$$
于是当$n\to\infty$时有
$$\E|Y_n-Y|^p=1/2^k\to0$$
也即$Y_n\xrightarrow{L^p}Y$. 然而对于任意$\omega\in[0,1]$, 极限$\displaystyle\limn Y_n(\omega)$不存在, 因此$\{Y_n\}$并不几乎必然收敛于$Y$.
\end{example}

以上例子主要来源于Hong (2017), 从中我们发现: 依概率收敛并不意味着依均方收敛, 也不意味着几乎必然收敛, 并且依分布收敛也不意味着依概率收敛, 依$L^p$收敛则与几乎必然收敛互不包含. 现在我们将随机收敛的关系用定理表述如下.
\begin{theorem}\label{thm:thm3.1}
  (1) $X_n\xrightarrow{L^p}X\Rightarrow X_n\xrightarrow{p}X$.

  (2) $X_n\xrightarrow{a.s.}X\Rightarrow X_n\xrightarrow{p}X$.

  (3) $X_n\xrightarrow{p}X\Rightarrow X_n\xrightarrow{d}X$.

  (4) $X_n\xrightarrow{d}X\Rightarrow X_n=\text{O}_p(1)$.
\end{theorem}
\begin{proof}
  (1) 对于任意给定的$\varepsilon>0$, 由Markov不等式\footnote{设$X$是$K\times1$维随机向量, $\phi:\R^K\to\R_+$为单调递增函数, 那么对任意$t>0$都有$$\PP[|X|\geq t]\leq \frac{\E[\phi(|X|)]}{\phi(t)}$$}可知当$n\to\infty$时有
  $$\PP[|X_n-X|>\varepsilon]\leq\frac{\E|X_n-X|^p}{\varepsilon^p}=0$$
  从而$X_n\xrightarrow{p}X$.

  (2) 任取$\varepsilon>0$, 可以得到
  \begin{equation}\label{eq3.1}
    \PP[|X_n-X|<\varepsilon]\geq \PP[|X_n-X|<\varepsilon,\,\forall j\geq n]
  \end{equation}
  也即
  $$\left\{\omega: |X_j(\omega)-X(\omega)|<\varepsilon,\,\forall j\geq n\right\}\subset\left\{\omega: |X_n(\omega)-X(\omega)|<\varepsilon\right\}$$
  因为$X_n\xrightarrow{a.s.}X$, 根据几乎必然收敛的定义可知, 存在充分大的正整数$N$, 使得对于任意给定的$\delta>0$, 当$n>N$时就有
  $\PP[|X_n-X|<\varepsilon,\,\forall j\geq n]>1-\delta$. 在(\ref{eq3.1})两端取极限得到
  $$\limn \PP[|X_n-X|<\varepsilon]\geq \limn \PP[|X_n-X|<\varepsilon,\,\forall j\geq n]=1$$
  也即$X_n\xrightarrow{p}X$.

  (3) 设$F_n$和$F$分别为$X_n$和$X$的累积分布函数, $n\ge0$, 固定$x\in\mathscr{C}(F)$, 于是对于任意$\varepsilon>0$都有
  \begin{align}
  \PP[X_n\leq x]&\leq \PP[X_n\leq x, X\leq x+\varepsilon]+\PP[X_n\leq x, X>x+\varepsilon]\nonumber \\
  &\leq \PP[X_n\leq x]+\PP[X_n-X\leq x-X, x-X_n<-\varepsilon] \nonumber \\
  &\leq \PP[X_n\leq x]+\PP[X_n-X<-\varepsilon]+\PP[X_n-X>-\varepsilon] \nonumber \\
  &\leq \PP[X\leq x+\varepsilon]+\PP[|X_n-X|>\varepsilon] \label{eq3.2}
  \end{align}
  类似地有
  \begin{equation}\label{eq3.3}
    \PP[X_n\leq x]\geq \PP[X\leq x-\varepsilon]-\PP[|X_n-X|>\varepsilon]
  \end{equation}
  根据(\ref{eq3.2})和(\ref{eq3.3})可知
  $$F(x-\varepsilon)-\PP[|X_n-X|>\varepsilon]\leq F_n(x)\leq F(x+\varepsilon)+\PP[|X_n-X|>\varepsilon]$$
  由于$X_n\xrightarrow{p}X$, 令$n\to\infty$可知对一切$\varepsilon\in(0,\infty)$都有
  \begin{equation}\label{eq3.4}
    F(x-\varepsilon)\leq\liminf_{n\to\infty}F_n(x)\leq\limsup_{n\to\infty}F_n(x)\leq F(x+\varepsilon)
  \end{equation}
  注意到$x\in\mathscr{C}(F)$, 故而$F(x-)=F(x+)$, 在(\ref{eq3.4})中令$\varepsilon\downarrow 0$即得$\displaystyle\limn F_n(x)=F(x)$.

  (4) 由于$X$为随机变量, 故而存在$\delta>0$, 使得$F$在$\delta$和$-\delta$处连续, 并且对于任意给定的$\varepsilon>0$都有
  \begin{equation}\label{eq3.8}
    \PP[|X|>\delta]=F(-\delta)+[1-F(\delta)]<\frac{\varepsilon}{2}
  \end{equation}
  因为$X_n\xrightarrow{d}X$, 因此对于充分大的$n$都有
  \begin{align}
  &|F_n(-\delta)-F(-\delta)|<\frac{\varepsilon}{4} \label{eq3.9} \\
  &|F(\delta)-F_n(\delta)|<\frac{\varepsilon}{4} \label{eq3.10}
  \end{align}
  将(\ref{eq3.8})$-$(\ref{eq3.10})相加, 由三角不等式即可推知$\PP[|X_n|>\delta]<\varepsilon$.
\end{proof}
\begin{remark}
尽管依分布收敛无法推出依概率收敛, 但在特殊情况下是可以的: 如果$X_n\xrightarrow{d}X$, 并且存在$c\in\R$使得$\PP[X=c]=1$, 那么$X_n\xrightarrow{p}c$.
\end{remark}
我们将上述定理表述成如下示意图:
\begin{center}
\begin{tikzpicture}
\begin{axis}[
scale = 0.6,
xmin = -2.5, xmax = 2.5,
ymin = 0, ymax = 2.5,
axis lines = none,
xtick = {0}, ytick = \empty,
clip = false,
]
\draw[-{Triangle}] (0, 2) to (0, 0);
\node [below] at (0, 0) {$p$};
\node [above] at (0, 2) {$L^p$};
\draw[-{Triangle}] (0.35, -0.2) to (4.25, -0.2);
\node [right] at (4.2, -0.2) {$d$};
\draw[-{Triangle}] (-4.2,-0.2) to (-0.3, -0.2);
\node [left] at (-4.2, -0.2) {$a.s.$};
\draw[-{Triangle}] (4.9, -0.2) to (9, -0.2);
\node [right] at (9, -0.2) {$\text{O}_p(1)$};
\end{axis}
\end{tikzpicture}
\end{center}

现在介绍推导渐近多元分布时特别重要的Slutsky定理与连续映射定理(Continuous Mapping Theorem, CMT).
\begin{theorem}[Slutsky定理]
  设$\{X_n\}$和$\{Y_n\}$为两个随机变量序列, 并且$(X_n,Y_n)$定义在概率空间$(\Omega_n,\F_n,\PP_n)$上, 若$X_n\xrightarrow{d} X$, 并且存在$c\in\R$使得$Y_n\xrightarrow{p}c$, 那么

  (1) $X_n+Y_n\xrightarrow{d}X+c$.

  (2) $X_nY_n\xrightarrow{d}cX$.

  (3) $X_n/Y_n\xrightarrow{d}X/c$, 其中$c\ne0$.
\end{theorem}
\begin{proof}
  (1) 设$F_\infty$为$X$的累积分布函数, 于是$X+c$的cdf由$F(x)=F_\infty(x-c)$, $x\in\R$定义, 固定$x\in\mathscr{C}(F)$, 于是$x-c\in\mathscr{C}(F_\infty)$.

  对于任意给定的$\varepsilon>0$, 可以得到
  \begin{align}
  \PP[X_n+Y_n\leq x]&=\PP[X_n+Y_n\leq x, |Y_n-c|\leq\varepsilon]+\PP[X_n+Y_n\leq x, |Y_n-c|>\varepsilon] \nonumber \\
  &\leq \PP[X_n+Y_n\leq x, |Y_n-c|\leq\varepsilon]+\PP[|Y_n-c|>\varepsilon] \nonumber \\
  &\leq \PP[X_n+c-\varepsilon\leq x]+\PP[|Y_n-c|>\varepsilon] \label{eq3.5}
  \end{align}
  类似地有
  \begin{equation}\label{eq3.6}
    \PP[X_n+Y_n\leq x]\geq \PP[X_n+c+\varepsilon\leq x]-\PP[|Y_n-c|>\varepsilon]
  \end{equation}
  选取$x-c$, $x-c+\varepsilon$与$x-c-\varepsilon$, 使得它们都是$\mathscr{C}(F_\infty)$中的元素, 那么由(\ref{eq3.5})和(\ref{eq3.6})可知
  \begin{align*}
  F_\infty(x-c-\varepsilon)&\leq\liminf_{n\to\infty}\PP[X_n+Y_n\leq x] \\
  &\leq\limsup_{n\to\infty}\PP[X_n+Y_n\leq x]\leq F_\infty(x-c+\varepsilon)
  \end{align*}
  现在令$\varepsilon\to0^+$, 于是
  $$\limn \PP[X_n+Y_n\leq x]=F_\infty(x-c)=F(x)$$

  (2), (3) 若$c=0$, 根据定理\ref{thm:thm3.1}(3), 只需证明$X_nY_n\xrightarrow{p}0$即可. 对于任意给定的$\varepsilon, \delta>0$, 可以得到
  \begin{align*}
  \PP[|X_nY_n|>\varepsilon]&=\PP[|X_nY_n|>\varepsilon, |Y_n|\geq\delta]+\PP[|X_nY_n|>\varepsilon, |Y_n|<\delta] \\
  &\leq \PP[|X_n|\geq\varepsilon/\delta]+\PP[|Y_n|\geq\delta]
  \end{align*}
  可以选取$\pm\varepsilon/\delta$为$\mathscr{C}(F_\infty)$中的元素, 从而
  $$0\leq\liminf_{n\to\infty}\PP[|X_nY_n|>\varepsilon]\leq\limsup_{n\to\infty}\PP[|X_nY_n|>\varepsilon]\leq \PP[|X_n|>\varepsilon/\delta]$$
  令$\varepsilon\to0^+$, 并且可以将$\delta$选得充分小, 使得$\varepsilon/\delta$充分大, 因此$X_nY_n\xrightarrow{p}0$.

  若$c\neq0$, 那么$X_nY_n=cX_n+X_n(Y_n-c)$, 上面已经证明了$X_n(Y_n-c)\xrightarrow{p}0$, 因此只需证明$cX_n\xrightarrow{d}cX$即可. 当$c>0$时, $x/a\in\mathscr{C}(F_\infty)$, 于是
  $$\PP[cX_n\leq x]=\PP[X_n\leq x/c]\to \PP[X\leq x/c]=\PP[cX\leq x]$$
  也即$cX_n\xrightarrow{d}cX$, $c<0$时的证明类似.
\end{proof}
\begin{remark}
如果$X_n\xrightarrow{d}X$, $Y_n\xrightarrow{d}Y$, 那么不一定有$X_n+Y_n\xrightarrow{d}X+Y$.
\end{remark}
\begin{example}
设$X_n$和$Y_n$是相互独立的服从的标准正态分布的随机变量, 那么
$$X_n+Y_n\xrightarrow{d}N(0,2)$$
另一方面, 如果对于一切$n\ge1$都有$X_n=Y_n$, 那么
$$X_n+Y_n=2X_n\sim N(0,4)$$
显然$X_n+Y_n\not\xrightarrow{d}X+Y$.
\end{example}
\begin{theorem}[连续映射定理]
  设$X_1,\cdots,X_n$和$X$为$K\times1$维随机向量. 若$X_n\xrightarrow{d}X$, $f:\R^K\to\R^L$为Borel可测函数, $\PP[X\in D_f]=0$, 其中$D_f$为$f$的不连续点构成的集合, 那么$f(X_n)\xrightarrow{d}f(X)$.
\end{theorem}
\begin{proof}
  见Klenke (2013) Theorem 13.25.
\end{proof}
\begin{theorem}[连续映射定理]\label{thm:thm3.3}
  设$X_1,\cdots,X_n$和$X$为$K\times1$维随机向量, $g:\R^K\to\R^L$为Borel可测函数, 并且在常向量$\alpha$处连续, 那么$X_n\xrightarrow{p}\alpha\Rightarrow g(X_n)\xrightarrow{p}g(\alpha)$.
\end{theorem}
\begin{proof}
  根据连续函数的定义可知, 对于任意给定的$\varepsilon>0$, 存在$0<\delta<\infty$, 使得当$||X_n-\alpha||<\delta$时就有
  $$||g(X_n)-g(\alpha)||<\varepsilon$$
  因此
  $$\PP[||X_n-\alpha||<\delta]\leq\PP[||g(X_n)-g(\alpha)||<\varepsilon]$$
  因为$X_n\xrightarrow{p}X$, 故而上式左端趋近于1, 所以$g(X_n)\xrightarrow{p}g(\alpha)$.
\end{proof}
\begin{remark}
如果我们将定理\ref{thm:thm3.3}中的$X_n\xrightarrow{p}X$改为$X_n\xrightarrow{a.s.}X$, 其它条件不变, 那么相应的结论变为$g(X_n)\xrightarrow{a.s.}g(\alpha)$, 它的证明非常Trivial.
\end{remark}
\begin{example}
给定假设\ref{pro:pro2.1}, \ref{pro:pro2.3}和\ref{pro:pro2.5}, 那么$s^2\xrightarrow{p}\sigma^2$且$s\xrightarrow{p}\sigma$. 理由如下:

根据相关假设, $\displaystyle(n-K)\left.\frac{s^2}{\sigma^2}\right|\mathbold{X}\sim\chi^2_{n-K}$. 进而有
  \begin{align*}
  \E[s^2|\mathbold{X}]&=\sigma^2 \\
  \var(s^2|\mathbold{X})&=\frac{2\sigma^4}{n-K}
  \end{align*}
  当$n\rightarrow\infty$时, $\displaystyle\E|s^2-\sigma^2|^2=\frac{2\sigma^4}{n-K}\rightarrow 0$并不依赖于$\mathbold{X}$. 因此$s^2\xrightarrow{q.m.}\sigma^2$, 由定理\ref{thm:thm3.1}(1)可知$s^2\xrightarrow{p}\sigma^2$, 根据CMT即可推知$s\xrightarrow{p}\sigma$.
\end{example}
\section{大数定律}
本节主要介绍渐近理论的第二个工具: 大数定律 (Law of Large Numbers, LLN), 它表明i.i.d.随机变量的均值将会依概率收敛到一阶矩.
\begin{theorem}\label{thm:thm3.6}
  设$X_1, X_2,\cdots, X_n$是i.i.d.随机变量, $b_n>0$, $S_n=X_1+\cdots+X_n$, 并且当$n\to\infty$时以下条件成立: (1) $b_n\to\infty$; (2) $\sum_{i=1}^{n}\PP[|X_i|>b_n]\to0$; (3) $b_n^{-2}\sum_{i=1}^{n}\E[X_i^2\mathbbm{1}\{|X_i|\leq b_n\}]\to0$. 那么
  $$\frac{S_n-a_n}{b_n}\xrightarrow{p}0$$
  其中$a_n=\sum_{j=1}^{n}\E[X_i\mathbbm{1}\{|X_i|\leq b_n\}]$.
\end{theorem}
\begin{proof}
  设$Y_j=X_j\mathbbm{1}\{|X_j|\leq b_n\}$, $T_n=\sum_{j=1}^{n}Y_j$, 于是$a_n=\E[T_n]$. 注意到
  $$\frac{\sum_{j=1}^{n}Y_j-a_n}{b_n}=\frac{\sum_{j=1}^{n}\{Y_j-\E[Y_j]\}}{b_n}$$
  的均值为0, 根据Chebyshev不等式\footnote{对任意随机变量$X$和$\varepsilon>0$, 总有$\displaystyle\PP[|X-\E[X]|>\varepsilon]<\frac{\var(X)}{\varepsilon^2}$.}可知, 当$n\to\infty$时有
  \begin{align*}
  \PP\left[\left|\frac{T_n-a_n}{b_n}\right|>\varepsilon\right]&\leq\varepsilon^{-2}\E\left|\frac{T_n-a_n}{b_n}\right|^2=\varepsilon^{-2}b_n^{-2}\text{var}(T_n) \\
  &=(b_n\varepsilon)^{-2}\sum_{j=1}^{n}\text{var}(Y_j)\leq(b_n\varepsilon)^{-2}\sum_{j=1}^{n}\E[Y_j^2]\to0
  \end{align*}
  另一方面有
  $$\PP[S_n\neq T_n]\leq \PP\left[\bigcup_{j=1}^n\{X_j\neq Y_j\}\right]\leq\sum_{j=1}^{n}\PP[|X_j|>b_n]\to0$$
  从而
  $$\PP\left[\left|\frac{S_n-a_n}{b_n}\right|>\varepsilon\right]\leq \PP[S_n\neq T_n]+\PP\left[\left|\frac{T_n-a_n}{b_n}\right|>\varepsilon\right]\to0$$
  由此证得定理.
\end{proof}
上述结论实则是一般化的弱大数定律, 为了不必验证充分条件而更方便地使用它, 我们在此基础上做进一步的推导.
\begin{theorem}\label{thm:thm3.2}
  设$X_1, X_2,\cdots, X_n$是i.i.d.随机变量, 当$x\to\infty$时有
  $$x\PP[|X_i|>x]\to0$$
  令$S_n=X_1+\cdots+X_n$, $\mu_n=\E[X_1\mathbbm{1}\{|X_1|\leq n\}]$, 那么$S_n/n-\mu_n\xrightarrow{p}0$.
\end{theorem}
\begin{proof}
令$a_n=n\mu_n$, $b_n=n$, 显然有
  $$\sum_{j=1}^{n}\PP[|X_j|>n]=n\PP[|X_i|>n]\to0$$
  当$n\to\infty$时, 根据期望恒等式\footnote{对任意非负随机变量$X$和$p>0$, $\displaystyle\E[X^p]=\int_{0}^{\infty}px^{p-1}\PP[X>x]\,\text{d}x$.}可知
  \begin{align*}
  &b_n^{-2}\sum_{i=1}^n\E[X_i^2I(|X_i|\leq b_n)]=\frac{1}{n}\E[X^2\mathbbm{1}\{|X|\leq n\}]\leq\frac{1}{n}\E[\min\,\{|X|,n\}^2] \\
  =&\frac{1}{n}\int_{0}^{\infty}2x\PP[\min\,\{|X|,n\}>x]\,\text{d}x=\frac{1}{n}\int_{0}^{n}2x\PP[|X|>x]\,\text{d}x \\
  =&\frac{1}{n}\int_{M}^{n}2x\PP[|X|>x]\,\text{d}x+\text{o}(1)\leq 2\sup_{x\geq M}x\PP[|X|>x]+\text{o}(1)
  \end{align*}
  由于$0<M<n$是任意的, 因而$b_n^{-2}\sum_{i=1}^{n}\E[X_i^2\mathbbm{1}\{|X_i|\leq b_n\}]\to0$, 由定理\ref{thm:thm3.6}可知结论成立.
\end{proof}
\begin{corollary}[Khinchin弱大数定律]
设$X_1, X_2,\cdots, X_n$是i.i.d.随机变量, $S_n=X_1+\cdots+X_n$, $\E|X_1|<\infty$且$\mu=\E[X_1]$, 那么$S_n/n\xrightarrow{p}\mu$.
\end{corollary}
\begin{proof}
  令$Y=|X_1|\mathbbm{1}\{|X_1|>x\}$, 显然$Y>x\mathbbm{1}\{|X_1|>x\}$, 两端取期望得
  $$x\PP[|X_1|>x]\leq \E[|X_1|\mathbbm{1}\{|X_1|>x\}]=\int_{y}^{\infty}\PP[|X_1|>y]\,\text{d}y$$
  于是当$x\to\infty$时, $x\PP[|X_1|>x]\to0$. 另一方面
  $$X_1\mathbbm{1}\{|X_1|\leq n\}\to X_1,\quad n\to\infty$$
  又因为$\E|X_1|<\infty$, 根据控制收敛定理\footnote{如果$X_n\xrightarrow{a.s.}X$, 对于一切$n=1,2,\cdots$都有$|X_n|\leq Y$, 并且$\E[Y]<\infty$, 那么$\E[X_n]\to\E[X]$.}(Dominated Convergence Theorem, DCT)可知, 当$n\to\infty$时有
  $$\mu_n=\E[X_1\mathbbm{1}\{|X_1|\leq n\}]\to \E[X_1]=\mu$$
  再根据定理\ref{thm:thm3.2}, 对于任意的$\varepsilon>0$, 当$n\to\infty$时有$\PP[|S_n/n-\mu_n|>\varepsilon/2]\to0$, 因此由三角不等式可知
  $$\limn \PP[|S_n/n-\mu|>\varepsilon]=0$$
  由此证得推论.
\end{proof}
\begin{remark}
定理\ref{thm:thm3.2}中不需要矩条件$\E|X_1|<\infty$成立\footnote{它所需要的矩条件是更弱的$\E|X_1|^{1-\varepsilon}<\infty$, 其中$0<\varepsilon<1$. 因此不能说定理\ref{thm:thm3.2}取消了矩条件这一限制.}, 而Khinchin弱大数定律则需要这一条件. 另一方面, Khinchin弱大数定律可以扩展到独立同分布的$K\times1$维随机向量的情形, 只需将矩条件改为$\E||X_1||<\infty$即可.
\end{remark}
\begin{example}[弱大数定律不成立的情况]\label{eg4}
设$X_1,X_2,\cdots,X_n$是独立随机变量, 并且都服从标准Cauchy分布, 也即
$$\PP[X_1\leq x]=\int_{-\infty}^{x}\frac{\text{d}t}{\uppi(1+t^2)}$$
考虑条件
$$x\PP[|X_1|>x]=2x\int_{x}^{\infty}\frac{\text{d}t}{\uppi(1+t^2)}=x\left(1-\frac{2\arctan x}{\uppi}\right)$$
根据L'Hospital法则可知
$$\lim_{x\to\infty}x\left(1-\frac{2\arctan x}{\uppi}\right)=\lim_{t\to0}\frac{2}{\uppi(t^2+1)}=\frac{2}{\uppi}$$
因此定理\ref{thm:thm3.2}失效. 另一方面
$$\E|X_1|=\int_{-\infty}^{\infty}\frac{|x|}{\uppi(1+x^2)}\,\text{d}x=\lim_{x\to\infty}\frac{\log(1+x^2)}{\uppi}=\infty$$
也意味着Khinchin弱大数定律中的矩条件不成立.

实际上, 随机变量$\overbar{X}_n=S_n/n$的特征函数 (Characteristic Function, CF)为
\begin{align*}
\phi_{\overbar{X}_n}(t)=\E[\exp({\text{i}t\overbar{X}_n})]=\prod_{i=1}^{n}\E\left[\exp\left({\text{i}t\frac{X_i}{n}}\right)\right]=\left[\exp\left(-\left|\frac{t}{n}\right|\right)\right]^n=\exp(-|t|)
\end{align*}
因此$S_n/n$仍服从标准Cauchy分布, 而非依概率收敛于任何常数.
\end{example}

本节讨论的LLN都离不开i.i.d.这一条件, 后续在进行时间序列数据的计量分析时, 我们会使用遍历定理作为LLN的推广, 它允许随机变量间存在相关性.

\section{中心极限定理}
现在来看中心极限定理 (Central Limit Theorem, CLT). 我们首先给出Lindeberg-Feller CLT, 它是一种非常重要且一般化的中心极限定理.
\begin{theorem}[Lindeberg-Feller中心极限定理]
    假设对于一切$n\ge1$与$j=1,2,\cdots,r_n$, $\{X_{nj}\}$是独立随机变量构成的序列, 对于一切$1\leq j\leq r_n$都有:
    $$\E[X_{nj}]=0,\quad 0<\E[X_{nj}^2]\equiv\sigma_{nj}^2<\infty$$
    并且还满足Lindeberg条件:
    \begin{equation}\label{eq3.7}
      \limn s_n^{-2}\sum_{j=1}^{r_n}\E[X_{nj}^2\mathbbm{1}\{|X_{nj}|>\varepsilon s_n\}]=0
    \end{equation}
    那么当$n\to\infty$时有
    $$S_n/s_n\xrightarrow{d}N(0,1)$$
    其中$S_n=\sum_{j=1}^{r_n}X_{nj}$, 并且$s_n^2=\sum_{j=1}^{r_n}\sigma_{nj}^2$.
\end{theorem}
\begin{proof}
  见Athreya and Lahiri (2006) Theorem 11.1.1.
\end{proof}
\begin{corollary}[Lindeberg-Levy中心极限定理]
设$X_1,X_2,\cdots,X_n$为i.i.d.随机变量, 如果$\E[X_1^2]<\infty$, 那么
      $$\sqrt{n}(\overbar{X}_n-\mu)\xrightarrow{d}N(0,\sigma^2)$$
      其中$\overbar{X}_n=n^{-1}\sum_{j=1}^{n}X_j$, $\E[X_1]=\mu$, 并且$\text{var}(X_1)=\sigma^2<\infty$.
\end{corollary}
\begin{proof}
  令$\displaystyle Z_j=(X_j-\mu)/\sigma\sqrt{n}$, $S_n=\sum_{j=1}^{n}Z_j$, 以及
  $$s_n^2=\sum_{j=1}^{n}\text{var}(Z_j)=1$$
  考虑验证Lindeberg条件(\ref{eq3.7}). 当$n\to\infty$时有
      \begin{align*}
      s_n^{-2}\sum_{j=1}^{n}\E[X_j^2\mathbbm{1}\{|X_j|>\varepsilon s_n\}]&=\sum_{j=1}^{n}\E\left[\left(\frac{X_j-\mu}{\sigma\sqrt{n}}\right)^2\mathbbm{1}\left\{\left|\frac{X_j-\mu}{\sigma\sqrt{n}}\right|>\varepsilon\right\} \right]\\
      &=n\left\{\frac{1}{\sigma^2n}\E[(X_1-\mu)^2\mathbbm{1}\{|X_1-\mu|>\varepsilon\sigma\sqrt{n}\}]\right\} \\
      &=\sigma^{-2}\E[(X_1-\mu)^2\mathbbm{1}\{|X_1-\mu|>\varepsilon\sigma\sqrt{n}\}]\to0
      \end{align*}
      根据Lindeberg-Feller CLT可知$S_n/s_n\xrightarrow{d}N(0,1)$, 再由CMT即可推知
      $$\sqrt{n}(\overbar{X} _n-\mu)\xrightarrow{d}N(0,\sigma^2)$$
\end{proof}
\begin{remark}
以上两个定理的区别在于, Lindeberg-Feller CLT仅要求独立随机变量, 而Lindeberg-Levy CLT在此基础上还要求它们是同分布的.
\end{remark}
\begin{example}
正如例\ref{eg4}提到的那样, 由于Cauchy分布的任意阶矩不存在, 因此无法对其使用CLT, 其样本均值$\overbar{X}_n/n$也服从标准Cauchy分布.
\end{example}
现在我们将要把CLT从一元推广至多元情形, 这里仍和前面一样, 需要随机变量序列至少是独立的. 在正式推导之前, 我们直接给出一个引理.
\begin{lemma}[Cram\'{e}r-Wold方法]
设$\{X_n\}$是由$K\times1$维随机向量构成的序列, 那么$X_n\xrightarrow{d}X$, 当且仅当对于一切$\lambda\in\R^K$都有$\lambda'X_n\xrightarrow{d}\lambda'X$.
\end{lemma}
\begin{proof}
  见Athreya and Lahiri (2006) Theorem 10.4.5.
\end{proof}
\begin{theorem}[多元Lindeberg-Feller中心极限定理]
  设$n\ge1$与$j=1,2,\cdots,r_n$, $\{X_{nj}\}$是由$K\times1$维独立随机向量构成的序列, 对于一切$1\leq j\leq r_n$都有:
    $$\E[X_{nj}]=0,\quad v_n^2=\lambda_{\min}(\overbar{\V}_n)>0$$
    并且对于任意$\varepsilon>0$都有
    \begin{equation}\label{eq3.11}
      \limn v_n^{-2}\sum_{j=1}^{r_n}\E[||X_{nj}||^2\mathbbm{1}\{||X_{nj}||\geq\varepsilon v_n\}]=0
    \end{equation}
    那么当$n\to\infty$时
    $$\overbar{\V}_n^{-1/2}\sum_{j=1}^{r_n}X_{nj}\xrightarrow{d}N(0,\mathbold{I}_K)$$
    其中$\overbar{\V}_n=\sum_{j=1}^{r_n}\V_{nj}$, 并且$\V_{nj}=\E[X_{nj}X_{nj}']$.
\end{theorem}
\begin{proof}
  任取$\lambda\in\R^K$使得$\lambda'\lambda=1$. 定义$U_{nj}=\lambda'\overbar{\V}_n^{-1/2}X_{nj}$, 它的方差为
  $$\sigma^2_ {nj}=\lambda'\overbar{\V}_n^{-1/2}\V_{nj}\overbar{\V}_n^{-1/2}\lambda$$
  并且
  $$s_n^2=\sum_{j=1}^{r_n}\sigma_{nj}^2=\lambda'\lambda=1$$
  另一方面, 由Cauchy-Schwarz不等式和二次不等式可知
  $$||U_{nj}||^2\leq\lambda'\overbar{\V}_n^{-1}\lambda||X_{nj}||^2\leq\frac{||X_{nj}||^2}{\lambda_{\min}(\overbar{\V}_n)}=\frac{||X_{nj}||^2}{v_n^2}$$
  也即$||U_{nj}||\leq||X_{nj}||/v_n$. 由条件(\ref{eq3.11}), 当$n\to\infty$时
  \begin{align*}
  s_n^{-2}\sum_{j=1}^{r_n}\E[||U_{nj}||^2\mathbbm{1}\{U_{nj}\geq\varepsilon s_n\}]&=\sum_{j=1}^{r_n}\E[||U_{nj}||^2\mathbbm{1}\{U_{nj}\geq\varepsilon s_n\}]\\
  &\leq v_n^{-2}\sum_{j=1}^{r_n}\E[||X_{nj}||^2\mathbbm{1}\{||X_{nj}||\geq\varepsilon v_n\}]\to0
  \end{align*}
  根据Lindeberg-Feller CLT可知
  $$\sum_{j=1}^{r_n}U_{nj}=\lambda'\overbar{\V}_n^{-1/2}\sum_{j=1}^{r_n}X_{nj}\xrightarrow{d}\lambda'Z$$
  其中$Z\sim N(0,\mathbold{I}_K)$. 根据Cram\'{e}r-Wold方法即可推知
  $$\overbar{\V}_n^{-1/2}\sum_{j=1}^{r_n}X_{nj}\xrightarrow{d}N(0,\mathbold{I}_K)$$
  由此证得定理.
\end{proof}
\begin{corollary}[多元Lindeberg-Levy中心极限定理]
  设$X_1,X_2,\cdots,X_n\in\R^K$为i.i.d.随机向量, 如果$\E||X_1||^2<\infty$, 那么当$n\to\infty$时有
  $$\sqrt{n}(\overbar{X}_n-\mu)\xrightarrow{d}N(0,\V)$$
  其中$\mu=\E[X_1]$, 并且$\V=\E[(X_1-\mu)(X_1-\mu)']$.
\end{corollary}
\begin{proof}
  与一维情形时的类似, 只需令
  $$Z_j=\frac{1}{\sqrt{n}}\V_j^{-1/2}X_j$$
  然后验证Lindeberg条件(\ref{eq3.11}), 最后使用CMT即可.
\end{proof}
最后我们介绍Delta方法, 它是概率论意义上的Taylor展开, 可以将计量经济学中遇到的光滑可微的非线性统计量线性化, 进而保证CLT的使用. 因此, 它可以看作为CLT从样本均值到非线性统计量的推广.
\begin{theorem}[Delta方法]
  设$g:\Theta\to\R^L$为连续可微函数, $\Theta\subset\R^K$. 如果
  $$\sqrt{n}(\hat{\theta}_n-\theta)\xrightarrow{d} N(0,\V)$$
  并且$\theta$是$\Theta$的内点, 那么当$n\to\infty$时有
  \begin{equation}\label{eq3.12}
    \sqrt{n}[g(\hat{\theta}_n)-g(\theta)]\xrightarrow{d}N(0,\mathbold{G}\V\mathbold{G}')
  \end{equation}
  其中$\mathbold{G}=\nabla g(\theta)$为$g$的$L\times K$维Jocobi矩阵, $L\leq K$.
\end{theorem}
\begin{proof}
  根据中值定理可知
  \begin{align*}
  \sqrt{n}[g(\hat{\theta}_n)-g(\theta)]&=g'(\xi)\sqrt{n}(\hat{\theta}_n-\theta) \\
  &=\mathbold{G}\sqrt{n}(\hat{\theta}_n-\theta)+[g'(\xi)-\mathbold{G}]\sqrt{n}(\hat{\theta}_n-\theta)
  \end{align*}
  其中$\xi$位于$\hat{\theta}_n$和$\theta$之间. 由于$\hat{\theta}_n\xrightarrow{p}\theta$, 故而当$n\to\infty$时$\xi\xrightarrow{p}\theta$, 进而由CMT可知$g'(\xi)-\mathbold{G}=\text{o}_p(1)$, 再根据定理\ref{thm:thm3.1}(4)可知$\sqrt{n}(\hat{\theta}_n-\theta)=\text{O}_p(1)$, 于是
  $$\sqrt{n}[g(\hat{\theta}_n)-g(\theta)]\xrightarrow{p}\mathbold{G}\sqrt{n}(\hat{\theta}_n-\theta)$$
  由Slutsky定理可知(\ref{eq3.12})成立.
\end{proof}
\begin{example}
假设$\sqrt{n}(\overbar{X}_n-\mu)\xrightarrow{d}N(0,\sigma^2)$, $\mu\neq0$且$0<\sigma<\infty$, 现在要求出$\sqrt{n}(\overbar{X}_n^{-1}-\mu^{-1})$的极限分布.

令$g(\overbar{X}_n)=\overbar{X}_n^{-1}$, 由于$g$在$\mu$处连续可微, 根据一阶Taylor展开可知
$$g(\overbar{X}_n)=g(\mu)+g'(\overbar{\mu}_n)(\overbar{X}_n-\mu)$$
其中$\overbar{\mu}_n$介于$\overbar{X}_n$和$\mu$之间. 由于$\overbar{X}_n\xrightarrow{p}\mu$, 因此当$n\to\infty$时有
$$\sqrt{n}(\overbar{X}_n^{-1}-\mu^{-1})=-\frac{\sigma}{\overbar{\mu}_n^2}\sqrt{n}(\overbar{X}_n-\mu)\xrightarrow{d}N(0,\sigma^2/\mu^4)$$
\end{example}
\section{一致可积与矩收敛}
本章最后一节给出随机变量序列的一致可积(Uniformly Integrability, UI)概念及其性质, 它对于重抽样方法十分重要. 首先, 随机变量$X$是可积的, 如果$\E|X|<\infty$成立, 或等价地有
$$\lim_{M\to\infty}\E[|X|\mathbbm{1}\{|X|>M\}]=\lim_{M\to\infty}\int_{M}^{\infty}|x|\,\text{d}F(x)=0$$
下面给出一致可积的概念. 注意, 可积性是关于随机变量的概念, 而一致可积性针对的是随机变量序列.
\begin{definition}
随机变量序列$\{X_n\}$是一致可积的, 如果
\begin{equation}\label{eq3.13}
  \lim_{M\to\infty}\sup_{n\ge1}\E[|X_n|\mathbbm{1}\{|X_n|>M\}]=0 \nonumber
\end{equation}
随机向量序列$\{X_n\}$是一致可积的, 如果
$$\lim_{M\to\infty}\sup_{n\ge1}\E[||X_n||\mathbbm{1}\{||X_n||>M\}]=0$$
\end{definition}
\begin{remark}
随机变量序列的一致可积并不意味着序列内的元素都是可积的, 这一点与数学分析中的一致连续和一致收敛概念具有很大不同.
\end{remark}
根据定义可以直接判断, 如果序列$\{X_n\}$内的随机向量是独立同分布的, 并且$\E||X_1||<\infty$, 那么该序列一致可积.

一致可积是更一般化的概念, 对于非同分布的随机向量序列, 对其施加一定正则条件后, i.i.d.序列的结果即可应用到一致可积的序列上. 例如, 如果各个$X_n$是独立的并构成一致可积序列, 那么WLLN仍然成立.

另一方面, 我们也可以对随机向量的幂运用一致可积性概念, 假设以下所用符号与多元Lindeberg-Feller CLT中的相同. 我们称$\{X_n^2\}$是一致平方可积的 (uniformly square integrable), 如果
\begin{equation}\label{eq3.14}
  \lim_{M\to\infty}\sup_{n\ge1}\E[||X_n||^2\mathbbm{1}\{||X_n||^2>M\}]=0
\end{equation}
它是一个与Lindeberg条件相似, 但稍强一点的条件: 对于任意给定的$\varepsilon>0$, 选取正数$\delta$使得$0<\delta<v_n^2$. 定义$X_n=Z_{nj}$, 再选取充分大的$M$使得(\ref{eq3.14})成立, 也即
$$\sup_{n\ge1}\E[||X_n||^2\mathbbm{1}\{||X_n||^2>M\}]<\delta\varepsilon$$
因为当$n\to\infty$时, $n\varepsilon v_n\to\infty$, 因此
$$\frac{1}{nv_n^2}\sum_{j=1}^n\E[||Z_{nj}||^2\mathbbm{1}\{||Z_{nj}||>n\varepsilon v_n\}]\leq\frac{\delta\varepsilon}{v_n^2}<\varepsilon$$
也即Lindeberg条件成立.

直接按照定义来判断随机变量序列是否是一致可积的显然十分麻烦, 现在我们给出一致可积的Liapunov充分条件.
\begin{theorem}[Liapunov条件]
  如果存在$r>1$, $\E||X_n||^r\leq C<\infty$, 那么$\{X_n\}$一致可积.
\end{theorem}
\begin{proof}
  任取$\varepsilon>0$, 再取$M\geq(C/\varepsilon)^{1/(r-1)}$, 于是
  \begin{align*}
  \E[||X_n||\mathbbm{1}\{||X_n||>M\}]&=\E\left[\frac{||X_n||^r}{||X_n||^{r-1}}\mathbbm{1}\{||X_n||>M\}\right]\\
  &\leq\frac{\E[||X_n||^r\mathbbm{1}\{||X_n||>M\}]}{M^{r-1}}\\
  &\leq\frac{\E||X_n||^r}{M^{r-1}}\leq\frac{C}{M^{r-1}}<\varepsilon
  \end{align*}
  由于上式对一切$n$成立, 因此对上确界也成立.
\end{proof}
\begin{theorem}\label{thm:thm3.7}
  如果随机向量序列$\{||X_i||^r\}$是一致可积的, 那么当$n\to\infty$时有
  \begin{equation}\label{eq3.15}
    n^{-1/r}\max_{1\leq i\leq n}\,||X_i||\xrightarrow{p}0
  \end{equation}
\end{theorem}
\begin{proof}
  任取$\delta>0$, 事件$\displaystyle\left\{\max_{1\leq i\leq n}\,||X_i||>\delta n^{1/r}\right\}$表明至少存在某个$||X_i||$大于$\delta n^{1/r}$, 故而它和事件$\{\bigcup_{n=1}^\infty\{||X_i||>\delta n^{1/r}\}\}$是等价的.

  根据测度的有限次可加性得到
  \begin{align*}
  \PP\left[n^{-1/r}\max_{1\leq i\leq n}\,||X_i||>\delta\right]&=\PP\left[\bigcup_{i=1}^n\,\{||X_i||^r>n\delta^r\}\right] \\
  &\leq\sum_{i=1}^{n}\PP[||X_i||^r>n\delta^r] \\
  &\leq\frac{1}{n\delta^r}\sum_{i=1}^{n}\E[||X_i||^r\mathbbm{1}\{||X_i||^r>n\delta^r\}] \\
  &\leq\delta^{-r}\max_{1\leq i\leq n}\,\E[||X_i||^r\mathbbm{1}\{||X_i||^r>n\delta^r\}]
  \end{align*}
  根据一致可积性, 当$n\delta^r\to\infty$时, 上式收敛于0.
\end{proof}
(\ref{eq3.15})表明在一致可积性下, 当$n\to\infty$时, $||X_i||$最大的观测值收敛的速率低于$n^{1/r}$, 并且随着$r$增大, 收敛速率越慢.

有时我们对统计量的矩感兴趣, 假设$X_1,X_2,\cdots,X_n$是均值为$\mu$, 方差为$\sigma^2$的i.i.d.样本, 定义标准化的样本均值统计量
$$Z_n=\sqrt{n}(\overbar{X}_n-\mu)$$
则它的均值为0, 方差为$\sigma^2$. 不仅如此, 还可以计算出$Z_n$的高阶矩并得到显式表达, 例如
$$\E[Z_n^3]=\frac{\kappa_3}{\sqrt{n}},\quad \E[Z_n^4]=\frac{\kappa_4}{n}+3\sigma^2$$
其中$\kappa_n$为$X_1$的$n$阶矩. 进一步, 如果存在$X_1$的有限$r$阶矩, 那么$\E[Z_n]\to\E[Z]$, 其中$Z\sim N(0,\sigma^2)$. 然而对于样本均值的非线性统计量, 则不一定能得到$Z_n$高阶矩的表达式.

另一方面, 之前我们给出了依分布收敛$X_n\xrightarrow{d}X$, 现在我们想知道如何得到$\E[X_n]\to\E[X]$, 下面先给出一个渐近分布的均值存在的充分条件.
\begin{theorem}\label{thm:thm3.4}
  如果$X_n\xrightarrow{d}X$, 并且$\E||X_n||\leq C$, 那么$\E||X||\leq C$.
\end{theorem}
\begin{proof}
  这里只证明一维形式. 根据期望恒等式, 对于任意非负随机变量$X$都有
  \begin{equation}\label{eq3.16}
    \E[X]=\int_{0}^{\infty}\PP[X>u]\,\text{d}u
  \end{equation}
  设$F_n$和$F$分别为$|X_n|$和$|X|$的累积分布, 根据(\ref{eq3.16})和Fatou引理\footnote{如果$X_n$为非负随机变量, 那么
  $\displaystyle\E\left[\liminf_{n\to\infty}X_n\right]\leq\liminf_{n\to\infty}\E[X_n]$.}可知
  \begin{align*}
  \E|X|&=\int_{0}^{\infty}[1-F(x)]\,\text{d}x=\int_{0}^{\infty}\limn[1-F(x)]\,\text{d}x \\
  &\leq\liminf_{n\to\infty}\int_{0}^{\infty}[1-F_n(x)]\,\text{d}x=\liminf_{n\to\infty}\E|X_n|\leq C
  \end{align*}
  由此证得定理.
\end{proof}
值得注意的是, 该定理并不意味着$\E[X_n]\to\E[X]$. 考虑随机变量$X_n$, 它的概率分布为$\PP[X_n=n]=1/n$, 并且$\PP[X_n=0]=1-1/n$, 于是$X_n\xrightarrow{d}X$并且$\PP[X=0]=1$, 显然$\E[Z_n]=1$并不收敛于$\E[Z]=0$, 然而此时(\ref{eq3.16})也成立. 究其原因, 这是因为累积分布序列$\{F_n\}$缺少紧性 (tightness), 解决方法则是一致可积.
\begin{theorem}\label{thm:thm3.5}
  如果$X_n\xrightarrow{d}X$并且$\{X_n\}$是一致可积的, 那么$\E[X_n]\to\E[X]$.
\end{theorem}
\begin{proof}
  这里只证明一维的形式. 记$X_n=X_n^+-X_n^-$, 其中$X_n^+=\max\,\{X_n,0\}$, $X_n^-=-\min\,\{X_n,0\}$, 因此只需证明$X_n$非负的情况. 记$a\wedge b=\min\,\{a,b\}$.

  任取$\varepsilon>0$, 根据定理\ref{thm:thm3.4}可知$X$是可积的, 于是
  $$\E[X-(X\wedge M)]=\E[(X-M)\mathbbm{1}\{X>M\}]\leq\E[X\mathbbm{1}\{X>M\}]<\varepsilon$$
  根据条件, 随机变量序列$\{X_n\}$一致可积, 故而存在正数$M<\infty$, 使得对于充分大的$n$有
  $$\E[X_n-(X_n\wedge M)]=\E[(X_n-M)\mathbbm{1}\{X_n>M\}]\leq\E[X_n\mathbbm{1}\{X_n>M\}]<\varepsilon$$
  由于函数$X_n\wedge M$是连续有界的, 又因为$X_n\xrightarrow{d}X$, 根据CMT和DCT可知$\E[X_n\wedge M]\to\E[X\wedge M]$, 也即对充分大的$n$有
  $$|\E[(X_n\wedge M)-(X\wedge M)]|<\varepsilon$$
  根据三角不等式可知
  \begin{align*}
  |\E[X_n-X]|&\leq|\E[X_n-(X_n\wedge M)]|\\
  &\quad+|\E[(X_n\wedge M)-(X\wedge M)]|\\
  &\quad+|\E[X-(X\wedge M)]|<3\varepsilon
  \end{align*}
  由此证得定理.
\end{proof}
\begin{remark}
该定理同样可以推广至随机向量的情形. 另一方面, 根据定理\ref{thm:thm3.1}可知, 几乎必然收敛, 依$L^p$收敛和依概率收敛可以推出依分布收敛, 因此也能得到相同的结论.
\end{remark}

可以证明, 随机变量序列$\{X_n\}$的一致可积等价于它在$L^1(\Omega,\F,\PP)$上一致有界且一致绝对连续. 另一方面, DCT中被可测函数所控制的函数序列必定是一致可积的, 因此定理\ref{thm:thm3.5}是DCT的推广, 证明见Davison (2020) Theorem 12.11.


\chapter{最小二乘的渐近性质}
第二章分析了OLS估计量的有限样本性质, OLS在假设\ref{pro:pro2.1}$-$\ref{pro:pro2.4}下是BLUE, 但是这个结论所需要的假设过于严格. 第三章我们主要介绍了渐近理论基础, 本章主要是渐近理论在OLS模型上的应用, 在大样本框架下推导其渐近性质.
\section{OLS的一致性}
这一节将要证明, 在更为宽松的假设条件下, OLS估计量是对线性投影系数的一致估计, 也即随着样本容量的增大, $\hb$将会依概率收敛到线性投影系数$\beta$.
\begin{definition}
如果当$n\to\infty$时有$\hat{\theta}_n\xrightarrow{p}\theta$, 则称$\hat{\theta}_n$是$\theta$的 (弱)一致估计量.
\end{definition}
\begin{example}[有偏但一致的估计量]
设$\{T_n\}$为$\theta$的一系列估计量构成的集合, $\delta\ne0$, 并且$T_n$具有以下概率分布
\begin{table}[!htbp]
\centering
\begin{tabular}{lcc}
$T_n$     & $\theta$   & $n\delta+\theta$                           \\
$\PP$ & $\displaystyle 1-\frac{1}{n}$ & $\displaystyle \frac{1}{n}$
\end{tabular}
\end{table}

\noindent 于是$T_n\xrightarrow{p}\theta$, 但是$\E[T_n]=\theta+\delta$.
\end{example}
在正式进行推导前, 我们先给出一系列正则条件.
\begin{proposition}\label{pro:pro4.1}
(1) $\{Y_i,X_i\}_{i=1}^n$是可观测的i.i.d.随机样本.

(2) $\E[Y_i^2]<\infty$.

(3) $\E||X_i||^2<\infty$.

(4) $\Q_{XX}=\E[X_iX_i']$为正定矩阵.
\end{proposition}
\begin{remark}
假设\ref{pro:pro4.1}或许和诸多计量经济学教材给出的假设均有所不同, 实际上这里给出的假设是较弱的充分条件.
\end{remark}
\begin{theorem}\label{thm:thm4.7}
  在假设\ref{pro:pro4.1}下, 当$n\to\infty$时, OLS估计量$\hb\xrightarrow{p}\beta$, 也即$\hb=\beta+\text{o}_p(1)$.
\end{theorem}
\begin{proof}
  首先写出$\hb-\beta$的表达式
  \begin{align*}
  \hb-\beta&=\left(n^{-1}\sum_{i=1}^{n}X_iX_i'\right)^{-1}\left(n^{-1}\sum_{i=1}^{n}X_ie_i\right) \\
  &=\hat{\Q}_{XX}^{-1}\hat{\Q}_{Xe}
  \end{align*}
  一方面, 根据假设\ref{pro:pro4.1}(4)可知WLLN所需要的矩条件成立, 也即
  $$\E|X_{ji}X_{li}|\leq(\E[X_{ji}^2])^{1/2}(\E[X_{li}^2])^{1/2}<\infty,\quad \forall 1\leq j,l\leq K$$
  于是根据WLLN和CMT可知
  $$\hat{\Q}_{XX}^{-1}\xrightarrow{p}(\E[X_iX_i])^{-1}=\Q_{XX}^{-1}$$
  另一方面, 根据定理\ref{thm:thm1.5}(3)和WLLN可知
  $$\hat{\Q}_{Xe}\xrightarrow{p}\E[X_ie_i]=0$$
  由于函数$g(\mathbold{A},b)=\mathbold{A}^{-1}b$对一切使得$\mathbold{A}^{-1}$存在的$(\mathbold{A},b)$连续, 因此根据CMT得到
  $$\hb-\beta=\hat{\Q}_{XX}^{-1}\hat{\Q}_{Xe}\xrightarrow{p}\Q_{XX}^{-1}0=0$$
  由此证得定理.
\end{proof}
\begin{remark}
假设\ref{pro:pro4.1}(2)中提到了矩条件$\E[Y_i^2]<\infty$和$\E||X_i||^2<\infty$, 尽管它们并没有在上述定理的证明中使用, 但却是证明定理\ref{thm:thm1.5}(3)的必要条件. 从这个角度可以看出, 证明OLS估计量一致性的关键在于$\E[X_ie_i]=0$, 也即每个回归元和误差项不相关.
\end{remark}

参考Hong (2020)的方法, 我们可以将假设\ref{pro:pro4.1}(2), (3)加强为更直观的条件: $\E[e_i|X_i]=0$且$\E[\varepsilon_i^2]=\sigma^2<\infty$. 根据简单LIE可知
$$\E[X_ie_i]=\E[\E[X_ie_i|X_i]]=\E[X_i\E[e_i|X_i]]=0$$
并且WLLN所需要的矩条件也成立, 从而可以得到OLS一致性的证明.

Amemiya (1985)则提到了另一种证明OLS为一致估计的方法, 只需要当$n\to\infty$时
\begin{equation}\label{eq4.1}
  \lambda_{\min}(\X'\X)\to\infty
\end{equation}
其中$\lambda_{\min}(\mathbold{X}'\X)$表示矩阵$\X'\X$的最小特征根. 根据主成分分析, 特征值可以用来刻画矩阵的信息含量, 因此(\ref{eq4.1})表明随着样本容量的增加, $\X'\X$中包含的信息对于线性投影系数$\beta$的估计精度也会无限增加.

事实上, 根据二次不等式可知对于任意$\tau\in\R^K$, 如果$\tau'\tau=1$, 那么当$n\to\infty$时
\begin{align*}
\tau'\var(\hb|\X)\tau&=\sigma^2\tau'(\X'\X)^{-1}\tau \\
&\leq \sigma^2\lambda_{\max}[(\X'\X)^{-1}]\\
&\leq \sigma^2\lambda_{\min}^{-1}(\X'\X)\to0
\end{align*}
也即OLS估计量$\hb$的条件方差将趋近于0, 从而依概率收敛到真实参数.
\section{OLS的渐近正态性}
上一节分析了OLS估计量$\hb$的一致性, 但是一致性无法具体描述$\hb$在大样本下的渐近分布. 为了应用CLT, 假设\ref{pro:pro4.1}不够充分, 为此我们给出以下更强的假设条件.
\begin{proposition}\label{pro:pro4.2}
(1) $\{Y_i,X_i\}_{i=1}^n$是可观测的i.i.d.随机样本.

(2) $\E[Y_i^4]<\infty$.

(3) $\E||X_i||^4<\infty$.

(4) $\Q_{XX}=\E[X_iX_i']$为正定矩阵.
\end{proposition}
在假设\ref{pro:pro4.2}中, 我们要求$Y$和$X$的四阶矩有限, 由于高阶矩有限意味着低阶矩也有限, 因此假设\ref{pro:pro4.2}可以推出假设\ref{pro:pro4.1}.
\begin{theorem}\label{thm:thm4.1}
  在假设\ref{pro:pro4.2}下, 协方差矩阵
  $$\mathbold{\Omega}\equiv\E[X_iX_i'e_i^2]$$
  是有限的, 并且当$n\to\infty$时
  $$\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_ie_i\xrightarrow{d}N(0,\mathbold{\Omega})$$
\end{theorem}
\begin{proof}
  首先, 根据谱范数的性质, 对任意实矩阵都有
  $$||X_iX_i'||^{1/2}=||X_i||$$
  在假设\ref{pro:pro4.2}下, 根据Cauchy-Schwarz不等式可知
  $$||\mathbold{\Omega}||\leq\E||X_iX_i'e_i^2||=\E[||X_i||^2e_i^2]\leq(\E||X_i||^4)^{1/2}(\E[e^4])^{1/2}<\infty$$

  因为样本$\{Y_i,X_i\}$是i.i.d.的, 故而$\{X_ie_i\}$也是, 又因为$\E[X_ie_i]=0$, 根据多元Lindeberg-Levy CLT可知
  $$\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_ie_i=\sqrt{n}\hat{\Q}_{XX}\xrightarrow{d}N(0,\mathbold{\Omega})$$
  其中$\hat{\Q}_{XX}=n^{-1}\sum_{i=1}^{n}X_iX_i'$.
\end{proof}
\begin{theorem}\label{thm:thm4.2}
  在假设\ref{pro:pro4.2}下, 当$n\to\infty$时
  \begin{equation}\label{eq4.2}
    \sqrt{n}(\hb-\beta)\xrightarrow{d}N(0,\mathbold{V}_\beta)
  \end{equation}
  其中$\Q_{XX}=\E[X_iX_i']$, $\mathbold{\Omega}=\E[X_iX_i'e_i^2]$, 并且
  $$\V_\beta=\Q_{XX}^{-1}\mathbold{\Omega}\Q_{XX}^{-1}$$
\end{theorem}
\begin{proof}
  注意到
  $$\sqrt{n}(\hb-\beta)=\left(\frac{1}{n}\sum_{i=1}^{n}X_iX_i'\right)^{-1}\left(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_ie_i\right)$$
  由于
  $$\hat{\Q}_{XX}^{-1}\xrightarrow{p}\Q_{XX}^{-1}$$
  其中$\hat{\Q}_{XX}=n^{-1}\sum_{i=1}^{n}X_iX_i'$. 再根据定理\ref{thm:thm4.1}与Slutsky定理可知(\ref{eq4.2})成立.
\end{proof}
\begin{remark}
该定理表明$\hat{\beta}=\beta+\text{O}_p(n^{-1/2})$, 这是比一致性更强的结论.
\end{remark}
通常来说, 我们称$\text{avar}[\sqrt{n}(\hb-\beta)]=\V_\beta$为$\sqrt{n}(\hb-\beta)$的渐近协方差矩阵, $\V_\beta$的表达式$\Q_{XX}^{-1}\mathbold{\Omega}\Q_{XX}^{-1}$呈现出矩阵$\mathbold{\Omega}$被$\Q_{XX}^{-1}$所夹住的夹心 (sandwich)形式.

现在来看$\hb$在有限样本下的条件方差
\begin{equation}\label{eq4.3}
  \V_{\text{OLS}}=(\X'\X)^{-1}(\X'\mathbold{\Sigma}\X)(\X'\X)^{-1}
\end{equation}
由于$\V_{\text{OLS}}$是$\hb$精确的条件方差, 而$\V_\beta$是$\sqrt{n}(\hb-\beta)$的渐近方差, 那么可以得出$\V_{\beta}\approx n\V_{\text{OLS}}$. 现在将(\ref{eq4.3})乘以$n$倍得到
$$n\V_{\text{OLS}}=\left(\frac{1}{n}\X'\X\right)^{-1}\left(\frac{1}{n}\X'\mathbold{\Sigma}\X\right)^{-1}\left(\frac{1}{n}\X'\X\right)^{-1}$$
当$n\to\infty$时, 确实有$n\V_{\text{OLS}}\xrightarrow{p}\V_\beta$.

在某些特殊情况下, $\mathbold{\Omega}$和$\V_\beta$的表达式可以简化, 条件为
\begin{equation}\label{eq4.4}
  \text{cov}(X_iX_i',e_i^2)=0
\end{equation}
此时
\begin{align*}
\mathbold{\Omega}&=\E[X_iX_i']\E[e_i^2]=\sigma^2\Q_{XX} \\
\V_\beta&=\Q_{XX}^{-1}\mathbold{\Omega}\Q_{XX}^{-1}=\sigma^2\Q_{XX}^{-1}\equiv \V_\beta^0
\end{align*}
其中$\sigma^2=\E[e_i^2]$. 我们称$\V_\beta^0$是$\sqrt{n}(\hb-\beta)$在同方差下的渐近协方差矩阵.

值得注意的是, 尽管我们证明了$\sqrt{n}(\hb-\beta)$服从渐近正态分布, 但对于任意固定的样本容量$n$, $\sqrt{n}\hb$的分布也可能与正态分布相距甚远. 随着样本容量$n$增大, $\sqrt{n}\hb$将会越来越接近正态分布, 但多大的$n$才能使这种逼近足够好? 仍然没有简单确切的答案. 事实上, 不论样本容量$n$有多大, 对于某些满足假设\ref{pro:pro4.2}的数据, 正态分布的逼近程度可以任意差.

最后我们来看分块回归的情形. 首先将矩阵分割为$X'=[X_1',X_2']$及$\beta=[\beta_1',\beta_2']'$, 此时
\begin{align*}
Y&=X'\beta+e \\
&=X_1'\beta_1+X_2'\beta_2+e
\end{align*}
再做分割
$$\Q_{XX}=\begin{bmatrix}
            \Q_{11} & \Q_{12} \\
            \Q_{21} & \Q_{22}
          \end{bmatrix},\quad \BO=\begin{bmatrix}
                                    \BO_{11} & \BO_{12} \\
                                    \BO_{21} & \BO_{22}
                                  \end{bmatrix}$$
根据(\ref{eq1.19})可知
$$\Q_{XX}^{-1}=\begin{bmatrix}
                 \Q_{11\cdot2}^{-1} & -\Q_{11\cdot2}^{-1}\Q_{12}\Q_{22}^{-1} \\
                 -\Q_{22\cdot1}^{-1}\Q_{21}\Q_{11}^{-1} & \Q_{22\cdot1}^{-1}
               \end{bmatrix}$$
其中$\Q_{11\cdot2}=\Q_{11}-\Q_{12}\Q_{22}^{-1}\Q_{21}$, 以及$\Q_{22\cdot1}=\Q_{22}-\Q_{21}\Q_{11}^{-1}\Q_{12}$, $\BO$的各分块由$\Q_{XX}$的各分块决定. 最终可以将渐近协方差矩阵$\V_\beta$记作
$$\V_\beta=\begin{bmatrix}
             \V_{11} & \V_{12} \\
             \V_{21} & \V_{22}
           \end{bmatrix}$$
可以证明
\begin{align*}
\V_{11}&=\Q_{11\cdot2}^{-1}(\BO_{11}-\Q_{12}\Q_{22}^{-1}\BO_{21}-\BO_{12}\Q_{22}^{-1}\Q_{21}+\Q_{12}\Q_{22}^{-1}\BO_{22}\Q_{22}^{-1}\Q_{21})\Q_{11\cdot2}^{-1} \\
\V_{21}&=\Q_{22\cdot1}^{-1}(\BO_{21}-\Q_{21}\Q_{11}^{-1}\BO_{11}-\BO_{22}\Q_{22}^{-1}\Q_{21}+\Q_{21}\Q_{11}^{-1}\BO_{12}\Q_{22}^{-1}\Q_{21})\Q_{11\cdot2}^{-1} \\
\V_{22}&=\Q_{22\cdot1}^{-1}(\BO_{22}-\Q_{21}\Q_{11}^{-1}\BO_{12}-\BO_{21}\Q_{11}^{-1}\Q_{12}+\Q_{21}\Q_{11}^{-1}\BO_{12}\Q_{11}^{-1}\Q_{12})\Q_{22\cdot1}^{-1}
\end{align*}

\section{渐近方差估计量}
利用大数定律, 我们先将证明在假设\ref{pro:pro4.1}下, 估计量$\hat{\sigma}^2=n^{-1}\sum_{i=1}^{n}\hat{e}_i^2$与$s^2=e'e/(n-K)$是对$\sigma^2$的一致估计.
\begin{theorem}\label{thm:thm4.3}
  在假设\ref{pro:pro4.1}下, 当$n\to\infty$时有$\hat{\sigma}^2\xrightarrow{p}\sigma^2$及$s^2\xrightarrow{p}\sigma^2$.
\end{theorem}
\begin{proof}
  首先将OLS估计残差写为
  $$\hat{e}_i=Y_i-X_i'\hat{\beta}=e_i-X_i'(\hb-\beta)$$
  从而
  \begin{equation}\label{eq4.6}
    \hat{e}_i^2=e_i^2-2e_iX_i'(\hb-\beta)+(\hb-\beta)'X_iX_i'(\hb-\beta)
  \end{equation}
  以及
  \begin{align}
      \hat{\sigma}^2&=n^{-1}\sum_{i=1}^{n}e_i^2-2\left(n^{-1}\sum_{i=1}^{n}e_iX_i'\right)(\hb-\beta) \nonumber \\
      &\quad +(\hb-\beta)'\left(n^{-1}\sum_{i=1}^{n}X_iX_i'\right)(\hb-\beta) \label{eq4.5}
  \end{align}
  根据WLLN可知
  \begin{align*}
  &n^{-1}\sum_{i=1}^{n}e_i^2\xrightarrow{p}\sigma^2\\
  &n^{-1}\sum_{i=1}^{n}e_iX_i'\xrightarrow{p}\E[e_iX_i']=0 \\
  &n^{-1}\sum_{i=1}^{n}X_iX_i'\xrightarrow{p}\E[X_iX_i']=\Q_{XX}
  \end{align*}
  根据OLS估计量的一致性可知(\ref{eq4.5})依概率收敛于$\sigma^2$. 由于当$n\to\infty$时, $\displaystyle\frac{n}{n-K}\to1$, 因此
  $$s^2=\frac{n}{n-K}\hat{\sigma}^2\xrightarrow{p}\sigma^2$$
  由此证得定理.
\end{proof}

定理\ref{thm:thm4.2}表明$\sqrt{n}(\hb-\beta)$服从渐近正态分布并且协方差矩阵为$\V_\beta$, 出于渐近统计推断的目的, 我们需要得到$\V_\beta$的一致估计量. 下面将分同方差和异方差的情形进行讨论.
\begin{theorem}
  在假设\ref{pro:pro4.1}下, 当$n\to\infty$时有
  $$\hat{\V}_\beta^0\xrightarrow{p}\V_\beta^0$$
  其中$\hat{\V}_\beta^0=s^2\hat{\Q}_{XX}^{-1}$.
\end{theorem}
\begin{proof}
  根据定理\ref{thm:thm4.3}可知$s^2\xrightarrow{p}\sigma^2$, 又因为$\hat{\Q}_{XX}\xrightarrow{p}\Q_{XX}$, 根据CMT即可得出结论.
\end{proof}
\begin{remark}
该定理并不要求回归满足同方差条件, 无论是同方差还是异方差总有$\hat{\V}_\beta^0\xrightarrow{p}\V_\beta^0$, 只是说在同方差条件下有$\text{avar}(\sqrt{n}\hb)=\sigma^2\Q_{XX}^{-1}$, 我们才选择$\hat{\V}_{\beta}^0$作为它的一致估计量.
\end{remark}
现在考虑异方差的情形, 此时协方差矩阵$\V_\beta=\Q_{XX}^{-1}\mathbold{\Omega}\Q_{XX}^{-1}$无法进一步化简. 正如前面提到的$\hat{\Q}_{XX}^{-1}\xrightarrow{p}\Q_{XX}^{-1}$, 我们只需得到$\mathbold{\Omega}$的一致估计量即可. 首先有
$$\hat{\Om}^\text{ideal}=\frac{1}{n}\sum_{i=1}^{n}X_iX_i'e_i^2\xrightarrow{p}\Om$$
由于$e_i$不可观测, 一个自然的想法是用$\hat{e}_i$替代$e_i$并得到
\begin{align}
\hat{\V}_\beta^{\text{HC0}}&=\hat{\Q}_{XX}^{-1}\hat{\mathbold{\Omega}}\hat{\Q}_{XX}^{-1} \label{eq4.12} \\
&=n(\X'\X)^{-1}\left(\sum_{i=1}^{n}X_iX_i'\hat{e}_i^2\right)(\X'\X)^{-1} \nonumber
\end{align}
幸运的是, 这样的想法是可行的.
\begin{theorem}\label{thm:thm4.4}
  在假设\ref{pro:pro4.2}下, 当$n\to\infty$时有$\hat{\mathbold{\Omega}}\xrightarrow{p}\mathbold{\Omega}$以及
  $$\hat{\V}_\beta^{\text{HC0}}\xrightarrow{p}\V_\beta$$
\end{theorem}
\begin{proof}
  首先将$\hat{\mathbold{\Omega}}$写为
  $$\hat{\mathbold{\Omega}}=\frac{1}{n}\sum_{i=1}^{n}X_iX_ie_i^2+\frac{1}{n}\sum_{i=1}^{n}X_iX_i(\hat{e}_i^2-e_i^2)$$
  在假设\ref{pro:pro4.2}下, 相关矩条件成立, 于是根据WLLN可知上式第一项
  $$\frac{1}{n}\sum_{i=1}^{n}X_iX_i'e_i^2\xrightarrow{p}\E[X_iX_i'e_i^2]=\mathbold{\Omega}$$
  为了证明$\hat{\mathbold{\Omega}}$是$\mathbold{\Omega}$的一致估计量, 我们需要证明当$n\to\infty$时有
  $$\frac{1}{n}\sum_{i=1}^{n}X_iX_i'(\hat{e}_i^2-e_i^2)\xrightarrow{p}0$$
  首先可以得到
  \begin{equation}\label{eq4.7}
    \left|\left|\frac{1}{n}\sum_{i=1}^{n}X_iX_i'(\hat{e}_i^2-e_i^2)\right|\right|\leq\frac{1}{n}\sum_{i=1}^{n}||X_iX_i'(\hat{e}_i^2-e_i^2)||=\frac{1}{n}\sum_{i=1}^{n}||X_i||^2|\hat{e}_i^2-e_i^2|
  \end{equation}
  再对(\ref{eq4.6})运用三角不等式和Schwarz不等式\footnote{对于任意$K\times1$维向量$a$和$b$, 总有$|a'b|\leq||a||\,||b||$.}得到
  \begin{align}
  |\hat{e}_i^2-e_i^2|&\leq2|e_iX_i'(\hb-\beta)|+|(\hb-\beta)'X_iX_i'(\hb-\beta)| \nonumber \\
  &=2|e_i|\,|X_i'(\hb-\beta)|+|(\hb-\beta)'X_i|^2 \nonumber \\
  &\leq2|e_i|\,||X_i||\,||\hb-\beta||+||X_i||^2||\hb-\beta||^2 \label{eq4.8}
  \end{align}
  将(\ref{eq4.7})和(\ref{eq4.8})结合得到
  \begin{align}
  \left|\left|\frac{1}{n}\sum_{i=1}^{n}X_iX_i'(\hat{e}_i^2-e_i^2)\right|\right|&\leq2\left(\frac{1}{n}\sum_{i=1}^{n}||X_i||^3|e_i|\right)||\hb-\beta||+\left(\frac{1}{n}\sum_{i=1}^{n}||X_i||^4\right)||\hb-\beta||^2 \nonumber \\
  &=\text{o}_p(1) \label{eq4.9}
  \end{align}
  也即$\hat{\mathbold{\Omega}}\xrightarrow{p}\mathbold{\Omega}$.
\end{proof}
\begin{remark}
式(\ref{eq4.9})成立所需要的矩条件
  $$\E[||X_i||^3|e_i|]\leq(\E||X_i||^4)^{3/4}(\E[e^4])^{1/4}<\infty$$
  是根据H\"{o}lder不等式\footnote{设$p,q>1$且$1/p+1/q=1$, 则对于任意$K\times L$维矩阵, 总有$\E||X'Y||\leq(\E||X||^p)^{1/p}(\E||Y||^q)^{1/q}$.}得到的, 故而$n^{-1}\sum_{i=1}^{n}||X_i||^3|e_i|=\text{O}_p(1)$, 因此上式不等号右端的第一项是$\text{o}_p(1)$.
\end{remark}
下面我们将用另一种方法证明一个比$\hat{\mathbold{\Omega}}=\mathbold{\Omega}+\text{o}_p(1)$更强的结论. 首先由Schwarz不等式可知
\begin{equation}\label{eq4.10}
  |\hat{e}_i-e_i|=|X_i(\hb-\beta)|\leq||X_i||\,||\hb-\beta||
\end{equation}
我们先来得到证明上式的有界性.

考虑矩条件$\E||X_i||^r<\infty$, 根据Liapunov充分条件可知随机变量序列$\{X_n\}$一致可积, 再根据定理\ref{thm:thm3.7}得到
$$n^{-1/r}\max_{1\leq i\leq n}\,||X_i||\xrightarrow{p}0$$
又因为$||\hb-\beta||=\text{O}_p(n^{-1/2})$, 从而
\begin{equation}\label{eq4.11}
  \max_{1\leq i\leq n}\,|\hat{e}_i-e_i|\leq\max_{1\leq i\leq n}\,||X_i||\,||\hb-\beta||=\text{o}_p(n^{-1/2+1/r})
\end{equation}
由于假设\ref{pro:pro4.2}要求$r\geq4$, 因此$\displaystyle q_n =\max_{1\leq i\leq n}\,|\hat{e}_i-e_i|$的收敛率至少为$\text{o}_{p}(n^{-1/4})$, 并且随着$r$增加, 收敛越快. 进一步根据(\ref{eq4.11})得到
\begin{align*}
\left|\left|\frac{1}{n}\sum_{i=1}^{n}X_iX_i'(\hat{e}_i^2-e_i^2)\right|\right|&\leq\frac{1}{n}\sum_{i=1}^{n}||X_iX_i'||\,|\hat{e}_i^2-e_i^2| \\
&\leq\frac{2}{n}\sum_{i=1}^{n}||X_i||^2|e_i|\,|\hat{e}_i-e_i|+\frac{1}{n}\sum_{i=1}^{n}||X_i||^2|\hat{e}_i-e_i|^2 \\
&\leq\frac{2}{n}\sum_{i=1}^{n}||X_i||^2|e_i|q_n+\frac{1}{n}\sum_{i=1}^{n}||X_i||^2q_n^2\leq\text{o}_p(n^{-1/4})
\end{align*}
显然可以推出$\hat{\mathbold{\Omega}}=\mathbold{\Omega}+\text{o}_p(1)$.

本节的最后我们来考虑$\V_\beta$的其它估计量$\hat{\V}_\beta^{\text{HC1}}$, $\hat{\V}_\beta^{\text{HC2}}$及$\hat{\V}_\beta^{\text{HC3}}$. 其中
$$\hat{\V}_\beta^{\text{HC1}}=\frac{n}{n-K}\hat{\V}_\beta^{\text{HC0}}$$
由于$n\to\infty$时有$n/(n-K)\to1$, 于是$\hat{\V}_\beta^{\text{HC1}}\xrightarrow{p}\V_\beta$.

$\hat{\V}_\beta^{\text{HC2}}$与$\hat{\V}_\beta^{\text{HC3}}$则与(\ref{eq4.12})的形式一样, 只是将$\hat{\mathbold{\Omega}}$分别替换为
$$\overbar{\mathbold{\Omega}}=\frac{1}{n}\sum_{i=1}^{n}(1-h_{ii})^{-1}X_iX_i'\hat{e}_i^2$$
以及
$$\tilde{\mathbold{\Omega}}=\frac{1}{n}\sum_{i=1}^{n}(1-h_{ii})^{-2}X_iX_i'\hat{e}_i^2$$
为了证明$\hat{\V}_\beta^{\text{HC2}}$与$\hat{\V}_\beta^{\text{HC3}}$是$\V_\beta$的一致估计量, 只需证明当$n\to\infty$时, $\overbar{\mathbold{\Omega}}-\hat{\mathbold{\Omega}}$与$\tilde{\mathbold{\Omega}}-\hat{\mathbold{\Omega}}$依概率收敛于0. 在此之前我们先证明一个引理.
\begin{lemma}\label{lem:lem4.1}
如果$\{X_n\}$为i.i.d.随机变量序列, $\Q_{XX}$为正定矩阵, 并且对于某个$r\ge2$有$\E||X||^r<\infty$, 那么
$\displaystyle\max_{1\leq i\leq n}\,h_{ii}=\text{o}_p(n^{2/r-1})$.
\end{lemma}
\begin{proof}
  因为$\hat{\Q}_{XX}\xrightarrow{p}\Q_{XX}$, $\Q_{XX}$正定, 根据CMT可知
  $$\lambda_{\min}(\hat{\Q}_{XX})\xrightarrow{p}\lambda_{\min}(\Q_{XX})>0$$
  根据二次不等式可知
  \begin{align}
  h_{ii}&=X_i(\X'\X)^{-1}X_i' \nonumber \\
  &\leq\lambda_{\max}[(\X'\X)^{-1}](X_iX_i') \nonumber \\
  &=\lambda_{\min}[n^{-1}(\X'\X)]^{-1}n^{-1}||X_i||^2 \nonumber \\
  &\leq[\lambda_{\min}(\Q_{XX})+\text{o}_p(1)]^{-1}n^{-1}\max_{1\leq i\leq n}\,||X_i||^2 \label{eq4.13}
  \end{align}
  根据定理\ref{thm:thm3.7}可知
  $$\max_{1\leq i\leq n}\,||X_i||^2=\left(\max_{1\leq i\leq n}\,||X_i||\right)^2=\text{o}_p(n^{2/r})$$
  联系(\ref{eq4.13})即可证得定理.
\end{proof}
\begin{theorem}
  在假设\ref{pro:pro4.2}下, 当$n\to\infty$时有$\overbar{\mathbold{\Omega}}\xrightarrow{p}\mathbold{\Omega}$与$\tilde{\mathbold{\Omega}}\xrightarrow{p}\mathbold{\Omega}$.
\end{theorem}
\begin{proof}
  根据假设\ref{pro:pro4.2}和引理\ref{lem:lem4.1}可知
  $$h^\ast_n=\max_{1\leq i\leq n}\,h_{ii}=\text{o}_p(1)$$
  从而
  \begin{align}
  ||\overbar{\Om}-\hat{\Om}||&\leq\frac{1}{n}\sum_{i=1}^{n}||X_iX_i'||\hat{e}_i^2|(1-h_{ii})^{-1}-1| \nonumber\\
  &\leq\left(\frac{1}{n}\sum_{i=1}^{n}||X_i||^2\hat{e}_i^2\right)|(1-h_n^\ast)^{-1}-1| \label{eq4.14}
  \end{align}
  按照之前的做法可以得出(\ref{eq4.14})不等号右端括号内的部分依概率收敛于$\E[||X_i||^2e^2_i]$, 绝对值内的部分为$\text{o}_p(1)$, 因此(\ref{eq4.14})也是$\text{o}_p(1)$, 从而$\overbar{\Om}=\hat{\Om}+\text{o}_p(1)\xrightarrow{p}\Om$.

  类似可以证明
  \begin{align*}
  ||\tilde{\Om}-\hat{\Om}||&\leq\frac{1}{n}\sum_{i=1}^{n}||X_iX_i'||\hat{e}_i^2|(1-h_{ii})^{-2}-1|\\
  &\leq\left(\frac{1}{n}\sum_{i=1}^{n}||X_i||^2\hat{e}_i^2\right)|(1-h_n^\ast)^{-2}-1|=\text{o}_p(1)
  \end{align*}
  因此定理成立.
\end{proof}
根据以上论述, 异方差稳健的协方差矩阵估计量HC0, HC1, HC2及HC3都是$\V_\beta$的一致估计量, 由于它们在大样本下具有相同的渐近分布, 后面我们统一使用$\hat{\V}_\beta$来表示$\V_{\beta}$的一致估计量.

值得注意的是, 尽管$\hat{\V}_\beta$是渐近方差$\text{avar}(\sqrt{n}\hb)$的一致估计量, 但“$\hat{\V}_{\beta}/n$是$\text{avar}(\hb)$的一致估计”这一说法却没有什么意义, 因为当$n\to\infty$时$\V/n\to0$. 而不论$\hat{\V}_\beta$是不是$\V_\beta$的一致估计, 总有$\hat{\V}_\beta\xrightarrow{p}0$.
\section{参数检验}
不同于第二章的基于正态随机扰动项的线性假设检验, 本节的内容无需依靠这一过于苛刻的条件, 而是基于大样本理论给出的渐近统计量进行统计推断, 不仅如此, 本节考虑的是一般的非线性假设.
\subsection{回归系数的函数}

通常而言, 研究者会对参数向量$\beta=[\beta_0,\beta_1,\cdots,\beta_k]$的某个具体形式感兴趣, 举例来说就是像单个系数$\beta_j$或$\beta_j/\beta_l$这样关于$\beta$的某个函数$\theta=R(\beta)$, 其中$R:\R^K\to\R^J$为某个函数. $\theta$的估计量为
$$\hat{\theta}=R(\hat{\beta})$$
如果$R$是连续函数, 那么根据CMT和$\hat{\beta}\xrightarrow{p}\beta$这一事实可以推知$\hat{\theta}$是$\theta$的一致估计量, 现将其表述为以下定理.
\begin{theorem}
  在假设\ref{pro:pro4.1}下, 如果当$n\to\infty$时, $R(\beta)$在真实参数$\beta$处连续, 那么$\hat{\theta}\xrightarrow{p}\theta$.
\end{theorem}

不仅如此, 如果函数$R:\R^K\to\R^J$足够光滑, 由Delta法可以得到$\hat{\theta}$的渐近正态性, 为此我们给出一个新的假设.
\begin{proposition}\label{pro:pro4.3}
非随机的可测向量值函数$R:\R^K\to\R^J$在真实参数$\beta$处连续可微, 并且矩阵$\mathbold{R}=\nabla_\beta R(\beta)$的秩为$J$.
\end{proposition}
\begin{theorem}\label{thm:thm4.5}
  在假设\ref{pro:pro4.2}和\ref{pro:pro4.3}下, 当$n\to\infty$时有
  \begin{equation}\label{eq4.15}
    \sqrt{n}(\hat{\theta}-\theta)\xrightarrow{d}N(0,\V_\theta)
  \end{equation}
  其中$\V_\theta=\mathbold{R}\V_\beta\mathbold{R}'$.
\end{theorem}

出于统计推断的目的, 我们需要渐近协方差矩阵$\V_\theta=\RH\V_\beta\RH'$的一致估计量. 一个直觉是先利用估计量
\begin{equation}\label{eq4.16}
  \hat{\RH}=\nabla_\beta R(\hb)=\frac{\partial}{\partial\beta}R(\hat{\beta})
\end{equation}
然后得到$\V_\theta$的估计量$\hat{\V}_\theta=\hat{\RH}\hat{\V}_\beta\hat{\RH}'$, 称其为渐近协方差矩阵估计量. 特别地, 在随机扰动项满足同方差的条件下有
$$\hat{\V}_\theta^0=\hat{\RH}\hat{\V}_\beta^0\hat{\RH}'=s^2\hat{\RH}\hat{\mathbold{Q}}_{XX}^{-1}\hat{\RH}'$$
现在我们简单给出这一直觉是正确的证明.

\begin{theorem}\label{thm:thm4.6}
  在假设\ref{pro:pro4.2}和\ref{pro:pro4.3}下, 当$n\to\infty$时有$\hat{\V}_\theta\xrightarrow{p}\V_\theta$.
\end{theorem}
\begin{proof}
  根据定理\ref{thm:thm4.4}可知$\hat{\V}_\beta\xrightarrow{p}\V_\beta$, 又根据$\hb\xrightarrow{p}\beta$和$\RH=\displaystyle\nabla_\beta R(\beta)$在$\beta$处的连续性可知
  $$\hat{\RH}=\nabla_\beta R(\hb)\xrightarrow{p}\nabla_\beta R(\beta)=\RH$$
  于是由CMT即可推得定理成立.
\end{proof}
\subsection{\emph{T}检验和Wald检验}
现在我们想要检验原假设$\HH_0: R(\beta)=r$, 这里的$r$为一个$J\times1$维非随机向量. 类似第二章的做法, 我们按$J=1$和$J>1$的情况进行分类讨论, 其中$J=1$时的假设对应$T$统计量, 而$J>1$时的假设对应Wald统计量 (不是之前的$F$统计量).

在正式开始之前, 我们还需要一个技术性的假设.
\begin{proposition}\label{pro:pro4.4}
矩阵$\V_\theta=\RH\V_\beta\RH'$正定.
\end{proposition}

先来看$J=1$的情况, 也即单个约束条件下的参数检验.
\begin{theorem}[渐近$T$统计量]
  在假设\ref{pro:pro4.2}, \ref{pro:pro4.3}和\ref{pro:pro4.4}下, 如果原假设$\HH_0:R(\beta)=r$成立, 那么当$n\to\infty$时有
  $$T=\frac{\sqrt{n}[R(\hb)-r]}{\sqrt{\hat{\RH}\hat{\V}_\beta\hat{\RH}'}}\xrightarrow{d}N(0,1)$$
\end{theorem}
\begin{proof}
  在原假设$\HH_0$成立的条件下
  \begin{align*}
  \sqrt{n}[R(\hb)-r]&=\sqrt{n}[R(\beta)-r]+\nabla_\beta R(\overbar{\beta})\sqrt{n}(\hb-\beta) \\
  &=\nabla_\beta R(\overbar{\beta})\sqrt{n}(\hb-\beta)
  \end{align*}
  这里的$\overbar{\beta}$介于$\hb$和$\beta$之间.

  由于$R:\R^K\to\R^J$连续可微, 并且当$n\to\infty$时有$\hb\xrightarrow{p}\beta$, 故此时也有
  $$\nabla_\beta R(\overbar{\beta})\xrightarrow{p}\nabla_\beta R(\beta)$$
  又根据定理\ref{thm:thm4.2}可知
  $$\sqrt{n}(\hb-\beta)\xrightarrow{d}N(0,\V_\beta)$$
  因此根据Slutsky定理有
  $$\sqrt{n}[R(\hb)-r]\xrightarrow{d}N(0,\RH\V_\beta\RH')$$
  再由定理\ref{thm:thm4.6}可得$\hat{\V}_\theta\xrightarrow{p}\V_\theta$, 最后再使用一次Slutsky定理即可证得定理.
\end{proof}
\begin{remark}
在渐近$T$检验中, 分母是一个标量, 它被称为$\hat{\theta}$的标准误 (standard error).
\end{remark}

在$J>1$的多重约束条件下, 上述渐近$T$检验将被拓展为Wald检验, 但它们在本质上是一样的.
\begin{theorem}[Wald检验统计量]
  在假设\ref{pro:pro4.2}, \ref{pro:pro4.3}和\ref{pro:pro4.4}下, 如果原假设$\HH_0:R(\beta)=r$成立, 那么当$n\to\infty$时有
  $$W=n[R(\hb)-r]'[\hat{\RH}\hat{\V}_\beta\hat{\RH}']^{-1}[R(\hb)-r]\xrightarrow{d}\chi^2_J$$
\end{theorem}
\begin{proof}
  按照之前的做法可以得到
  $$\sqrt{n}[R(\hb)-r]\xrightarrow{d}Z\sim N(0,\RH\V_\beta\RH')$$
  根据引理\ref{lem:lem2.2}可知$Z'\V_\theta^{-1}Z\sim\chi^2_J$, 又因为$\hat{\V}_\theta^{-1}\xrightarrow{p}\V_\theta^{-1}$, 使用Slutsky定理即可推知
  $$\sqrt{n}[R(\hb)-r]'\hat{\V}_\theta^{-1}\sqrt{n}[R(\hb)-r]\xrightarrow{d}\chi^2_J$$
  证毕.
\end{proof}


如果将函数$R:\R^K\to\R^J$设置为线性函数, 也即$\displaystyle\RH=\nabla_\beta R(\beta)$为常矩阵, 那么原假设可以简化为第二章那样的线性假设$\HH_0:\RH\beta=r$, 此时$T$统计量和Wald统计量分别为
\begin{align}
\begin{split}
T&=\frac{\sqrt{n}(\RH\hb-r)}{\sqrt{\RH\hat{\V}_\beta\RH'}}\xrightarrow{d}N(0,1) \\
W&=n(\RH\hb-r)'(\RH\hat{\V}_\beta\RH')^{-1}(\RH\hb-r)\xrightarrow{d}\chi^2_J
\end{split}
\label{eq4.17}
\end{align}
它们是最为常见的检验统计量. 特别地, 如果条件同方差假设成立, 那么可以使用以下更有效的统计量
\begin{align}
\begin{split}
T_{\text{Hom}}&=\frac{\RH\hb-r}{\sqrt{s^2\RH(\X'\X)^{-1}\RH'}}\xrightarrow{d}N(0,1) \\
W_{\text{Hom}}&=(\RH\hb-r)'[s^2\RH(\X'\X)^{-1}\RH']^{-1}(\RH\hb-r)\xrightarrow{d}\chi^2_J
\end{split}
\label{eq4.18}
\end{align}
此时的$W_{\text{Hom}}=J\cdot F$, 因此Wald统计量在条件同方差下又称$J\cdot F$统计量.

特别地, 如果想要检验回归系数是否联合为0, 可以使用以下$(n-K)R^2$检验, 它是LM检验 (或称得分检验)中的一种.
\begin{theorem}\label{thm:thm2.9}
  在假设\ref{pro:pro4.2}和条件同方差$\E[e_i^2|X_i]=\sigma^2$成立的情况下, 如果原假设$\HH_0: \beta_1=\beta_2=\cdots=\beta_k=0$成立, 那么当$n\to\infty$时有
  $$(n-K)R^2\xrightarrow{d}\chi^2_{K-1}$$
\end{theorem}
\begin{proof}
  根据推论\ref{cor:cor2.2}可知在$\HH_0$成立的条件下有
  $$F=\frac{R^2/(K-1)}{(1-R^2)/(n-K)}$$
  根据Wald检验又可知
  $$(K-1)F=\frac{(n-K)R^2}{1-R^2}\xrightarrow{d}\chi^2_{K-1}$$
  由于$K$为常数, 故而$(K-1)F=\text{O}_p(1)$, 从而
  $$\frac{R^2}{1-R^2}=\text{O}_p(n^{-1})=\text{o}_p(1)$$
  也即$1-R^2\to 1$. 根据Slutsky定理可得
  \begin{align*}
  (n-K)R^2&=(K-1)F\cdot (1-R^2)\xrightarrow{d}\chi^2_{K-1}
  \end{align*}
  证毕.
\end{proof}

\begin{remark}
下面我们给出两点注意事项.

(1). 在线性假设下, 我们使用的是$\RH$而非$\hat{\RH}$, 这是因为在非线性假设下, $\RH$包含了未知的真实参数, 因此需要使用$\hat{\RH}$才能得到可计算的统计量. 而在线性假设下, $\RH$为常矩阵, 我们可以直接使用它构造统计量.

(2). 如果在条件同方差的情况下使用(\ref{eq4.17})而非(\ref{eq4.18})进行参数检验, 尽管这样是可行的, 因为前者更加稳健且适用于一般情况, 但是在较小的样本量下, 由于没有利用同方差这一信息, (\ref{eq4.17})的真实分布可能与渐近$\chi^2$分布相差甚远.
\end{remark}
最后, 如果我们想知道使用正态分布来逼近$T$统计量的准确程度, 一个直觉是利用Edgeworth展开, 具体内容参考 Hansen (2022a)的7.19节内容.
\newpage
\section{异方差的检验}

为了检验条件同方差假设$\E[e_i^2|X_i]=\sigma^2$, 可以考虑以下辅助回归
\begin{align*}
e_i^2&=\gamma_0+\sum_{j=1}^{K-1}\gamma_jX_{ji}+\sum_{1\leq j\leq l\leq K-1}\gamma_{jl}X_{ji}X_{li}+v_i \\
&=\gamma'\text{vech}\,(X_iX_i')+v_i
\end{align*}
这里的$\text{vech}$是一个向量化算子, 它将$K\times K$维对称矩阵$X_iX_i'$的下三角元素转变为一个$K(K+1)/2\times 1$维向量. 如果原假设$\HH_0:\E[e_i^2|X_i]=\sigma^2$和条件$\E[e_i^4|X_i]=\mu$成立, 那么当$n\to\infty$时有
$$(n-J-1)R^2\xrightarrow{d}\chi^2_J$$
其中$J=K(K+1)/2-1$. 然而$e_i^2$是不可观测的, 此时我们可以考虑使用OLS残差平方$\hat{e}_i^2$将其替代, White (1980)证明了这种替代不会影响到统计量$(n-J-1)R^2$的渐近分布, 这个检验称为White检验, 以下给出它的直观理解.

记$U_i=\text{vech}\,(X_iX_i')$, 则辅助回归方程可以记为
$$e_i^2=U_i'\gamma+v_i$$
当原假设$\HH_0:\E[e_i^2|X_i]=\sigma^2$以及$\E[e_i^4|X_i]=\mu_4$成立时, $v_i$满足条件同方差, 故而
$$\sqrt{n}(\tilde{\gamma}-\gamma)\xrightarrow{d}N(0,\sigma_v^2\Q_{UU}^{-1})$$
其中$\tilde{\gamma}$为OLS估计量. 设$\RH$为$J\times(J+1)$维矩阵, 它的第$(i,i+1)$个元素为1, 其中$i=1,2,\cdots,J$, 并且其余元素均为0. 于是
$$\sqrt{n}\RH\tilde{\gamma}=\sqrt{n}\RH(\tilde{\gamma}-\gamma)\xrightarrow{d} N(0,\sigma_v^2\RH\Q_{UU}^{-1}\RH')$$
从而$\RH\tilde{\gamma}=\text{O}_p(n^{-\frac{1}{2}})$. 注意到原假设可以改写为$\HH_0:\gamma=0$, 根据Wald检验可知
$$n(\RH\tilde{\gamma})'(s_v^2\RH\hat{\Q}_{UU}^{-1}\RH')^{-1}(\RH\tilde{\gamma})\xrightarrow{d}\chi^2_J$$

现在用$\hat{e}_i^2$替换$e_i^2$, 得到新的辅助回归
\begin{equation}\label{eq4.19}
  \hat{e}_i^2=U_i'\gamma+\tilde{v}_i
\end{equation}
记(\ref{eq4.19})的OLS估计量为$\hat{\gamma}$. 注意到
\begin{align*}
\hat{e}_i^2&=[e_i-X_i'(\hb-\beta)]^2 \\
&=e_i^2+(\hb-\beta)'X_iX_i'(\hb-\beta)-2(\hb-\beta)'X_ie_i
\end{align*}
它的第一项为$e_i^2$, 第二项和$U_i=\text{vech}\,(X_iX_i')$成正比, 而第三项和$X_ie_i$成正比. 因此OLS估计量$\hat{\gamma}$可以分解为
$$\hat{\gamma}=\tilde{\gamma}+\hat{\delta}+\hat{\eta}$$
其中第一项决定了$(n-J-1)R^2$的渐近$\chi^2$分布.

对于第三项, 在模型正确识别的情况下, $X_ie_i$与$U_i$不相关, 故而$X_ie_i$对$U_i$回归产生的OLS估计量是$\text{O}_p(n^{-\frac{1}{2}})$, 它乘以因子$2(\hb-\beta)'=\text{O}_p(n^{-\frac{1}{2}})$后以$n^{-1}$的速度依概率为0, 也即$\text{O}_p(n^{-1})$. 又因为$\RH\tilde{\gamma}=\text{O}_p(n^{-\frac{1}{2}})$, 因此$\hat{\eta}$对$\tilde{\gamma}$的影响可忽略不计.

对于第二项, $X_iX_i'$和$U_i$完全相关, 故而$X_iX_i'$对$U_i$回归产生的OLS估计量是$\text{O}_p(1)$, 当它乘以因子$||\hb-\beta||^2=\text{O}_p(n^{-1})$后仍然也以$n^{-1}$的速度依概率为0, 也即$\hat{\delta}=\text{O}_p(n^{-1})$, 因此$\hat{\delta}$对$\tilde{\gamma}$的影响也可忽略不计.

\begin{remark}
下面给出关于White检验的评注.

(1) White检验本质上检验的是$\E[e_i^2|X_i]$是否与$X_i$的二次项相关, 故而当$\E[e_i^2|X_i]$不依赖于$X_i$的二次项, 而依赖于$X_i$的更高次项时, White检验无效.

(2) White检验如果不能拒绝$\HH_0$, 只表明没有找到存在条件异方差的证据, 而无法表明条件同方差假设成立.

(3) 当$\E[e_i^4|X_i]=\mu$不成立时, 无法使用$(n-J-1)R^2$统计量, 此时应该通过Wald原理构造异方差稳健的检验统计量, 这里指的是辅助回归中的误差项$v_i$存在异方差.
\end{remark}

\chapter{系统估计}
以上章节讨论的都是具有一个方程的线性回归模型, 但有时候会出现多个方程的情形, 如果多个方程之间存在某种关系, 那么联合估计整个系统可能会提高估计的效率.
\section{回归系统}
许多单变量线性回归模型中用到的技术都可以用在多方程线性回归模型中, 二者最大的区别在于表示矩阵估计量的符号.

假定回归系统包含$m$个回归方程, 并且每个方程均有$n$个观测值
\begin{align}
  \begin{split}
  Y_1&=X_1'\beta_1+e_1 \\
  &\vdots \\
  Y_m&=X_m'\beta_m+e_m
  \end{split}
  \label{eq5.1}
\end{align}
也即
$$Y_j=X_j'\beta_j+e_j,\quad j=1,2,\cdots,m$$
这里的下标$j$表示第$j$个回归方程, 而非第$j$个观测值. $Y_j$表示第$j$个因变量, 数据矩阵$X_j$和系数向量$\beta_j$分别为$K_j\times n$和$K_j\times1$维, $e_j$为随机误差项. 在整个回归系统中, 回归元在每个$j$上可以不同也可以相同, 共计有$\overbar{K}=\sum_{j=1}^{m}K_j$个回归系数.

从一个总体中随机抽样得到的多方程线性回归模型为
\begin{equation}\label{eq5.3}
  Y_i=\overbar{X}_i\beta+e_i,\quad i=1,2,\cdots,n
\end{equation}
其中$Y_i$和$e_i$均为$m\times1$维列向量, 并且
$$\overbar{X}_i=\begin{bmatrix}
                              X_{i1}' & 0 & \cdots & 0 \\
                              0 & X_{i2}' & \cdots & 0 \\
                              \vdots & \vdots &  & \vdots \\
                              0 & 0 & \cdots & X_{im}'
                            \end{bmatrix},\quad \beta=\begin{bmatrix}
                                                        \beta_1 \\
                                                        \beta_2 \\
                                                        \vdots \\
                                                        \beta_m
                                                      \end{bmatrix}$$
分别为$m\times\overbar{K}$维矩阵和$\overbar{K}\times1$维列向量. 现在我们将这$n$个观测堆叠, 由此得到
$$Y=\overbar{\mathbold{X}}\beta+e$$
其中
$$Y=\begin{bmatrix}
      Y_1 \\
      Y_2 \\
      \vdots \\
      Y_n
    \end{bmatrix},\quad e=\begin{bmatrix}
                            e_1 \\
                            e_2 \\
                            \vdots \\
                            e_n
                          \end{bmatrix},\quad \overbar{\mathbold{X}}=\begin{bmatrix}
                                                             \overbar{X}_1 \\
                                                             \overbar{X}_2 \\
                                                             \vdots \\
                                                             \overbar{X}_n
                                                           \end{bmatrix}$$
它们的维数分别为$mn\times1$, $mn\times1$以及$mn\times \overbar{K}$.

在许多应用情况下, 每个$j$中对应的回归元都是一样的, 此时有$X_j=X$, $K_j=K$, $j=1,2,\cdots,m$. 将$n$个观测值堆叠后可以得到$n\times m$维的表示形式
$$Y=\mathbold{XB}+\mathbold{E}$$
其中$\mathbold{B}=[\beta_1,\beta_2,\cdots,\beta_m]$为$K\times m$维矩阵, 并且
\begin{equation}\label{eq5.6}
Y=\begin{bmatrix}
      Y_1' \\
      Y_2' \\
      \vdots \\
      Y_n'
    \end{bmatrix},\quad \mathbold{X}=\begin{bmatrix}
                                       X_1' \\
                                       X_2' \\
                                       \vdots \\
                                       X_n'
                                     \end{bmatrix}, \quad \mathbold{E}=\begin{bmatrix}
                                                                                      e_1' \\
                                                                                      e_2' \\
                                                                                      \vdots \\
                                                                                      e_n'
                                                                                    \end{bmatrix}
\end{equation}
它们的维数分别为$n\times m$, $n\times K$以及$n\times m$, 并且每个$X_i'$的维数为$1\times K$, $i=1,2,\cdots,n$.

除此之外, 回归元相同时的数据矩阵\footnote{如果不关注观测值, 那么可以将下标$i$去掉.}还可以方便地表示为$m\times mK$维矩阵
$$\overbar{X}_i=\begin{bmatrix}
                              X_i' & 0 & \cdots & 0 \\
                              0 & X_i' & \cdots & 0 \\
                              \vdots & \vdots &  & \vdots \\
                              0 & 0 & \cdots & X_i'
                            \end{bmatrix}=\mathbold{I}_m\otimes X_i'$$
其中$\otimes$表示Kronecker积\footnote{对于任意的矩阵$\mathbold{A}_{m\times n}=\begin{bmatrix}
                                                                    a_{11} & \cdots & a_{1n} \\
                                                                    \vdots &  & \vdots \\
                                                                    a_{m1} & \cdots & a_{mn}
                                                                  \end{bmatrix}$与$\mathbold{B}_{p\times q}$, Kronecker积为$\mathbold{A}\otimes\mathbold{B}=\begin{bmatrix}
                                                                                                      a_{11}\mathbold{B} & \cdots & a_{1n}\mathbold{B} \\
                                                                                                       \vdots& & \vdots \\
                                                                                                       a_{m1}\mathbold{B}& \cdots & a_{mn}\mathbold{B}
                                                                                                    \end{bmatrix}_{mp\times nq}$.
Kronecker积的运算律: \begin{itemize}
                  \item $(\mathbold{A}\otimes \mathbold{B})(\mathbold{C}\otimes \mathbold{D})=(\mathbold{AC})\otimes(\mathbold{BD})$;
                  \item $(\mathbold{A}\otimes\mathbold{B})'=\mathbold{A}'\otimes\mathbold{B}'$;
                  \item $(\mathbold{A}\otimes\mathbold{B})^{-1}=\mathbold{A}^{-1}\otimes\mathbold{B}^{-1}$.
                \end{itemize}}.
注意, 这里并没有$\overbar{\X}=\mathbold{I}_m\otimes \X$这一关系.

\section{系统普通最小二乘估计}
线性回归系统(\ref{eq5.1})可以由OLS进行估计, 称为系统普通最小二乘估计 (System Ordinary Least Squares, SOLS), 可以将其表示为
$$\hat{\beta}_j=\left(\sum_{i=1}^{n}X_{ji}X_{ji}'\right)^{-1}\left(\sum_{i=1}^{n}X_{ji}Y_{ji}\right)$$
它的堆叠形式为向量
$$\hb=\begin{bmatrix}
        \hb_1 \\
        \hb_2 \\
        \vdots \\
        \hb_m
      \end{bmatrix}$$
此外还可以用系统符号来表示
\begin{equation}\label{eq5.2}
  \hb=(\overbar{\X}'\overbar{\X})^{-1}\overbar{\X}'Y=\left(\sum_{i=1}^{n}\overbar{X}_i\overbar{X}_i'\right)^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'Y_i\right)
\end{equation}
其中
$$\overbar{\X}'\overbar{\X}=\begin{bmatrix}
          \sum_{i=1}^{n}X_{1i}X_{1i}' & 0 & \cdots & 0 \\
          0 & \sum_{i=1}^{n}X_{2i}X_{2i}' & \cdots & 0 \\
          \vdots & \vdots &  & \vdots \\
          0 & 0 & \cdots & \sum_{i=1}^{n}X_{mi}X_{mi}'
        \end{bmatrix},\quad \overbar{\X}'Y=\begin{bmatrix}
                                   \sum_{i=1}^{n}X_{1i}Y_{1i} \\
                                   \sum_{i=1}^{n}X_{2i}Y_{2i} \\
                                   \vdots \\
                                   \sum_{i=1}^{n}X_{mi}Y_{mi}
                                 \end{bmatrix}$$
特别地, 如果所有单方程的回归元相同, 那么有
$$\hb_j=\left(\sum_{i=1}^{n}X_iX_i'\right)^{-1}\left(\sum_{i=1}^{n}X_iY_{ji}\right)$$
以及
$$\mathbold{B}=[
                 \hb_1, \hb_2, \cdots, \hb_m
               ]=(\X'\X)^{-1}\X'Y$$
\section{SOLS的期望与方差}
如果以下条件期望假设成立
\begin{equation}\label{eq5.4}
  \E[e_i|\OX_i]=0
\end{equation}
那么我们可以计算有限样本期望以及$\hb$的方差. 等式(\ref{eq5.4})等价于$\E[Y_j|X_j]=X_j'\beta_j$, 这意味着回归模型是正确识别的.

现在将估计量中心化为
$$\hb-\beta=(\overbar{\X}'\overbar{\X})^{-1}\overbar{\X}'e=\left(\sum_{i=1}^{n}\overbar{X}_i\overbar{X}_i'\right)^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'e_i\right)$$
根据(\ref{eq5.4}), 两端取条件期望即可得到$\E[\hb|\overbar{\X}]=\beta$. 因此, 在模型正确识别的情况下, SOLS是无偏估计.

为了计算SOLS估计量的方差, 我们先定义第$i$个观测值的误差的条件协方差矩阵为$\E[e_ie_i'|\OX_i]=\mathbold{\Sigma}_i$. 如果各观测值相互独立, 那么有$mn\times mn$维矩阵
$$\E[ee'|\overbar{\X}]=\text{diag}\,\{\mathbold{\Sigma}_1, \mathbold{\Sigma}_2,\cdots,\mathbold{\Sigma}_n\}$$
于是
$$\var\left(\left.\sum_{i=1}^{n}\overbar{X}_i'e_i\right|\overbar{\X}\right)=\sum_{i=1}^{n}\var\left(\left.\overbar{X}_i'e_i\right|\OX_i\right)=\sum_{i=1}^{n}\overbar{X}_i'\mathbold{\Sigma}_i\overbar{X}_i$$
因此
$$\var(\hb|\overbar{\X})=(\overbar{\X}'\overbar{\X})^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'\mathbold{\Sigma}_i\overbar{X}_i\right)(\overbar{\X}'\overbar{\X})^{-1}$$

如果每个单方程具有完全相同的回归元, 也即$\overbar{X}_i=\mathbold{I}_m\otimes X_i'$, 那么根据Kronecker积的运算律即可推知
\begin{align*}
\overbar{\X}'\overbar{\X}&=\sum_{i=1}^{n}\overbar{X}_i'\overbar{X}_i =\sum_{i=1}^{n}(\mathbold{I}_m\otimes X_i)(\mathbold{I}_m\otimes X_i') \\
&=\sum_{i=1}^{n}(\mathbold{I}_m\otimes X_iX_i')=\mathbold{I}_m\otimes\left(\sum_{i=1}^{n}X_iX_i'\right) \\
&=\mathbold{I}_m\otimes (\X'\X)
\end{align*}
从而$(\overbar{\X}'\overbar{\X})^{-1}=\mathbold{I}_m\otimes(\X'\X)^{-1}$. 另一方面, 根据Kronecker积的定义可知$\mathbold{\Sigma}_i=\mathbold{\Sigma}_i\otimes 1$, 再由运算律得到
$$(\mathbold{I}_m\otimes X_i)\mathbold{\Sigma}_i(\mathbold{I}_m\otimes X_i')=\mathbold{\Sigma}_i\otimes X_iX_i'$$
于是
\begin{equation}\label{eq5.7}
  \var(\hb|\X)=[\mathbold{I}_m\otimes (\X'\X)^{-1}]\left[\sum_{i=1}^{n}(\mathbold{\Sigma}_i\otimes X_iX_i')\right][\mathbold{I}_m\otimes (\X'\X)^{-1}]
\end{equation}
另一方面, 如果误差项满足条件同方差
\begin{equation}\label{eq5.10}
  \E[e_ie_i'|\OX_i]=\mathbold{\Sigma},\quad i=1,2,\cdots,n
\end{equation}
那么协方差矩阵可以简化为
\begin{equation}\label{eq5.5}
  \var(\hb|\X)=(\overbar{\X}'\overbar{\X})^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'\mathbold{\Sigma}\overbar{X}_i\right)(\overbar{\X}'\overbar{\X})^{-1}
\end{equation}
如果以上\textbf{两种简化条件同时成立}, 那么利用(\ref{eq5.7})和(\ref{eq5.5})可以得到一个相当简单的协方差矩阵表达式
\begin{equation}\label{eq5.8}
  \var(\hb|\X)=\mathbold{\Sigma}\otimes (\X'\X)^{-1}
\end{equation}

\section{SOLS的渐近性质}
前面已经提到, 单方程线性回归模型中的诸多定理都可以普遍应用到回归系统中来, 本节将说明之前证明的OLS的渐近性质在SOLS中也成立.
\begin{theorem}\label{thm:thm5.1}
  如果每个单方程都满足假设\ref{pro:pro4.2}, 那么当$n\to\infty$时有
  $$\sqrt{n}(\hb-\beta)\xrightarrow{d}N(0,\V_\beta)$$
  其中$\V_\beta=\Q^{-1}\mathbold{\Omega}\Q^{-1}$, $\mathbold{\Omega}=\E[\overbar{X}_ie_ie_i'\overbar{X}_i']=\E[\overbar{X}_i'\mathbold{\Sigma}_i\overbar{X}_i]$, 并且
  $$\Q=\E[\overbar{X}_i'\overbar{X}_i]=\begin{bmatrix}
                                     \E[X_{1i}X_{1i}'] &  &  &  \\
                                      & \E[X_{2i}X_{2i}'] &  &  \\
                                      &  & \ddots &  \\
                                      &  &  & \E[X_{mi}X_{mi}']
                                   \end{bmatrix}$$
\end{theorem}
\begin{proof}
  根据定理\ref{thm:thm1.5}(3)可知, 对于一切$j=1,2,\cdots,m$都有
  \begin{equation}\label{eq5.9}
    \E[X_je_j]=0
  \end{equation}
  考虑向量
  $$\overbar{X}_i'e_i=\begin{bmatrix}
                        X_{1i}e_{1i} \\
                        X_{2i}e_{2i} \\
                        \vdots \\
                        X_{mi}e_{mi}
                      \end{bmatrix}$$
  在式(\ref{eq5.9})下, $\overbar{X}_i'e_i$具有零均值并且在各观测$i$之间是i.i.d.的. 根据多元Lindeberg-Levy CLT可知
  $$n^{-\frac{1}{2}}\sum_{i=1}^{n}\overbar{X}_i'e_i\xrightarrow{d} N(0,\mathbold{\Omega})$$
  其中
  $$\mathbold{\Omega}=\E[\overbar{X}_ie_ie_i'\overbar{X}_i']=\E[\overbar{X}_i'\mathbold{\Sigma}_i\overbar{X}_i]$$
  这里的第二个等号是由LIE得到的, 也即
  $$\E[\overbar{X}_ie_ie_i'\overbar{X}_i']=\E[\overbar{X}_i\E[e_ie_i'|X_i]\overbar{X}_i']=\E[\overbar{X}_i'\mathbold{\Sigma}_i\overbar{X}_i]$$
  注意到
  $$\sqrt{n}(\hb-\beta)=\left(n^{-1}\sum_{i=1}^{n}\overbar{X}_i\overbar{X}_i'\right)^{-1}\left(n^{-\frac{1}{2}}\sum_{i=1}^{n}\overbar{X}_i'e_i\right)$$
  参考第四章的做法, 容易证得$\sqrt{n}(\hb-\beta)\xrightarrow{d}N(0,\V_\beta)$.

\end{proof}

\begin{example}[协方差矩阵的化简]
在条件同方差假设(\ref{eq5.10})下, $\mathbold{\Omega}$自然而然地化简为
$$\mathbold{\Omega}=\E[\overbar{X}_i'\mathbold{\Sigma}\overbar{X}_i]$$
如果所有单方程的回归元相同, 那么
\begin{align*}
\mathbold{\Omega}&=\E[\overbar{X}_ie_ie_i'\overbar{X}_i'] \\
&=\E[(\mathbold{I}_m\otimes X_i)(e_ie_i'\otimes 1)(\mathbold{I}_m\otimes X_i')] \\
&=\E[e_ie_i'\otimes X_iX_i']
\end{align*}
如果条件同方差和回归元相同这两个假设同时成立, 那么
\begin{align*}
\mathbold{\Omega}&=\E[(\mathbold{I}_m\otimes X_i)(\mathbold{\Sigma}\otimes1)(\mathbold{I}_m\otimes X_i')] \\
&=\E[\mathbold{\Sigma}\otimes(X_iX_i')]
\end{align*}
不仅如此, 此时还可以将$\V_\beta$化简为$\V_\beta=\mathbold{\Sigma}\otimes(\E[X_iX_i'])^{-1}$.
\end{example}

\begin{example}[协方差矩阵的估计]
我们定义第$i$个观测值的$m\times1$维残差向量$\hat{e}_i=Y_i-\overbar{X}_i\hb$, 以及误差的$m\times m$维协方差矩阵的最小二乘估计量
\begin{equation}\label{eq5.14}
  \hat{\mathbold{\Sigma}}=n^{-1}\sum_{i=1}^{n}\hat{e}_i\hat{e}_i'
\end{equation}

考虑最一般的形式, 直觉告诉我们协方差矩阵$\V_\beta$的估计量为
$$\hat{\V}_{\beta}=(\overbar{\X}'\overbar{\X})^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'\hat{e}_i\hat{e}_i\overbar{X}_i'\right)(\overbar{\X}'\overbar{\X})^{-1}$$
在条件同方差假设(\ref{eq5.10})下, 协方差矩阵估计量简化为
$$\hat{\V}_{\beta}^0=(\overbar{\X}'\overbar{\X})^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'\hat{\mathbold{\Sigma}}\overbar{X}_i'\right)(\overbar{\X}'\overbar{\X})^{-1}$$
如果所有单方程的回归元相同, 那么
$$\hat{\V}_{\beta}=[\mathbold{I}_m\otimes(\X'\X)^{-1}]\left[\sum_{i=1}^{n}(\hat{e}_i\hat{e}_i'\otimes X_iX_i')\right][\mathbold{I}_m\otimes(\X'\X)^{-1}]$$
如果条件同方差和回归元相同这两个假设同时成立, 那么
$$\V_{\beta}^0=\hat{\mathbold{\Sigma}}\otimes(\X'\X)^{-1}$$
容易证明$n\hat{\V}_{\text{OLS}}$和$n\hat{\V}_{\text{OLS}}^0$分别是$\V_\beta$和$\V_\beta^0$的一致估计量.
\end{example}

有时候我们对参数$\theta=R(\beta_1,\cdots,\beta_m)=R(\beta)$感兴趣, 其中$R$是关于系数的可测函数, 此时$\theta$的最小二乘估计量为$\hat{\theta}=R(\hb)$, 使用Delta法可以得到以下定理. 根据这一定理, 我们可以像第四章那样构造适用于参数检验的标准误和检验统计量.
\begin{theorem}
  如果每个单方程都满足假设\ref{pro:pro4.2}和\ref{pro:pro4.3}, 那么当$n\to\infty$时有
  $$\sqrt{n}(\hat{\theta}-\theta)\xrightarrow{d}N(0,\V_\theta)$$
  其中$\V_\theta=\RH\V_\beta\RH'$, $\displaystyle\RH=\nabla_\beta R(\beta)$.
\end{theorem}
\section{SGLS的渐近性质}
首先推导SGLS估计量, 在回归方程
$$Y_i=\overbar{X}_i\beta+e_i$$
等号两端左乘$\mathbold{\Sigma}_i^{-\frac{1}{2}}$得到
$$Y_i^\ast=\overbar{X}_i^\ast\beta+e_i^\ast$$
此时误差向量满足$\E[e_i^\ast e_i^\ast{'}]=\mathbold{I}_m$, 使用SOLS对以上方程估计即可得到SGLS估计量
\begin{equation}\label{eq5.12}
  \hb_\text{GLS}=\left(\sum_{i=1}^{n}\overbar{X}_i'\mathbold{\Sigma}_i^{-1}\overbar{X}_i\right)^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'\mathbold{\Sigma}_i^{-1}Y_i\right)
\end{equation}
除了用这种方法外, 还可以基于以下矩阵形式的回归系统推导SGLS估计量
$$Y=\overbar{\X}\beta+e$$
这里误差项$e$的协方差矩阵为$\E[ee']=\mathbold{I}_n\otimes \mathbold{\Sigma}_i$, 于是
\begin{equation}\label{eq5.13}
  \hb_\text{GLS}=[\overbar{\X}'(\mathbold{I}_n\otimes\mathbold{\Sigma}_i^{-1})\overbar{\X}]^{-1}[\overbar{\X}'(\mathbold{I}_n\otimes\mathbold{\Sigma}_i^{-1})Y]
\end{equation}
表达式(\ref{eq5.12})和(\ref{eq5.13})在代数上是等价的, 证明也非常容易.

不同于之前在定理\ref{thm:thm5.1}中只需将假设\ref{pro:pro4.2}扩张到每个单方程成立, 在讨论关于GLS估计量的渐近性质之前, 我们需要给出一些更强的正则假设.

\begin{proposition}\label{pro:pro5.1}
$\E[\overbar{X}_i\otimes e_i]=0$.
\end{proposition}

\begin{lemma}\label{lem:lem5.1}
如果假设\ref{pro:pro5.1}成立, 那么当$n\to\infty$时有$n^ {-1}\sum_{i=1}^{n}\overbar{X}_i'\mathbold{\Sigma}^{-1}_ie_i\xrightarrow{p}\E[\overbar{X}_i'\mathbold{\Sigma}_i^{-1}e_i]=0$.
\end{lemma}
\begin{proof}
  该引理的证明需要用到矩阵理论的$\text{vec}$算子\footnote{对于矩阵$\mathbold{A}_{m\times n}=\begin{bmatrix}
                                                                   a_1 & \cdots & a_n
                                                                 \end{bmatrix}$, $\text{vec}(\mathbold{A})=\begin{bmatrix}
                                                                                             a_1 \\
                                                                                             \vdots \\
                                                                                             a_n
                                                                                           \end{bmatrix}_{mn\times1}$.

\vbox{}

\noindent $\text{vec}$算子具有如下性质: 对于适合的矩阵$\mathbold{D}$, $\mathbold{E}$和$\mathbold{F}$, $\text{vec}(\mathbold{DEF})=(\mathbold{F}'\otimes \mathbold{D})\text{vec}(\mathbold{E})$.}, 根据它的性质可得
  $$\text{vec}(\E[\overbar{X}_i'\mathbold{\Sigma}_i^{-1}e_i])=\E[\text{vec}(\overbar{X}_i'\mathbold{\Sigma}_i^{-1}e_i)]=\E[(e_i'\otimes\overbar{X}_i')\text{vec}(\mathbold{\Sigma}_i^{-1})]=0$$
  因此由WLLN即可得到
  $n^ {-1}\sum_{i=1}^{n}\overbar{X}_i'\mathbold{\Sigma}_i^{-1}e_i\xrightarrow{p}\E[\overbar{X}_i'\mathbold{\Sigma}_i^{-1}e_i]=0$.
\end{proof}
\begin{remark}
该引理证明的逻辑是, 如果将$\E[\overbar{X}_i\mathbold{\Sigma}^{-1}\overbar{X}_i]$中的每一列堆叠起来后为零矩阵, 那么原来的矩阵也必为零矩阵.
\end{remark}

下面第二个假设类似于假设\ref{pro:pro4.2}(4), 是为了避免出现奇异矩阵的情况.
\begin{proposition}\label{pro:pro5.2}
协方差矩阵$\mathbold{\Sigma}_i$正定, 并且$\E[\overbar{X}_i'\mathbold{\Sigma}_i^{-1}\overbar{X}_i]$可逆.
\end{proposition}
除此之外, 我们还假定某些弱的矩条件成立, 正如假设\ref{pro:pro4.1}和假设\ref{pro:pro4.2}提到的那样. 首先写出表达式
$$\hb_{\text{GLS}}-\beta=\left(n^{-1}\sum_{i=1}^{n}\OX_i'\BS_i^{-1}\OX_i\right)^{-1}\left(n^{-1}\sum_{i=1}^{n}\OX_i'\BS_i^{-1}e_i\right)$$
根据WLLN和CMT可知
$$n^{-1}\sum_{i=1}^{n}\OX_i'\BS_i^{-1}\OX_i\xrightarrow{p}\E[\OX_i'\BS_i^{-1}\OX_i]$$
再根据引理\ref{lem:lem5.1}可得$n^{-1}\sum_{i=1}^{n}\OX_i'\BS_i^{-1}e_i\xrightarrow{p}0$, 从而GLS估计量是一致的. 注意到
$$\sqrt{n}(\hb_{\text{GLS}}-\beta)=\left(n^{-1}\sum_{i=1}^{n}\OX_i'\BS_i^{-1}\OX_i\right)^{-1}\left(n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\BS_i^{-1}e_i\right)$$
根据CLT可知
$$n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\BS_i^{-1}e_i\xrightarrow{d} N(0,\BO)$$
其中$\BO=\E[\OX_i\BS_i^{-1}e_ie_i'\BS_i^{-1}\OX_i]$, 于是
$$\sqrt{n}(\hb_{\text{GLS}}-\beta)\xrightarrow{d} \mathbold{Q}^{-1}\BO\mathbold{Q}^{-1}$$
这里$\Q=\E[\OX_i'\BS_i^{-1}\OX_i]$.
\section{似不相关回归}
最后来看如下具有条件期望和条件同方差的系统回归模型
\begin{align}
Y_i&=\overbar{X}_i\beta+e_i \label{eq5.11} \\
\E[e_i|\OX_i]&=0 \nonumber \\
\E[e_ie_i'|\OX_i]&=\mathbold{\Sigma} \nonumber
\end{align}
也即单方程的扰动项不存在自相关且方差相同, 但随机扰动项在不同方程之间存在相关性, 上述模型称为似不相关回归 (Seemingly Unrelated Regression, SUR).

由于$\mathbold{\Sigma}$的具体形式是未知的, 我们考虑使用(\ref{eq5.14})所给出的$\hat{\mathbold{\Sigma}}$来得到系统FGLS估计量
\begin{align}
\begin{split}
\hat{\beta}_{\text{SUR}}&=\left(\sum_{i=1}^{n}\overbar{X}_i'\hat{\mathbold{\Sigma}}^{-1}\overbar{X}_i\right)^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'\hat{\mathbold{\Sigma}}^{-1}Y_i\right) \\
&=[\overbar{\X}'(\mathbold{I}_n\otimes\hat{\mathbold{\Sigma}}^{-1})\overbar{\X}]^{-1}[\overbar{\X}'(\mathbold{I}_n\otimes\hat{\mathbold{\Sigma}}^{-1})Y]
\end{split}
\label{eq5.15}
\end{align}
称为SUR估计量, 由Zellner (1962)提出.

上述SUR估计量中的$\hat{\mathbold{\Sigma}}$的构建基于SOLS获得的残差, 而一旦我们得到了初始SUR估计量, 就能将(\ref{eq5.14})中的$\hat{e}_i$替换为$\check{e}_i=Y_i-\overbar{X}_i'\hb_{\text{SUR}}$, 并且构造新的协方差矩阵估计量$\hat{\mathbold{\Sigma}}=n^{-1}\sum_{i=1}^{n}\check{e}_i\check{e}_i'$, 然后将其迭代到SUR估计量中直至收敛.

可以证明, 仅凭每个单方程满足假设\ref{pro:pro4.2}无法推出$\E[\overbar{X}_i'\mathbold{\Sigma}^{-1}e_i]=0$成立, 这也是假设\ref{pro:pro5.1}更强的地方所在.

\begin{theorem}
  如果假设\ref{pro:pro5.1}成立, 并且$\E[\overbar{X}_i'\overbar{X}_i]$是有限且正定的, 那么当$n\to\infty$时有
  $$\hat{\mathbold{\Sigma}}=n^{-1}\sum_{i=1}^{n}\hat{e}_i\hat{e}_i'\xrightarrow{p}\mathbold{\Sigma}$$
  其中$\hat{e}_i=Y_i-\overbar{X}_i\hat{\beta}$为SOLS估计残差.
\end{theorem}
\begin{proof}
  注意到$\hat{e}_i=e_i-\OX_i(\hb-\beta)$, 于是
  \begin{equation}\label{eq5.16}
    \hat{e}_i\hat{e}_i'=e_ie_i'-e_i(\hb-\beta)'\OX_i'-\OX_i(\hb-\beta)e_i'+\OX_i(\hb-\beta)(\hb-\beta)'\OX_i'
  \end{equation}
  只需证明后三项的$\text{vec}$均值依概率收敛于0即可.

  首先, 式(\ref{eq5.16})第二项的$\text{vec}$均值为
  $$n^{-1}\sum_{i=1}^{n}(\OX_i\otimes e_i)\cdot\text{vec}[(\hb-\beta)']$$
  由于$\hb\xrightarrow{p}\beta$以及$n^{-1}\sum_{i=1}^{n}(\OX_i\otimes e_i)\xrightarrow{p}0$, 因此上式为$\text{o}_p(1)$. 第三项是第二项的转置, 因此也是$\text{o}_p(1)$. 最后一项的$\text{vec}$均值为
  $$n^{-1}\sum_{i=1}^{n}(\OX_i\otimes\OX_i)\cdot\text{vec}[(\hb-\beta)(\hb-\beta)']$$
  由于$n^{-1}\sum_{i=1}^{n}(\OX_i\otimes\OX_i)=\text{O}_p(1)$, 故而最后一项仍为$\text{o}_p(1)$. 因此
  $$\hat{\BS}=n^{-1}\sum_{i=1}^{n}e_ie_i'+\text{o}_p(1)$$
  因此$\text{plim}\,\hat{\BS}=\BS$.

\end{proof}
以下定理中, 我们都假定适合CLT的矩条件成立.
\begin{theorem}\label{thm:thm5.2}
  在假设\ref{pro:pro5.1}和\ref{pro:pro5.2}下, 当$n\to\infty$时有
  $$\sqrt{n}(\hb_{\text{SUR}}-\beta)\xrightarrow{d}N(0,\V_\beta^\ast)$$
  其中$\V_\beta^\ast=\Q^{-1}\mathbold{\Omega}\Q^{-1}$, $\Q=\E[\overbar{X}_i'\mathbold{\Sigma}\overbar{X}_i]$, 以及$\mathbold{\Omega}=\E[\overbar{X}_i'\mathbold{\Sigma}^{-1}e_ie_i'\mathbold{\Sigma}^{-1}\overbar{X}_i]$.
\end{theorem}
\begin{proof}
  注意到
  \begin{equation}\label{eq5.17}
    \sqrt{n}(\hb_{\text{SUR}}-\beta)=\left(n^{-1}\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}\OX_i\right)^{-1}\left(n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}e_i\right)
  \end{equation}
  对上式等号右端的第二项使用$\text{vec}$算子, 由于该项本身即为列向量, 因此
  $$n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}e_i-n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\BS^{-1}e_i=\left[n^{-\frac{1}{2}}\sum_{i=1}^{n}(e_i\otimes\OX_i)'\right]\text{vec}(\hat{\BS}^{-1}-\BS^{-1})$$
  根据假设\ref{pro:pro5.1}, CLT意味着$n^{-\frac{1}{2}}\sum_{i=1}^{n}(e_i\otimes\OX_i)=\text{O}_p(1)$, 又因为$\hat{\BS}$是$\BS$的一致估计量, 于是
  $$n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}e_i=n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\BS^{-1}e_i+\text{o}_p(1)$$
  类似可证$n^{-1}\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}\OX_i=n^{-1}\sum_{i=1}^{n}\OX_i'\BS^{-1}\OX_i+\text{o}_p(1)$. 因此可将(\ref{eq5.17})改写为
  $$\sqrt{n}(\hb_{\text{SUR}}-\beta)=\left(n^{-1}\sum_{i=1}^{n}\OX_i'\BS^{-1}\OX_i\right)^{-1}\left(n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\BS^{-1}e_i\right)+\text{o}_p(1)$$

  根据CLT可知
  $$n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\BS^{-1}e_i\xrightarrow{d}N(0,\BO)$$
  这里$\mathbold{\Omega}=\E[\overbar{X}_i'\mathbold{\Sigma}^{-1}e_ie_i'\mathbold{\Sigma}^{-1}\overbar{X}_i]$. 根据WLLN和CMT可知
  $$\left(n^{-1}\sum_{i=1}^{n}\OX_i'\BS^{-1}\OX_i\right)^{-1}-\Q^{-1}=\text{o}_p(1)$$
  这里$\Q=\E[\OX_i'\BS^{-1}\OX_i]$. 因为$n^{-\frac{1}{2}}\sum_{i=1}^{n}\OX_i'\BS^{-1}e_i=\text{O}_p(1)$, 于是
  $$\sqrt{n}(\hb_{\text{SUR}}-\beta)=\Q^{-1}\left(n^{-1/2}\sum_{i=1}^{n}\OX_i'\BS^{-1}e_i\right)+\text{o}_p(1)$$
  最后使用Slutsky定理即可证得结论.

\end{proof}
在FGLS背景下, $\Q$的一致估计量为
$$\hat{\Q}=n^{-1}\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}\OX_i$$
而$\BO$的一致估计量为
$$\hat{\BO}=n^{-1}\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}\hat{e}_i\hat{e}_i'\hat{\BS}^{-1}\OX_i$$
其中$\hat{e}_i=Y_i-\overbar{X}_i\hb$, 并且这里的$\hb$为SOLS估计量. 现在可以得到$\hb_{\text{SUR}}$的渐近协方差矩阵$\text{avar}(\hb_{\text{SUR}})$的一致估计量
$$\hat{\V}_{\text{SUR}}=\left(\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}\OX_i\right)^{-1}\left(\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}\hat{e}_i\hat{e}_i'\hat{\BS}^{-1}\OX_i\right)\left(\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}\OX_i\right)^{-1}$$
特别地, 如果定理\ref{thm:thm5.2}中的条件还包括(\ref{eq5.10}), 那么
\begin{align*}
\E[\OX_i'\BS^{-1}e_ie_i'\BS^{-1}\OX_i]&=\E[\E[\OX_i'\BS^{-1}e_ie_i'\BS^{-1}\OX_i|\OX_i]] \\
&=\E[\OX_i'\BS^{-1}\E[e_ie_i'|\OX_i]\BS^{-1}\OX_i]=\E[\OX_i'\BS^{-1}\OX_i]
\end{align*}
此时$\V_\beta^\ast=(\E[\OX_i'\BS^{-1}\OX_i])^{-1}$, $\text{avar}(\hb_{\text{SUR}})$的一致估计量为
$$\hat{\V}_{\text{SUR}}^0=\left(\sum_{i=1}^{n}\OX_i'\hat{\BS}^{-1}\OX_i\right)^{-1}$$


\begin{theorem}\label{thm:thm5.3}
  在假设\ref{pro:pro5.1}和\ref{pro:pro5.2}下, 如果条件(\ref{eq5.10})成立, 那么
  $$\V_\beta-\V_\beta^\ast\sim\,\text{p.s.d.}$$
  其中$\V_\beta=(\E[\OX_i'\OX_i])^{-1}\E[\OX_i'\BS\OX_i](\E[\OX_i'\OX_i])^{-1}$, 这表明SUR估计量比SOLS估计量更渐近有效.
\end{theorem}
\begin{proof}
  考虑以下式子
  $$\mathbold{A}=(\overbar{\X}'\overbar{\X})^{-1}[\overbar{\X}'(\mathbold{I}_n\otimes\mathbold{\Sigma})\overbar{\X}](\overbar{\X}'\overbar{\X})^{-1}-[\overbar{\X}'(\mathbold{I}_n\otimes \mathbold{\Sigma}^{-1})\overbar{\X}]^{-1}$$
  由于$\text{plim}\,n\mathbold{A}=\V_\beta-\V_\beta^\ast$, 所以只需证明$\mathbold{A}$半正定即可. 记$\mathbold{B}=\mathbold{I}_n\otimes\BS$, 于是
  $$\mathbold{A}=\mathbold{C}\mathbold{D}\mathbold{C}'$$
  其中$\mathbold{C}=(\overbar{\X}'\overbar{\X})^{-1}\overbar{\X}'\mathbold{B}^{\frac{1}{2}}$, $\mathbold{D}=\mathbold{I}_{mn}-\mathbold{B}^{-\frac{1}{2}}\overbar{\X}(\overbar{\X}'\mathbold{B}^{-1}\overbar{\X})^{-1}\overbar{\X}'\mathbold{B}^{-\frac{1}{2}}$, 显然$\mathbold{D}$是一个对称幂等矩阵, 因此
  $$\mathbold{A}=(\mathbold{CD})(\mathbold{CD})'\sim\,\text{p.s.d.}$$
  证毕.
\end{proof}
\begin{theorem}\label{thm:thm5.4}
  如果以下条件中的任意一个成立, 则SUR估计量等价于SOLS估计量.

  (1) 每个单方程的回归元均相同.

  (2) 协方差矩阵$\mathbold{\Sigma}$为对角矩阵.
\end{theorem}
\begin{proof}
  (1) 根据条件可知
  \begin{align*}
  \OX_i'\hat{\BS}^{-1}&=(\mathbold{I}_m\otimes X_i)\hat{\BS}^{-1}=\hat{\BS}^{-1}\otimes X_i \\
  &=(\hat{\BS}^{-1}\otimes\mathbold{I}_K)(\mathbold{I}_m\otimes X_i)=(\hat{\BS}^{-1}\otimes\mathbold{I}_K)\OX_i'
  \end{align*}
  因此
  \begin{align*}
  \hb_{\text{SUR}}&=\left(\sum_{i=1}^{n}\overbar{X}_i'\hat{\mathbold{\Sigma}}^{-1}\overbar{X}_i\right)^{-1}\left(\sum_{i=1}^{n}\overbar{X}_i'\hat{\mathbold{\Sigma}}^{-1}Y_i\right) \\
  &=\left[(\hat{\BS}^{-1}\otimes\mathbold{I}_K)\sum_{i=1}^{n}\OX_i'\OX_i\right]^{-1}\left[(\hat{\BS}^{-1}\otimes\mathbold{I}_K)\sum_{i=1}^{n}\OX_i'Y_i\right] \\
  &=\left(\sum_{i=1}^{n}\OX_i'\OX_i\right)^{-1}\left(\sum_{i=1}^{n}\OX_i'Y_i\right)=\hb
  \end{align*}

  (2) 设$\BS=\text{diag}(\sigma^2_1,\cdots,\sigma_m^2)$, 于是$\hat{\BS}^{-1}=\text{diag}(\hat{\sigma}_1^{-2},\cdots,\hat{\sigma}_m^{-2})$, 并且
  $$\OX_i'\hat{\BS}^{-1}\OX_i=\hat{\mathbold{\Psi}}^{-1}\OX_i'\OX_i,\quad \OX_i'\hat{\BS}^{-1}Y_i=\hat{\mathbold{\Psi}}^{-1}\OX_i'Y_i$$
  其中$\hat{\mathbold{\Psi}}$为分块对角矩阵, 它的第$j$个块为$\hat{\sigma}^2_j\mathbold{I}_{K_j}$, $j=1,2,\cdots,m$. 因此SUR估计量
  \begin{align*}
  \hb_{\text{SUR}}&=\left(\sum_{i=1}^{n}\hat{\mathbold{\Psi}}^{-1}\OX_i'\OX_i\right)^{-1}\left(\sum_{i=1}^{n}\hat{\mathbold{\Psi}}^{-1}\OX_i'Y_i\right) \\
  &=\left(\sum_{i=1}^{n}\OX_i'\OX_i\right)^{-1}\left(\sum_{i=1}^{n}\OX_i'Y_i\right)=\hb
  \end{align*}
\end{proof}
根据定理\ref{thm:thm5.4}, 为了使SUR估计量和SOLS估计量不同, $X_j$必须随着$j$的变化而变化, 此时某些解释变量更有可能被遗漏, 而定理\ref{thm:thm5.3}却表明SUR估计量更加渐近有效, 这就需要我们权衡有效性与稳健性.

举例而言, 对于某个双回归方程构成的系统, 倘若我们对第一个方程感兴趣, 而第二个方程出现了模型误设, 那么此时使用FGLS得到的SUR估计量均是非一致估计量, 而只要$\E[X_1e_i]=0$, 则对第一个方程使用OLS仍能得出一致估计量.

当然, 如果整个回归系统设定正确, 也即假设(\ref{pro:pro5.1}), (\ref{pro:pro5.2})以及条件(\ref{eq5.10})成立, 那么SUR估计量不仅是一致的, 且更加渐近有效.

\chapter{工具变量回归分析}
在前面各章节的渐近分析中, 无一例外都牵扯到了$\E[Xe]=0$这一关键条件, 当它不成立时 (例如第二章提到的遗漏变量)就会导致微观计量领域中占据重要地位的内生性问题, 内生性的解决办法之一就是使用本章讨论的工具变量 (Instrumental Variables, IV).

\section{内生性问题}
我们称线性模型
\begin{equation}\label{eq6.1}
  Y=X'\beta+e
\end{equation}
存在内生性, 如果以下式子成立
\begin{equation*}\label{eq6.2}
  \E[Xe]\neq0
\end{equation*}
此时称$X$关于$\beta$是内生的, $X$为内生变量. 为了将(\ref{eq6.1})同回归和投影模型进行区分, 后面称(\ref{eq6.1})为结构方程 (structural equation), $\beta$为结构参数 (structural parameter).

如果回归系数由线性投影模型所定义, 那么不可能出现内生性, 为了看清这一点, 我们定义线性投影系数$\beta^\ast=\E[XX']^{-1}\E[XY]$以及线性投影模型
\begin{align*}
Y&=X'\beta^\ast+e^\ast \\
\E[Xe^\ast]&=0
\end{align*}
然而在(\ref{eq6.2})下, 投影系数$\beta^\ast$并不等于结构参数$\beta$, 这是因为
\begin{align}
\beta^\ast&=\E[XX']^{-1}\E[XY] \nonumber \\
&=\E[XX']^{-1}\E[X(X'\beta+e)] \nonumber \\
&=\beta+\E[XX']^{-1}\E[Xe]\neq\beta \label{eq6.6}
\end{align}
内生性将导致结构参数$\beta$的OLS估计量不一致, 称为内生性偏误 (endogenous bias). 事实上, i.i.d.随机样本下的OLS估计量相对于投影系数是一致的.

造成内生性的原因主要包括遗漏变量偏误, 测量误差以及联立方程偏误, 其中遗漏变量偏误在第一章已经介绍过, 这里不再赘述.
\subsection{测量误差}
首先考虑被解释变量的测量误差, 回归模型具有通常的线性形式
\begin{equation}\label{eq6.3}
  Y^\ast=X'\beta+e
\end{equation}
如果出于某种原因, 我们无法观测到$y^\ast$的真实值, 而只能观测到它的替代值$y$, 此时总体误差定义为$v=y-y^\ast$, 将它代入到方程(\ref{eq6.3})中得到
\begin{equation}\label{eq6.4}
  Y=X'\beta+e+v
\end{equation}
上述模型的自变量和因变量都可观测, 故而可以直接使用OLS估计. 倘若最初模型(\ref{eq6.3})满足假设(\ref{pro:pro4.1}), 那么在变换后的模型(\ref{eq6.4})中, $e$仍然和每个回归元无关, 此时只要假设测量误差$e$关于每个解释变量无关, 则OLS估计量仍然是一致的.

尽管如此, 因变量的测量误差可归为回归扰动项的一部分, 它会增加$\sqrt{n}(\hb-\beta)$的渐近方差. 换言之, 如果因变量存在测量误差, 那么对$\beta$的估计精度会下降.

现在来看含解释变量的测量误差的回归模型
\begin{equation*}\label{eq6.5}
  Y=X'\beta+e
\end{equation*}
其中解释变量$X$无法被观测到, 只能观测到$X=Z+u$, 这里$u$是测量误差, 并且$u$独立于$Z$和$e$. 此时
\begin{equation}\label{eq6.7}
  Y=Z'\beta+e=(X-u)'\beta+e=X'\beta+v
\end{equation}
这里$v=e-u'\beta$, 并且
$$\E[Xv]=\E[(Z+u)(e-u'\beta)]=-\E[uu']\beta$$
只要$\E[ee']\neq0$且$\beta\neq0$, 那么OLS估计量是不一致的.

假设只有一个解释变量, 那么根据(\ref{eq6.6})和(\ref{eq6.7})可知
$$\text{plim}\,\hb=\beta^\ast=\beta\left(1-\frac{\E[u^2]}{\E[X^2]}\right)$$
因为$0<\E[u^2]/\E[X^2]<1$成立, 所以投影系数总比结构参数更接近于0, 这称为测量误差偏误 (measurement error bias)或衰减偏误 (attenuation bias).
\subsection{联立方程偏误}
考虑如下简单国民收入决定模型
\begin{align}
C&=\beta_0+\beta_1Y+e \label{eq6.8} \\
Y&=C+D \nonumber
\end{align}
其中$Y$为国民收入, $C$为消费支出, $D$为非消费支出, 边际消费倾向$0<\beta_1<1$.

假设$D$与$e$相互独立, 并且$\E[e]=0$. 现在考虑使用OLS来估计方程(\ref{eq6.8})的消费函数, 注意到
\begin{align*}
\E[Ye]&=\E[(C+D)e]=\beta_1\E[Ye]+\E[e^2]
\end{align*}
移项后得
$$\E[Ye]=\frac{\E[e^2]}{1-\beta_1}$$
显然OLS估计量也是非一致的. 由于国民收入与消费在这个回归系统内互相决定, 这又称为联立方程偏误 (simultaneous equations bias).

\section{工具变量}
\subsection{工具变量的定义}
上面已经提到, 当回归元与误差项相关时会导致内生性, 它的相反概念则是外生性 (exogeneity). 我们称回归元$X$关于$\beta$是外生的, 如果$\E[Xe]=0$成立, 称$X$为外生变量.

在许多情况下, 一个回归模型既有内生变量也有内生变量, 我们将回归元分割为$X=[X_1',X_2']'$, 其中$X_1$为外生变量而$X_2$为内生变量, $X_1$和$X_2$的维数分别为$K_1\times1$和$K_2\times1$. 类似地, 将结构参数分割为$\beta=[\beta_1',\beta_2']'$, 于是结构模型变为
\begin{equation}\label{eq6.10}
  Y=X_1'\beta_1+X_2'\beta_2+e
\end{equation}
由于$X_2$是内生变量, 结构参数$\beta_1$和$\beta_2$的OLS估计量均是不一致的, 为了解决这一问题, 我们需要用到工具变量.
\begin{definition}\label{def:def6.1}
$L\times1$维随机向量$Z$称为模型(\ref{eq6.10})的工具变量, 如果
\begin{align}
\E[Ze]&=0 \label{eq6.11} \\
\E[ZZ']&>0 \label{eq6.12} \\
\text{rank}\,(\E[ZX'])&=K \label{eq6.13}
\end{align}
\end{definition}
在以上定义中, (\ref{eq6.11})表明工具变量和误差项不相关, (\ref{eq6.12})排除了线性相关的冗余工具变量, (\ref{eq6.13})称为模型可识别的秩条件. 后面我们将看到, 为了使(\ref{eq6.13})成立, 一个必要条件是$L\geq K$. 如果$L=K$, 则称模型恰好识别 (just identified); 如果$L>K$, 则称模型过度识别 (over identified).

\begin{example}
Dube and Harish (2020)的研究表明了15至20世纪处于女王统治的国家更容易发动战争, 由于王位继承是内生的, 作者选取了上一代君主第一个孩子的性别以及是否有姊妹作为女王统治的工具变量, 这些因素当然和王位继承有关, 但由于性别因素是由遗传学决定的, 因此生男生女不太可能和误差项相关.
\end{example}

\subsection{结构参数的识别}
考虑分割工具变量
$$Z=\begin{bmatrix}
      Z_1 \\
      Z_2
    \end{bmatrix}=\begin{bmatrix}
                    X_1 \\
                    Z_2
                  \end{bmatrix}$$
由于$X_1$是外生的, 它的工具变量$Z_1=X_1$, 而内生变量$X_2$的工具变量$Z_2$则需来源于模型外, 它的维数为$L_2\times 1$且$L_2\geq K_2$.

现在来考虑关于内生变量$X_2$的简约式方程 (reduced form equation)
\begin{equation}\label{eq6.14}
  X_2=\Gamma'Z+u_2=\Gamma_1'Z_1+\Gamma_2'Z_2+u_2
\end{equation}
它是上一章所提到的多方程回归系统, $L\times K_2$维的系数矩阵$\Gamma$由线性投影
\begin{equation}\label{eq6.15}
  \Gamma=\E[ZZ']^{-1}\E[ZX_2']
\end{equation}
这意味着$\E[Zu_2']=0$, 投影系数在条件(\ref{eq6.12})下是唯一的. 现在考虑$Y$的简约式方程, 将$X_1=Z_1$和(\ref{eq6.14})代入到(\ref{eq6.10})中得到
\begin{align*}
Y&=Z_1'\beta_1+(\Gamma_1'Z_1+\Gamma_2'Z_2+u_2)'\beta_2+e \\
&=Z_1'\lambda_1+Z_2'\lambda_2+u_1 \\
&=Z'\lambda+u_1
\end{align*}
其中
\begin{align*}
\lambda_1&=\beta_1+\Gamma_1\beta_2 \\
\lambda_2&=\Gamma_2\beta_2 \\
u_1&=u_2'\beta_2+e
\end{align*}
此外, 还可以写作
\begin{equation}\label{eq6.16}
  \lambda=\overbar{\Gamma}\beta
\end{equation}
这里
$$\overbar{\Gamma}=\begin{bmatrix}
                     \mathbold{I}_{K_1} & \Gamma_1 \\
                     0 & \Gamma_2
                   \end{bmatrix}$$
以上这些式子刻画了结构参数($\beta_1$和$\beta_2$)与简约式参数($\Gamma$和$\lambda$)之间紧密的联系. 最后, 我们可以写出这个系统的简约式方程
\begin{align*}
Y&=\lambda'Z+u_1 \\
X_2&=\Gamma'Z+u_2
\end{align*}

一个参数是可识别的, 如果它能被可观测变量的概率分布唯一确定, 一种表明参数可识别的方法是将其写为总体矩的显式函数 (第一章所使用过的方法).

如果定义\ref{def:def6.1}中的条件被满足, 那么简约形式的参数$\Gamma$和$\lambda$是可识别的, 因为它们可以写为变量$(Y,X,Z)$总体矩的显式函数
\begin{align}
\Gamma&=\E[ZZ']^{-1}\E[ZX_2'] \label{eq6.18} \\
\lambda&=\E[ZZ']^{-1}\E[ZY] \label{eq6.19}
\end{align}
现在我们对结构参数$\beta$感兴趣, $\beta$与$(\Gamma,\lambda)$的关系通过(\ref{eq6.16})联系在一起, 如果能被(\ref{eq6.16})唯一确定, 那么$\beta$是可识别的. 根据线性代数可知, 当且仅当$\overbar{\Gamma}$满秩时, 也即
\begin{equation}\label{eq6.17}
  \text{rank}\,(\overbar{\Gamma})=K
\end{equation}
$\beta$可以由(\ref{eq6.16})唯一识别.

利用线性代数, 我们可以将矩阵$\overbar{\Gamma}$表示为$\overbar{\Gamma}=\E[ZZ']^{-1}\E[ZX']$, 将它和(\ref{eq6.19})代入到(\ref{eq6.16})中得到
$$\E[ZY]=\E[ZX']\beta$$
这是一个由$L$个方程构成的含$K$个未知参数的系统, 当且仅当
$$\text{rank}\,(\E[ZX'])=K$$
时有唯一解, 这也是定义\ref{def:def6.1}中的秩条件. 换言之, (\ref{eq6.17})和$(\ref{eq6.13})$是结构参数$\beta$可识别的等价条件.

当$L<K$时, 整个方程组无解. 当$L=K$时, (\ref{eq6.17})意味着矩阵$\overbar{\Gamma}$可逆, 此时$\beta=\overbar{\Gamma}^{-1}\lambda$是(\ref{eq6.16})的唯一解. 当$L>K$时, $\overbar{\Gamma}$不再是方阵, 我们可以对$\lambda=\overbar{\Gamma}\beta$使用系统最小二乘以获得结构参数$\beta$的显式表达$\beta=(\overbar{\Gamma}'\overbar{\Gamma})^{-1}\overbar{\Gamma}'\lambda$, 其中(\ref{eq6.17})同样保证了矩阵$\overbar{\Gamma}'\overbar{\Gamma}$的可逆性.

此外, 秩条件(\ref{eq6.13})意味着工具变量$Z$与解释变量$X$具有相关性. 假设模型只有一个回归元, $X=(1,X_1)'$, $Z=(1,W_1)'$, 那么
$$\text{rank}\,(\E[ZX'])=2\Leftrightarrow \E[Z_1W_1]-\E[Z_1]\E[W_1]\neq0$$
也即$Z_1$和$X_1$相关.

总结一下定义\ref{def:def6.1}的三点要求: 一个合适的工具变量需要与误差项不相关, 但要与解释变量相关, 且工具变量之间不存在完全的线性关系. 为了保证结构参数$\beta$可识别, 工具变量的个数至少要与解释变量一样多.
\subsection{工具变量估计量}
本节考虑模型恰好识别的特殊情况, 也即工具变量个数同解释变量个数一样多. 此时
\begin{equation}\label{eq6.20}
  \beta=\overbar{\Gamma}^{-1}\lambda=\E[ZX']^{-1}\E[ZY]
\end{equation}
上式的存在性由(\ref{eq6.12})和(\ref{eq6.13})保证. 除了用这种方法得到$\beta$的表达式外, 还可以通过$e=Y-X'\beta$和矩条件$\E[Ze]=0$推导.

结构参数$\beta$的工具变量估计量$\hb_{\text{IV}}$可以通过矩估计获得, 也即用样本矩替代(\ref{eq6.20})中的总体矩, 于是
\begin{align*}
\hb_{\text{IV}}&=\left(n^{-1}\sum_{i=1}^{n}Z_iX_i'\right)^{-1}\left(n^{-1}\sum_{i=1}^{n}Z_iY_i\right) \\
&=\left(\sum_{i=1}^{n}Z_iX_i'\right)^{-1}\left(\sum_{i=1}^{n}Z_iY_i\right)
\end{align*}
此外, 我们还可以使用矩阵符号进行推导
\begin{align*}
\hat{\beta}_{\text{IV}}&=(\Z'\X)^{-1}\Z'Y
\end{align*}
这里的$\Z$和$\X$分别是将观测值堆叠起来的矩阵, 其维数分别为$n\times L$和$n\times K$. 此外, 我们还能得到
\begin{align*}
\frac{\Z'\Z}{n}&=n^{-1}\sum_{i=1}^{n}Z_iZ_i'=\hat{\Q}_{ZZ} \\
\frac{\X'\Z}{n}&=n^{-1}\sum_{i=1}^{n}X_iZ_i'=\hat{\Q}_{XZ}\\
\frac{\Z'\X}{n}&=n^{-1}\sum_{i=1}^{n}Z_iX_i'=\hat{\Q}_{ZX}
\end{align*}
它们将被用于后续的渐近推导.
\section{二阶段最小二乘估计}

在上文推导IV估计量时, 我们考虑的是$L=K$这一特殊条件. 由于$L>K$时的矩阵$\overbar{\Gamma}$不再可逆, 因此无法通过$\lambda=\overbar{\Gamma}\beta$和矩估计获得$\beta$的估计量. 一个直觉是将多余的工具变量扔掉来获得IV估计量, 尽管这样是可行的, 但同时也会损失许多信息. 在这种情况下, 应该使用二阶段最小二乘(Two-Stage Least Squares, 2SLS)来估计$\beta$, 下一章将会证明在条件同方差下, 2SLS估计量在线性工具变量估计量类中是渐近有效的.

\textbf{第一阶段}: 使用$X$对$Z$进行SOLS回归得到拟合值$\hat{X}$. 考虑辅助线性回归模型
$$X_i=\Gamma'Z_i+u_i,\quad i=1,2,\cdots,n$$
将其写为堆叠的矩阵形式
$$\X=\Z\Gamma+\mathbold{u}$$
其中$\X$为$n\times K$维矩阵, $\Z$为$n\times L$维矩阵, $\Gamma$为$L\times K$维矩阵, $\mathbold{u}$为$n\times K$维随机误差项矩阵. 由此得到$\Gamma$的SOLS估计量
\begin{align*}
\hat{\Gamma}&=(\Z'\Z)^{-1}\Z'\X \\
&=\left(\sum_{i=1}^{n}Z_iZ_i'\right)^{-1}\left(\sum_{i=1}^{n}Z_iX_i'\right)
\end{align*}
进一步得到拟合值
$$\hat{X}_i=\hat{\Gamma}'X_i$$
用矩阵形式表示为
$$\hat{\X}=\Z\hat{\Gamma}=\Z(\Z'\Z)^{-1}\Z'\X$$

\textbf{第二阶段}: 使用$Y$对拟合值$\hat{X}$进行SOLS回归. 第二阶段的回归模型可以写为
$$Y_i=\hat{X}_i\beta+v_i,\quad i=1,2,\cdots,n$$
可以用矩阵形式表示为
$$Y=\hat{\X}\beta+v$$
第二阶段$\beta$的OLS估计量即为2SLS估计量
\begin{align*}
\hb_{\text{2SLS}}&=(\hat{\X}'\hat{\X})^{-1}\hat{\X}'Y \\
&=[\X'\Z(\Z'\Z)^{-1}\Z'\X]^{-1}\X'\Z(\Z'\Z)^{-1}\Z'Y
\end{align*}
其中$\hat{\X}=\Z\hat{\Gamma}=\Z(\Z'\Z)^{-1}\Z'\X$. 注意, 第二阶段回归的残差为$\hat{v}=Y-\hat{\X}\hb_{\text{2SLS}}$, 而结构方程中的残差为$\hat{e}=Y-\X\hb_{\text{2SLS}}$.

上述推导2SLS估计量的方法说明了它名称的由来. 除此之外, 我们还可以从简约式方程
$$Y=Z'\lambda+u_1$$
进行推导, 将$\lambda=\overbar{\Gamma}\beta$代入上式得到
\begin{align*}
Y&=Z'\overbar{\Gamma}\beta+u_1 \\
\E[Zu_1]&=0
\end{align*}
定义$W=\overbar{\Gamma}'Z$, 我们可以将上式写为
\begin{align*}
Y&=W'\beta+u_1 \\
\E[Wu_1]&=0
\end{align*}
由此得到$\beta$的最小二乘估计量
$$\hb=(\mathbold{W}'\mathbold{W})^{-1}\mathbold{W}'Y=(\overbar{\Gamma}'\Z'\Z\overbar{\Gamma})^{-1}\overbar{\Gamma}'\Z' Y$$
但因为$\overbar{\Gamma}$是不可知的, 上述估计量不可行, 考虑使用$\overbar{\Gamma}$的估计量$\hat{\Gamma}=(\Z'\Z)^{-1}\Z'\X$替代$\overbar{\Gamma}$, 最终可以得到2SLS估计量
\begin{align*}
\hb_{\text{2SLS}}&=(\hat{\Gamma}'\Z'\Z\hat{\Gamma})^{-1}\hat{\Gamma}'\Z'Y\\
&=[\X'\Z(\Z'\Z)^{-1}\Z'\X]^{-1}\X'\Z(\Z'\Z)^{-1}\Z'Y
\end{align*}

当模型恰好识别时, 2SLS估计量等价于IV估计量. 这是因为
$$[\X'\Z(\Z'\Z)^{-1}\Z'\X]^{-1}=(\Z'\X)^{-1}(\Z'\Z)(\X'\Z)^{-1}$$
于是
\begin{align*}
\hb_{\text{2SLS}}&=[\X'\Z(\Z'\Z)^{-1}\Z'\X]^{-1}\X'\Z(\Z'\Z)^{-1}\Z'Y \\
&=(\Z'\X)^{-1}(\Z'\Z)(\X'\Z)^{-1}\X'\Z(\Z'\Z)^{-1}\Z'Y \\
&=(\Z'\X)^{-1}\Z'Y=\hb_{\text{IV}}
\end{align*}
因此2SLS估计是IV估计的一种推广, 而后面将会看到, 2SLS估计是生成回归元估计的一个特例.

除此之外, 还能通过FWL定理来获得2SLS估计量. 首先做分割$\X=[\X_1,\X_2]$, $\Z=[\Z_1,\Z_2]$, 注意到$\hat{\X}_1=\mathbold{P}_Z\Z_1=\Z_1$\footnote{因为$\mathbold{P}_Z\Z=\Z$, 而$\Z_1$是$\Z$的子矩阵.}, 从而$\hat{\X}=[\hat{\X}_1,\hat{\X}_2]=[\Z_1,\hat{\X}_2]$. 于是$\Z_2$的结构参数$\beta_2$的2SLS估计量为
\begin{align*}
\hb_2&=[\hat{\X}_2'(\mathbold{I}_n-\mathbold{P}_1)\hat{\X}_2]^{-1}\hat{\X}_2'(\mathbold{I}_n-\mathbold{P}_1)Y \\
&=[\X_2'\mathbold{P}_Z(\mathbold{I}_n-\mathbold{P}_1)\mathbold{P}_Z\X_2]^{-1}\X_2'\mathbold{P}_Z(\mathbold{I}_n-\mathbold{P}_1)Y \\
&=[\X_2'(\mathbold{P}_Z-\mathbold{P}_1)\X_2]^{-1}\X_2'(\mathbold{P}_Z-\mathbold{P}_1)Y
\end{align*}
其中$\mathbold{P}_Z\mathbold{P}_1=\mathbold{P}_1$.

另一方面, 设$\tilde{\Z}_2=(\mathbold{I}_n-\mathbold{P}_1)\Z_2$, 它与矩阵$\Z_1$正交, 从而可以将$\mathbold{P}_Z$正交分解为$$\mathbold{P}_Z=\mathbold{P}_1+\mathbold{P}_2$$
其中$\mathbold{P}_2=\tilde{\Z}_2(\tilde{\Z}_2'\tilde{\Z}_2)^{-1}\tilde{\Z}_2'$, 因此又有
\begin{align}
\hb_2&=(\X_2'\mathbold{P}_2\X_2)^{-1}\X_2'\mathbold{P}_2'Y \nonumber \\
&=[\X_2'\tilde{\Z}_2(\tilde{\Z}_2'\tilde{\Z}_2)^{-1}\tilde{\Z}_2'\X_2]^{-1}\X_2'\tilde{\Z}_2(\tilde{\Z}_2'\tilde{\Z}_2)^{-1}\tilde{\Z}_2'Y \label{eq6.34}
\end{align}
表达式(\ref{eq6.34})在推导面板数据的FE2SLS估计量时会很有用.

\section{2SLS的渐近性质}
\begin{proposition}\label{pro:pro6.1}
(1) $\{Y_i,X_i,Z_i\}_{i=1}^n$是可观测的i.i.d.随机样本.

(2) $\E[Y^2_i]<\infty$.

(3) $\E||X_i||^2<\infty$.

(4) $\E||Z_i||^2<\infty$.

(5) $\E[Z_iZ_i']$正定.

(6) $\text{rank}\,(\E[Z_iX_i'])=K$.

(7) $\E[Z_ie_i]=0$.
\end{proposition}
以上假设的第(1)$-$(4)点表明所有变量都具有有限方差, 第(5)点排除了线性相关的工具变量, 第(6)点是结构参数可识别的秩条件, 第(7)点保证工具变量和结构误差项不相关. 其中, 第(5)$-$(7)点等价于定义\ref{def:def6.1}.

\begin{theorem}
  在假设\ref{pro:pro6.1}下, 当$n\to\infty$时有$\hb_{\text{2SLS}}\xrightarrow{p}\beta$.
\end{theorem}
\begin{proof}
  注意到
  \begin{align*}
  \hb_{\text{2SLS}}-\beta&=\left[\frac{\X'\Z}{n}\left(\frac{\Z'\Z}{n}\right)^{-1}\frac{\Z'\X}{n}\right]^{-1}\frac{\X'\Z}{n}\left(\frac{\Z'\Z}{n}\right)^{-1}\frac{\Z'e}{n} \\
  &=[\hat{\Q}_{XZ}\hat{\Q}_{ZZ}^{-1}\hat{\Q}_{ZX}]^{-1}\hat{\Q}_{XZ}\hat{\Q}_{ZZ}^{-1}\frac{\Z'e}{n}
  \end{align*}
  根据WLLN和CMT可知
  $$\hb_{\text{2SLS}}-\beta\xrightarrow{p}(\Q_{XZ}\Q_{ZZ}^{-1}\Q_{ZX})^{-1}\Q_{XZ}\Q_{ZZ}^{-1}\E[Z_ie_i]=0$$
  这里
  \begin{align*}
  \Q_{XZ}&=\E[X_iZ_i'] \\
  \Q_{ZZ}&=\E[Z_iZ_i'] \\
  \Q_{ZX}&=\E[Z_iX_i']
  \end{align*}
  其中, 假设\ref{pro:pro6.1}(1)和(2)$-$(4)保证WLLN成立, (5)$-$(6)保证CMT成立, (7)保证了最终结论成立.

\end{proof}
\begin{remark}
尽管2SLS估计量在大样本下可以具有一致性, 但在有限样本中它永远不会是无偏估计量. 从这个角度看, 工具变量的数目绝不是越多越好, 因为其数目越多, 出现同质性的工具变量的概率越大, 无法提供更多有效的信息, 反而会增大有限样本偏差. 此外, Kinal (1980)的结果还表明, 在假设\ref{pro:pro6.1}和恰好识别情况下的IV估计量不具有期望值.
\end{remark}

类似地, 如果要将CLT应用到2SLS估计量上, 我们还需要强化假设\ref{pro:pro6.1}.
\begin{proposition}\label{pro:pro6.2}
在假设\ref{pro:pro6.1}的基础上, 以下额外条件成立:

(1) $\E[Y_i^4]<\infty$.

(2) $\E||X_i||^4<\infty$.

(3) $\E||Z_i||^4<\infty$.

(4) $\mathbold{\Omega}=\E[Z_iZ_i'e_i^2]$正定.
\end{proposition}
\begin{theorem}
  在假设\ref{pro:pro6.2}下, 当$n\to\infty$时有
  $$\sqrt{n}(\hb_{\text{2SLS}}-\beta)\xrightarrow{d}N(0,\V_\beta)$$
  其中
  $$\V_\beta=(\Q_{XZ}\Q_{ZZ}^{-1}\Q_{ZX})^{-1}(\Q_{XZ}\Q_{ZZ}^{-1}\BO\Q_{ZZ}^{-1}\Q_{ZX})^{-1}(\Q_{XZ}\Q_{ZZ}^{-1}\Q_{ZX})^{-1}$$
\end{theorem}
\begin{proof}
  首先写出
  \begin{align*}
  \sqrt{n}(\hb_{\text{2SLS}}-\beta)=[\hat{\Q}_{XZ}\hat{\Q}_{ZZ}^{-1}\hat{\Q}_{ZX}]^{-1}\hat{\Q}_{XZ}\hat{\Q}_{ZZ}^{-1}\frac{\Z'e}{\sqrt{n}}
  \end{align*}
  根据Minkowski不等式可知
  $$(\E[e_i^4])^{1/4}\leq (\E[Y_i^4])^{1/4}+||\beta||(\E||X_i||^4)^{1/4}<\infty$$
  又由Cauchy-Schwarz不等式得到
  $$\E||Z_ie_i||^2\leq(\E||Z_i||^4)^{1/2}(\E[e_i^4])^{1/2}<\infty$$
  于是由多元Lindeberg-Levy CLT的条件成立, 因此
  $$\frac{\Z'e}{\sqrt{n}}=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Z_ie_i\xrightarrow{d}N(0,\BO)$$
  其中$\BO=\E[Z_iZ_i'e_i^2]$. 最后根据Slutsky定理得到
  $$\sqrt{n}(\hb_{\text{2SLS}}-\beta)\xrightarrow{d}N(0,\V_\beta)$$
  结论成立.

\end{proof}
特别地, 如果同方差假定成立
\begin{equation}\label{eq6.21}
  \E[Z_iZ_i'e_i^2]=\sigma^2\E[Z_iZ_i']
\end{equation}
那么$\sqrt{n}(\hb_{\text{2SLS}}-\beta)\xrightarrow{d}N(0,\V_\beta^0)$, 这里$\V_\beta^0=\sigma^2(\Q_{XZ}\Q_{ZZ}^{-1}\Q_{ZX})^{-1}$. 同之前的讨论一样, 2SLS的协方差矩阵的一致估计量为
\begin{align*}
\hat{\V}_{\text{2SLS}}&=(\hat{\X}'\hat{\X})^{-1}\left(\sum_{i=1}^{n}\hat{X}_i\hat{X}_i'\hat{e}_i^2\right)(\hat{\X}'\hat{\X})^{-1} \\
\hat{\V}_{\text{2SLS}}^0&=s^2(\hat{\X}'\hat{\X})^{-1}
\end{align*}
其中$\hat{\X}=\Z(\Z'\Z)^{-1}\Z'\X$, $\hat{e}_i=Y_i-X_i'\hat{\beta}_{\text{2SLS}}$, 以及$s^2=n^{-1}\sum_{i=1}^{n}\hat{e}_i^2$.

现在来看参数检验, 考虑线性假设$\HH_0: \RH\beta=r$, 在假设\ref{pro:pro6.2}和\ref{pro:pro4.3}下, 如果原假设$\HH_0$为真, 那么当$n\to\infty$时, 稳健Wald检验统计量
$$W=n(\RH\hat{\beta}_{\text{2SLS}}-r)'\left(\RH\hat{\Q}^{-1}\hat{\BO}\hat{\Q}^{-1}\RH'\right)^{-1}(\RH\hat{\beta}_{\text{2SLS}}-r)\xrightarrow{d}\chi^2_J$$
其中$\hat{\Q}=n^{-1}\sum_{i=1}^{n}\hat{X}_i\hat{X}_i'$, $\hat{\BO}=n^{-1}\sum_{i=1}^{n}\hat{X}_i\hat{X}_i'\hat{e}_i^2$. 如果条件同方差假设成立, 那么Wald检验统计量可以简化为
$$W=\frac{(\RH\hb_{\text{2SLS}}-r)'[\RH(\hat{\X}'\hat{\X})^{-1}\RH']^{-1}(\RH\hb_{\text{2SLS}}-r)}{\hat{e}'\hat{e}/(n-K)}\xrightarrow{d}\chi_J^2$$
其中$\hat{e}=Y-\X\hb_{\text{2SLS}}$.

尽管2SLS框架并没有对内生变量的分布做任何限制, 内生变量既可以是连续的, 也可以是离散的, 这些都不会影响到2SLS的一致性. 然而, 使用连续的外生变量作为二元内生变量的工具可能导致弱工具变量问题.

由于2SLS一阶段采用的是OLS回归, 而内生变量的取值只有0和1, 有的人可能会在2SLS的一阶段使用Probit模型来获得拟合值, 并将其纳入到第二阶段回归. Angrist and Pischke (2009)指出这是禁止回归 (forbidden regression), 会导致第二阶段的估计量不一致 , 主要原因在于, 期望算子不能穿过非线性函数, 也即$\E[g(X)]\neq g(\E[X])$.

然而, 通过Probit模型得到拟合值, 并将其作为虚拟内生变量的工具变量, 然后再施行通常的2SLS方法是可行的, 特别是当非线性模型对一阶段的CEF有更好的近似时, 通过非线性拟合值得到的2SLS会比使用线性一阶段的传统2SLS更有效 (Angrist and Pischke, 2009; Newey, 1990).
\newpage
\section{生成回归元}
获取2SLS估计量的整个流程实际上是"生成回归元 (generated regressors)估计"的一个例子. 我们称回归元是被生成的 (generated), 如果它是理想 (idealized)回归元的估计量或者它是待估参数的函数.

通常, 一个生成回归元$\hat{W}$作为不可观测的理想回归元$W$的一个估计量, 那么$\hat{W}_i$应该是整个样本的函数, 而非仅是观测$i$的函数. 由此可见, $\hat{W}_i$不再是i.i.d.的, 因为$\hat{W}_i$的实现值依赖于不同的观测, 这使得经典的i.i.d.随机样本假设无效. 因此, 回归估计量的样本分布受到影响, 协方差矩阵和标准误的估计将不正确.

生成回归元的计量经济理论由Pagan (1984)提出. 这里我们关注以下线性模型
\begin{align}
Y&=W'\beta+v \label{eq6.22} \\
W&=\mathbold{A}'Z \nonumber \\
\E[Zv]&=0 \nonumber
\end{align}
其中可观测值是$[Y,Z]$, 并且我们有$\mathbold{A}$的估计量$\hat{\mathbold{A}}$. 此时我们可以构造$W_i$的生成回归元$\hat{W}_i=\hat{\mathbold{A}}'Z_i$, 然后在(\ref{eq6.22})中用$\hat{W}_i$取代$W_i$, 由此可以得到$\beta$的OLS估计量
\begin{equation}\label{eq6.23}
  \hb=\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\right)^{-1}\left(\sum_{i=1}^{n}\hat{W}_iY_i\right)
\end{equation}
由于生成回归元本身就是估计量, 因此这里$\hb$的统计性质和i.i.d.随机样本下的OLS估计量的不同.

现在回到简约式方程$Y=Z'\lambda+u_1$, 定义$W=\overbar{\Gamma}'Z$, $\mathbold{A}=\overbar{\Gamma}$以及$\hat{\mathbold{A}}=\hat{\Gamma}$, 由此可见生成回归元这一框架包含了2SLS估计量.

现在的目标是取得$\hb$的近似分布. 首先将(\ref{eq6.22})代入到(\ref{eq6.23})中得到
$$\hb=\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\right)^{-1}\left[\sum_{i=1}^{n}\hat{W}_i(W_i'\beta+v_i)\right]$$
注意到$W_i'\beta=\hat{W}_i'\beta+(W_i-\hat{W}_i)'\beta$, 于是有
\begin{equation}\label{eq6.24}
  \hb-\beta=\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\right)^{-1}\left\{\sum_{i=1}^{n}\hat{W}_i\left[(W_i-\hat{W}_i)'\beta+v_i\right]\right\}
\end{equation}
上式由两个随机项构成, 其中一项可由经典回归理论解决, 而另一项则是生成回归元的部分, 经典回归理论无法解决.

倘若可以使$(W_i-\hat{W}_i)'\beta$这部分消失, 那么(\ref{eq6.24})将会得到简化. 为了使它成立, 就需要生成回归元的回归系数为0, 由此需要将$W_i$进行分割来获得生成回归元那部分的回归系数.

具体而言, 做分割$W_i=[W_{1i}',W_{2i}']'$, $\hat{W}_i=[W_{1i}',\hat{W}_{2i}']'$, 其中$W_{1i}$是可观测的向量, $\hat{W}_{2i}$是生成回归元, 再将$\beta$分割为$\beta=[\beta_1',\beta_2']'$, 此时$(W_i-\hat{W}_i)'\beta=(W_{2i}-\hat{W}_{2i})'\beta_2$. 如果$\beta_2=0$成立, 那么
$$\hb-\beta=\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\right)^{-1}\left(\sum_{i=1}^{n}\hat{W}_iv_i\right)$$

进一步, 由于$\hat{W}_i=\hat{\A}'Z_i$, 故而我们可以将估计量写为样本矩的函数
$$\sqrt{n}(\hb-\beta)=\left[\hat{\A}'\left(n^{-1}\sum_{i=1}^{n}Z_iZ_i'\right)\hat{\A}\right]^{-1}\hat{\A}'\left(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Z_iv_i\right)$$
如果$\hat{\A}\xrightarrow{p}\A$, 那么按照标准流程可知$\sqrt{n}(\hb-\beta)\xrightarrow{d}N(0,\V_\beta)$, 其中
\begin{equation}\label{eq6.25}
  \V_\beta=(\A'\E[Z_iZ_i']\A)^{-1}(\A'\E[Z_iZ_i'v_i^2]\A)(\A'\E[Z_iZ_i']\A)^{-1}
\end{equation}
它的渐近协方差矩阵估计量为
$$\hat{\V}_\beta=n\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\right)^{-1}\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\hat{v}_i^2\right)\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\right)^{-1}$$
其中$\hat{v}_i=Y_i-\hat{W}_i'\hat{\beta}$, 在合适的正则条件下, $\hat{\V}_\beta\xrightarrow{p}\V_\beta$.

最后, 为了检验假设$\HH_0: \beta_2=0$, 我们可以构建Wald检验统计量
$$W=n\hb_2'[\hat{\V}_\beta]_{22}^{-1}\hb_2$$
当$n\to\infty$时有$W\xrightarrow{d}\chi^2_q$, 其中$q=\text{dim}\,(\beta_2)$.
\begin{theorem}\label{thm:thm6.2}
  对于模型(\ref{eq6.22}), 如果$\E[Y_i^4]<\infty$, $\E||Z_i||^4<\infty$, $\A'\E[Z_iZ_i']\A>0$, $\hat{\A}\xrightarrow{p}\A$, 以及$\hat{W}_i=(W_{1i}',\hat{W}_{2i}')'$ 那么在$\HH_0:\beta_2=0$的情况下, 当$n\to\infty$时有: (1) $\sqrt{n}(\hb-\beta)\xrightarrow{d}N(0,\V_\beta)$, 这里的$\V_\beta$在(\ref{eq6.25})中已经给出. (2) Wald检验统计量$W\xrightarrow{d}\chi^2_q$, 其中$q=\text{dim}\,(\beta_2)$.
\end{theorem}
尽管上述理论允许我们检验假设$\HH_0:\beta_2=0$, 但它并没有修正$\hb$的标准误, 如果我们需要在不施加简化条件$\HH_0:\beta_2=0$的情况下得到$\hb$的渐近分布, 还需要用到后续章节的广义矩估计方法.

现在考虑另外一种特殊情形, 估计量$\hat{\A}$可以写成与回归元$\X$相关的最小二乘形式$\hat{\A}=(\Z'\Z)^{-1}\Z'\X$, 这样的估计量与以下回归系统紧密相关
\begin{align}
  X&=\A'Z+u \label{eq6.26} \\
  \E[Zu']&=0 \nonumber
\end{align}
显然, 这类估计量$\hat{\A}$包含了2SLS的特殊情形\footnote{$\hat{\Gamma}=(\Z'\Z)^{-1}\Z'\X$能写成这种形式, 但这种形式的估计量$\hat{\A}$不一定来源于2SLS估计.}. 此时我们将生成回归元记作$\hat{W}=\Z\hat{\A}$, 首先由
$$\hat{\mathbold{W}}=\Z(\Z'\Z)^{-1}\Z'(\Z\A+\mathbold{U})$$
可知$\mathbold{W}-\hat{\mathbold{W}}=-\Z(\Z'\Z)^{-1}\Z'\mathbold{U}$, 于是
\begin{align*}
\hb-\beta&=(\hat{\mathbold{W}}'\hat{\mathbold{W}})^{-1}\{\hat{\mathbold{W}}'[(\mathbold{W}-\hat{\mathbold{W}})\beta+v]\} \\
&=(\hat{\A}'\Z'\Z\hat{\A})^{-1}\{\hat{\A}'\Z'[-\Z(\Z'\Z)^{-1}\Z'\mathbold{U}\beta+v]\} \\
&=(\hat{\A}'\Z'\Z\hat{\A})^{-1}[\hat{\A}'\Z'(-\mathbold{U}\beta+v)] \\
&=(\hat{\A}'\Z'\Z\hat{\A})^{-1}\hat{\A}'\Z'e
\end{align*}
其中
$$e_i=v_i-u_i'\beta=Y_i-X_i'\beta$$
在合适的正则条件下有$\sqrt{n}(\hb-\beta)\xrightarrow{d}N(0,\V_\beta)$, 其中
$$\V_\beta=(\A'\E[Z_iZ_i']\A)^{-1}(\A'\E[Z_iZ_i'e_i^2]\A)(\A'\E[Z_iZ_i']\A)^{-1}$$
特别地, 如果条件同方差假设$\E[Z_iZ_i'e_i^2]=\sigma^2\E[Z_iZ_i']$成立, 那么
$$\V_\beta^0=\sigma^2(\A'\E[Z_iZ_i']\A)^{-1}$$
其中$\E[e_i^2]=\sigma^2$. 协方差矩阵$\V_\beta$的估计量为
$$\hat{\V}_\beta=n(\hat{\mathbold{W}}'\hat{\mathbold{W}})^{-1}\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\hat{e}_i^2\right)(\hat{\mathbold{W}}'\hat{\mathbold{W}})^{-1}$$
其中$\hat{e}_i=Y_i-X_i'\hb$. 在合适的正则条件下, $\hat{\V}_\beta$是$\V_\beta$的一致估计量.

\begin{theorem}
  考虑模型(\ref{eq6.22})和(\ref{eq6.26}), 如果$\E[Y_i^4]<\infty$, $\E||Z_i||^4<\infty$, $\A'\E[Z_iZ_i']\A>0$, $\hat{\A}=(\Z'\Z)^{-1}\Z'\X$, 以及$\hat{\A}\xrightarrow{p}\A$, 那么当$n\to\infty$时有
  $\sqrt{n}(\hb-\beta)\xrightarrow{d}N(0,\V_\beta)$, 并且$\hat{\V}_\beta\xrightarrow{p}\V_\beta$.
\end{theorem}
总结一下, 生成回归元和两步估计会影响到估计量的样本分布和协方差矩阵. 一种简化条件是生成回归元的回归系数为0, 此时经典的样本分布和渐近理论均适用; 另一种重要情况是生成回归元可以写作最小二乘拟合值的形式, 此时估计量的渐近分布仍是经典的.
\section{含期望误差的回归}
本节在生成回归元的基础上做进一步扩展, 纳入了一个期望误差项, 具体而言就是
\begin{align}
Y&=W'\beta+u'\alpha+v \label{eq6.30} \\
W&=\mathbold{A}'Z \nonumber \\
X&=W+u \nonumber \\
\E[Zv]&=0 \nonumber \\
\E[uv]&=0 \nonumber \\
\E[Zu']&=0 \nonumber
\end{align}
其中$[Y,X,Z]$是可观测变量, $W$是$X$在$Z$上的投影, $u$为期望误差.

在上述模型中, 矩阵$\A$的估计量可以由$X$对$Z$回归得到, 也即$\hat{\A}=(\Z'\Z)^{-1}\Z'\X$, 于是可以计算得到拟合值$\hat{W}_i=\hat{\A}'Z_i$以及残差$\hat{u}_i=X_i-\hat{W}_i$, 最后使用$Y$对拟合值$\hat{W}$及残差$\hat{u}$回归即可估计. 此时
$$Y_i=\hat{W}_i'\beta+\hat{u}_i'\alpha+v_i,\quad i=1,2,\cdots,n$$
通过第一阶段回归可得$\Z'\hat{\mathbold{U}}=0$, $\hat{\mathbold{W}}'\hat{\mathbold{U}}=0$以及$\mathbold{W}'\hat{\mathbold{U}}=0$, 于是根据(\ref{eq2.15})和(\ref{eq2.16})可知
$$\hb=(\hat{\mathbold{W}}'\hat{\mathbold{W}})^{-1}\hat{\mathbold{W}}'Y$$
注意到
$$Y=\hat{\mathbold{W}}\beta+\mathbold{U}\alpha+(\mathbold{W}-\hat{\mathbold{W}})\beta+v$$
通过$\mathbold{W}-\hat{\mathbold{W}}=-\Z(\Z'\Z)^{-1}\Z'\mathbold{U}$可知
\begin{align*}
\hb-\beta&=(\hat{\mathbold{W}}'\hat{\mathbold{W}})^{-1}\hat{\mathbold{W}}'[\mathbold{U}\alpha+(\mathbold{W}-\hat{\mathbold{W}})\beta+v] \\
&=(\hat{\A}'\Z'\Z\hat{\A})^{-1}\hat{\A}'\Z'(\mathbold{U}\alpha-\mathbold{U}\beta+v) \\
&=(\hat{\A}'\Z'\Z\hat{\A})^{-1}\hat{\A}'\Z'e
\end{align*}
其中
$$e_i=v_i+u_i'(\alpha-\beta)=Y_i-X_i'\beta$$

另一方面
$$\hat{\alpha}=(\hat{\mathbold{U}}'\hat{\mathbold{U}})^{-1}\hat{\mathbold{U}}'Y$$
又因为$\hat{\mathbold{U}}$是出自回归$\X=\Z\A+\mathbold{U}$的残差, 因此
$$\mathbold{U}-\hat{\mathbold{U}}=\Z(\Z'\Z)^{-1}\Z'\mathbold{U}$$
于是由$\hat{\mathbold{U}}'\mathbold{W}=0$和$\hat{\mathbold{U}}'\Z=0$可知
\begin{align*}
\hat{\alpha}-\alpha&=(\hat{\mathbold{U}}'\hat{\mathbold{U}})^{-1}\hat{\mathbold{U}}'[\mathbold{W}\beta+(\mathbold{U}-\hat{\mathbold{U}})\alpha+v] \\
&=(\hat{\mathbold{U}}'\hat{\mathbold{U}})^{-1}\hat{\mathbold{U}}'v
\end{align*}
现在我们给出以下定理.
\begin{theorem}\label{thm:thm6.1}
  考虑模型(\ref{eq6.30}), 如果$\E[Y_i^4]<\infty$, $\E||Z_i||^4<\infty$, $\E||X_i||^4<\infty$, $\A\E[Z_iZ_i']\A'>0$, 以及$\E[u_iu_i']>0$, 那么当$n\to\infty$时
  \begin{equation*}\label{eq6.31}
    \sqrt{n}\begin{bmatrix}
              \hb-\beta \\
              \hat{\alpha}-\alpha
            \end{bmatrix}\xrightarrow{d}N(0,\V)
  \end{equation*}
  其中
  $$\V=\begin{bmatrix}
         \V_{\beta\beta} & \V_{\beta\alpha} \\
         \V_{\alpha\beta} & \V_{\alpha\alpha}
       \end{bmatrix}$$
  以及
  \begin{align*}
  \V_{\beta\beta}&=(\A'\E[Z_iZ_i']\A)^{-1}(\A'\E[Z_iZ_i'e_i^2]\A)(\A'\E[Z_iZ_i']\A)^{-1} \\
  \V_{\alpha\beta}&=\E[u_iu_i']^{-1}(\E[u_iZ_i'e_iv_i]\A)(\A'\E[Z_iZ_i']\A)^{-1} \\
  \V_{\alpha\alpha}&=\E[u_iu_i']^{-1}\E[u_iu_i'v_i^2]\E[u_iu_i']^{-1}
  \end{align*}
\end{theorem}
按照之前的流程, 容易写出估计量$\hat{\alpha}$和$\hat{\beta}$的协方差矩阵
\begin{align*}
\hat{\V}_{\alpha\alpha}&=n(\hat{\mathbold{W}}'\hat{\mathbold{W}})^{-1}\left(\sum_{i=1}^{n}\hat{W}_i\hat{W}_i'\hat{e}_i^2\right)(\hat{\mathbold{W}}'\hat{\mathbold{W}})^{-1} \\
\hat{\V}_{\beta\beta}&=n(\hat{\mathbold{U}}'\hat{\mathbold{U}})^{-1}\left(\sum_{i=1}^{n}\hat{U}_i\hat{U}_i'\hat{v}_i^2\right)(\hat{\mathbold{U}}'\hat{\mathbold{U}})^{-1}
\end{align*}
特别地, 如果同方差假设成立
$$\E\left[\left.\begin{pmatrix}
            e_i^2 & e_iv_i \\
            e_iv_i & v_i^2
          \end{pmatrix}\right|Z_i\right]=\mathbold{C}$$
那么渐近协方差矩阵可以简化为
\begin{align*}
\V_{\beta\beta}&=(\A'\E[Z_iZ_i']\A)^{-1}\E[e_i^2] \\
\V_{\alpha\alpha}&=\E[u_iu_i']^{-1}\E[v_i^2]
\end{align*}
\section{控制函数法}
本节介绍另外一种通过最小二乘来得到2SLS估计量的方法, 称为控制函数法 (Control Function Approach, CFA), 该方法在非线性回归领域尤其有用. 关于CFA的更多内容具体可见Wooldridge (2015).

首先写出以下结构式和简约式方程
\begin{align*}
Y&=X_1'\beta_1+X_2'\beta_2+e \\
X_2&=\Gamma_1'Z_1+\Gamma_2'Z_2+u_2
\end{align*}
其中$X_2$为内生变量, 通过$u_2$和$e$的相关性导致内生性, $Z$为符合定义\ref{def:def6.1}的工具变量. 现在考虑$e$在$u_2$上的线性投影
\begin{align*}
e&=u_2'\alpha+v \\
\alpha&=\E[u_2u_2']^{-1}\E[u_2e] \\
\E[u_2v]&=0
\end{align*}
将其代入到结构方程中得到
\begin{align}
Y&=X_1'\beta_1+X_2'\beta_2+u_2'\alpha+v \label{eq6.27} \\
\E[X_1v]&=0 \nonumber \\
\E[X_2v]&=0 \nonumber \\
\E[u_2v]&=0 \nonumber
\end{align}
注意, 这里$X_2$和$v$不相关, 这是因为$X_2$仅通过$u_2$和$e$产生相关性, 而$v$是$e$在$u_2$上正交投影后产生的误差.

如果$u_2$是可观测的, 那么可以使用最小二乘来估计方程(\ref{eq6.27}). 但由于它实际不可观测, 我们考虑使用简约式残差$\hat{u}_{2i}=X_{2i}-\hat{\Gamma}_1'Z_{1i}-\hat{\Gamma}_2'Z_{2i}$替代$u_{2i}$, 回归系数$[\beta_1,\beta_2,\alpha]$的估计量可由$Y$对$[X_1,X_2,\hat{u}_2])$的最小二乘获得. 此时
\begin{equation}\label{eq6.28}
  Y_i=X_i'\beta+\hat{u}_{2i}'\alpha+\hat{v}_i
\end{equation}
也可以用矩阵形式表示为
$$Y=\X\beta+\hat{\mathbold{U}}_2\alpha+v$$
下面将会看到这里的$\hb$在代数形式上和$\hb_{\text{2SLS}}$是一样的.

定义幂等矩阵
$$\mathbold{P}_Z=\Z(\Z'\Z)^{-1}\Z'$$
于是简约式残差可以记作
$$\hat{\mathbold{U}}_2=(\mathbold{I}_n-\mathbold{P}_Z)\X_2$$
根据FWL定理可知
\begin{equation}\label{eq6.29}
  \hb=(\tilde{\X}'\tilde{\X})^{-1}\tilde{\X}Y
\end{equation}
其中$\tilde{\X}=[\tilde{\X}_1,\tilde{\X}_2]$, 并且
$$\tilde{\X}_1=\X_1-\hat{\mathbold{U}}_2(\hat{\mathbold{U}}_2'\hat{\mathbold{U}}_2)^{-1}\hat{\mathbold{U}}_2'\X_1=\X_1$$
上式最后一个等号成立是因为$\hat{\mathbold{U}}_2'\X_1=0$. 另一方面
\begin{align*}
\tilde{\X}_2&=\X_2-\hat{\mathbold{U}}_2(\hat{\mathbold{U}}_2'\hat{\mathbold{U}}_2)^{-1}\hat{\mathbold{U}}_2'\X_2 \\
&=\X_2-\hat{\mathbold{U}}_2'[\X_2'(\mathbold{I}_n-\mathbold{P}_Z)\X_2]^{-1}\X_2'(\mathbold{I}_n-\mathbold{P}_Z)\X_2 \\
&=\X_2-\hat{\mathbold{U}}_2=\mathbold{P}_Z\X_2
\end{align*}
因此$\tilde{\X}=[\X_1,\mathbold{P}_Z\X_2]=\mathbold{P}_Z\X$, 将它代入到(\ref{eq6.29})中得到
$$\hb=(\X'\mathbold{P}_Z\X)^{-1}\X'\mathbold{P}_ZY=\hb_{\text{2SLS}}$$

然而, 对于非线性模型, 例如
\begin{align}
Y&=Z_1'\delta+\alpha_1X+\alpha_2X^2+e \nonumber \\
\E[e|Z]&=0 \label{eq6.35}
\end{align}
其中$Z=[Z_1',Z_2']'$是外生解释变量, $X$是内生解释变量, 并且我们有一个不在$Z_1$中的$Z_2$作为$X$的工具变量. 于是利用$[Z_1,Z_2,Z_2^2]$即可实施标准的IV估计, 并且它是一致的. 而如果考虑使用CF方法, 则需要做出比(\ref{eq6.35})更强的假设, 此时2SLS和CF得到的估计量不相同, 并且CF估计量通常更有效, 但也更缺乏稳健性.

现在我们来考虑CF估计量$[\hb,\hat{\alpha}]$的分布. 令$W=\overbar{\Gamma}'Z$, 以及$u=X-W$\footnote{这里$u$可以被分割为$u=\begin{bmatrix}
      0 \\
      u_2
    \end{bmatrix}$, 分块矩阵的维数分别为$K_1\times1$和$K_2\times1$.}, 于是回归方程(\ref{eq6.27})变为
\begin{equation}\label{eq6.32}
  Y=W'\beta+u_2'\gamma+v
\end{equation}
其中$\gamma=\alpha+\beta_2$. 作为生成回归元模型的一种, 根据定理\ref{thm:thm6.1}可知, 如果相关正则条件成立, 那么当$n\to\infty$时有
$$\sqrt{n}\begin{bmatrix}
            \hb_2-\beta_2 \\
            \hat{\gamma}-\gamma
          \end{bmatrix}\xrightarrow{d}N(0,\V)$$
其中
$$\V=\begin{bmatrix}
       \V_{22} & \V_{2\gamma} \\
       \V_{\gamma2} & \V_{\gamma\gamma}
     \end{bmatrix}$$
以及
\begin{align*}
\V_{22}&=\left[(\overbar{\Gamma}'\E[Z_iZ_i']\overbar{\Gamma})^{-1}\overbar{\Gamma}'\E[Z_iZ_i'e_i^2]\overbar{\Gamma}(\overbar{\Gamma}'\E[Z_iZ_i']\overbar{\Gamma})^{-1}\right]_{22} \\
\V_{\gamma 2}&=\left[\E[u_{2i}u_{2i}']^{-1}\E[u_iZ_ie_iv_i]\overbar{\Gamma}(\overbar{\Gamma}'\E[Z_iZ_i']\overbar{\Gamma})^{-1}\right]_{\gamma2} \\
\V_{\gamma\gamma}&=\E[u_{2i}u_{2i}']^{-1}\E[u_{2i}u_{2i}'v_i^2]\E[u_{2i}u_{2i}']^{-1} \\
e_i&=Y_i-X_i'\beta
\end{align*}
由此可以推断出$\hat{\alpha}=\hat{\gamma}-\hb_2$的渐近分布.
\begin{theorem}\label{thm:thm6.3}
  考虑模型(\ref{eq6.32}), 如果$\E[Y_i^4]<\infty$, $\E||Z_i||^4<\infty$, $\E||X_i||^4<\infty$, $\A\E[Z_iZ_i']\A'>0$, 以及$\E[u_iu_i']>0$, 那么当$n\to\infty$时
  $$\sqrt{n}(\hat{\alpha}-\alpha)\xrightarrow{d}N(0,\V_\alpha)$$
  其中
  $$\V_\alpha=\V_{22}+\V_{\gamma\gamma}-\V_{\gamma2}-\V_{2\gamma}'$$
\end{theorem}


\section{模型设定检验}
\subsection{内生性检验}
上文已经提到, 当线性回归模型$Y=X'\beta+e$存在内生性时, 我们可以使用2SLS估计得到一致估计量, 但模型是否存在内生性往往难以判断, 本节给出 Hausman (1978)提出的检验内生性的方法.
\begin{theorem}\label{thm:thm6.5}
  在假设\ref{pro:pro6.2}和同方差假设$\E[e_i^2|X_i,Z_i]=\sigma^2$下, 如果原假设$\HH_0: \E[e_i|X_i]=0$成立, 那么当$n\to\infty$时有
  $$H=\frac{n(\hb_{\text{2SLS}}-\hb)'[(\hat{\Q}_{XZ}\hat{\Q}_{ZZ}^{-1}\hat{\Q}_{ZX})^{-1}-\hat{\Q}_{XX}^{-1}]^{-}(\hb_{\text{2SLS}}-\hb)}{s^2}\xrightarrow{d}\chi^2_K$$
  其中$s^2=\hat{e}'\hat{e}/n$, $\hat{e}=Y-\X\hb$为OLS估计残差, $-$表示广义逆.
\end{theorem}
\begin{proof}
  首先回顾OLS估计量
  $$\sqrt{n}(\hb-\beta)=\hat{\Q}_{XX}^{-1}n^{-\frac{1}{2}}\sum_{i=1}^{n}X_ie_i$$
  根据CLT可知
  $$n^{-\frac{1}{2}}\sum_{i=1}^{n}X_ie_i\xrightarrow{d}N(0,\sigma^2\Q_{XX}^{-1})$$
  其中$\E[e_i^2]=\sigma^2$. 注意到$n^{-\frac{1}{2}}\sum_{i=1}^{n}X_ie_i=\text{O}_p(1)$以及$\hat{\Q}_{XX}^{-1}\xrightarrow{p}\Q_{XX}^{-1}$, 于是
  $$\sqrt{n}(\hb-\beta)=\Q_{XX}^{-1}n^{-\frac{1}{2}}\sum_{i=1}^{n}X_ie_i+\text{o}_p(1)$$
  同理可知
  \begin{align*}
  \sqrt{n}(\hb_{\text{2SLS}}-\beta)&=\hat{\A}n^{-\frac{1}{2}}\sum_{i=1}^{n}X_ie_i \\
  &=\A n^{-\frac{1}{2}}\sum_{i=1}^{n}X_ie_i+\text{o}_p(1)
  \end{align*}
  其中
  $$\hat{\A}=(\hat{\Q}_{XZ}\hat{\Q}_{ZZ}^{-1}\hat{\Q}_{ZX})^{-1}\hat{\Q}_{XZ}\hat{\Q}_{ZZ}\xrightarrow{p}\A=(\Q_{XZ}\Q_{ZZ}^{-1}\Q_{ZX})^{-1}\Q_{XZ}\Q_{ZZ}^{-1}$$
  并且$n^{-\frac{1}{2}}\sum_{i=1}^{n}X_ie_i\xrightarrow{d}N(0,\sigma^2\Q_{ZZ})$.

  如果原假设$\HH_0:\E[e_i|X_i]=0$成立, 那么当$n\to\infty$时有
  \begin{align*}
  \sqrt{n}(\hb_{\text{2SLS}}-\hb)&=\sqrt{n}(\hb_{\text{2SLS}}-\beta)-\sqrt{n}(\hb-\beta) \\
  &=n^{-\frac{1}{2}}\sum_{i=1}^{n}[(\Q_{XZ}\Q_{ZZ}^{-1}\Q_{ZX})^{-1}\Q_{XZ}\Q_{ZZ}^{-1}Z_i-\Q_{XX}^{-1}X_i]e_i+\text{o}_p(1) \\
  &\xrightarrow{d}N[0,\sigma^2(\Q_{XZ}\Q_{ZZ}^{-1}\Q_{ZX})^{-1}-\sigma^2\Q_{XX}^{-1}]
  \end{align*}
  于是根据Slutsky定理可知$H\xrightarrow{d}\chi_K^2$.
\end{proof}
\begin{remark}
尽管Hausman检验要求同方差假设成立, 但是构造一个异方差稳健的Hausman检验统计量也是可能的, 只是表达式会异常复杂.
\end{remark}
另一方面, 我们还可以通过CF法来检验内生性. 考虑回归模型
$$Y=X_1'\beta_1+X_2'\beta_2+e$$
我们想检验原假设$\HH_0: \E[X_2e]=0$, 它等价于$\HH_0:\E[u_2e]=0$. 根据CF法可建立以下模型
$$Y_i=X_i'\beta+\hat{u}_{2i}\alpha+v_i$$
又因为
$$\alpha=\E[u_2u_2']^{-1}\E[u_2e]$$
根据定理\ref{thm:thm6.3}, 我们可以构造一个Wald检验统计量
$$W=n\hat{\alpha}'[\hat{\V}_{\alpha}]^{-1}\hat{\alpha}$$
以检验假设$\HH_0:\alpha=0$, 并且当$n\to\infty$时有$W\xrightarrow{d}\chi^2_{K_2}$.

最后我们强调一点, 这里的内生性检验本质上检验的是OLS估计量和2SLS估计量 (或CF估计量)在统计意义上是否相距很远. 换言之, Hausman检验的原理是, 如果模型不存在内生性, 那么这些估计量在数值上应该是差不多的.


\subsection{过度识别检验}
除了检验内生性外, 我们还需要检验工具变量的外生性$\E[Ze]=0$. 由于在恰好识别的情况下无法检验, 因此我们将重点放在过度识别检验上. 在同方差假设成立的情况下, 我们可以构造Sargan (1958)提出的检验统计量进行检验.
\begin{theorem}\label{thm:thm6.4}
  在假设\ref{pro:pro6.2}和同方差$\E[e_i^2|Z_i]=\sigma^2$下, 如果原假设$\HH_0:\E[Z_i|e_i]=0$成立, 那么当$n\to\infty$时有
  $$S=\frac{\hat{e}'\Z(\Z'\Z)^{-1}\Z'\hat{e}}{\hat{e}'\hat{e}/n}\xrightarrow{d}\chi^2_{L-K}$$
  其中$\hat{e}=Y-\X\hb_{\text{2SLS}}$.
\end{theorem}
\begin{proof}
  首先根据CLT可知
  $$n^{-\frac{1}{2}}\sum_{i=1}^{n}Z_ie_i\xrightarrow{d}N(0,\BO)$$
  其中$\BO=\E[Z_iZ_i'e_i^2]=\sigma^2\E[Z_iZ_i']$, 以及$\sigma^2=\E[e_i^2]$. 定义$\BO$的一致估计量
  $$\hat{\BO}=n^{-2}\hat{e}'\hat{e}\Z'\Z$$
  从而$\hat{\BO}^{-1}=n^2(\Z'\Z)^{-1}/\hat{e}'\hat{e}$. 根据线性代数可知, 存在矩阵$\hat{\BO}^{-\frac{1}{2}}$, 使得$\hat{\BO}^ {-1}=\hat{\BO}^{-\frac{1}{2}}\hat{\BO}^{-\frac{1}{2}}$, 故而
  $$n^{-\frac{1}{2}}\hat{\BO}^{-\frac{1}{2}}\sum_{i=1}^{n}Z_ie_i\xrightarrow{d}N(0,\mathbold{I}_l)$$
  根据引理\ref{lem:lem2.1}可知
  $$n^{-1}\hat{e}'\Z\hat{\BO}^{-1}\Z'\hat{e}\xrightarrow{d}\chi^2_{L-K}$$
  也即$S\xrightarrow{d}\chi^2_{L-K}$.
\end{proof}
当样本容量不大时, Sargan检验通常会过于拒绝原假设. 此外, Sargan检验要求条件同方差假设成立, 如果该假设不成立, 那么我们需要使用GMM过度识别检验, Wooldridge (1995)还介绍了一种在过度识别下的稳健得分检验, 它在数值上同GMM过度识别检验统计量相等.

注意, 即便过度识别检验没有拒绝原假设, 也不能说明所有工具变量都是外生的, 只能说明没有找到它们是内生的证据.
\subsection{弱工具变量}
最后我们关注弱工具变量 (weak instruments)问题. 如果工具变量$Z$和内生变量$X$完全不相关, 则无法识别结构参数. 如果二者仅微弱地相关, 那么$\E[ZX']^{-1}$将会很大, 从而影响2SLS估计量的渐近方差, 以及导致估计量很不准确并足以使得一切都不显著.

为了简化问题, 我们假设模型中没有外生变量, 此时将$X_2$, $Z_2$和$\beta_2$分别简写为$X$, $Z$和$\beta$, 模型为
\begin{align*}
Y&=X'\beta+e \\
X&=\Gamma'Z+u_2
\end{align*}
记$u=[u_1,u_2']'$, 协方差矩阵为
$$\E[u_iu_i']=\BS=\begin{bmatrix}
                    \BS_{11} & \BS_{12} \\
                    \BS_{21} & \BS_{22}
                  \end{bmatrix}$$
结构误差$e=u_1-u_2'\beta=u'\gamma$, 其中$\gamma=[1,-\beta']'$. 于是可以定义$\E[e_i^2|Z_i]=\gamma'\BS\gamma$, 以及$\BS_{2\Sigma}=\E[u_2e|Z]=\Sigma_{21}-\Sigma_{22}\beta$.

显然, 当$\Gamma=0$时, 模型完全不可识别, 现在我们想知道$\Gamma$很小时会如何. 按照 Staiger and Stock (1997)介绍的方法, 假定
\begin{equation}\label{eq6.33}
  \Gamma=n^{-\frac{1}{2}}\mathbold{C}
\end{equation}
这里的$\mathbold{C}$是一个固定的矩阵. $||\mathbold{C}||$越大表明识别越强, 反之则越弱, (\ref{eq6.33})称为局部趋零 (local-to-zero)假设.

对于2SLS估计量, 首先由CLT得到
$$n^{-\frac{1}{2}}\sum_{i=1}^{n}Z_iu_i'\xrightarrow{d}\xi=[\xi_1,\xi_2]$$
这里$\text{vec}(\xi)\sim N(0,\E[u_iu_i'\otimes Z_iZ_i'])$. 上式同时意味着
$$\frac{1}{\sqrt{n}}\Z'e\xrightarrow{d}\xi_e=\xi\gamma$$
根据Slutsky定理可知
\begin{align*}
\frac{1}{\sqrt{n}}\Z'\X=\frac{1}{n}\Z'\Z\mathbold{C}+\frac{1}{\sqrt{n}}\Z'\mathbold{U}_2\xrightarrow{d}\Q_{ZZ}\mathbold{C}+\xi_2
\end{align*}
以及
\begin{align*}
\X'\mathbold{P}_Z\X&=\left(\frac{1}{\sqrt{n}}\X'\Z\right)\left(\frac{1}{n}\Z'\Z\right)^{-1}\left(\frac{1}{\sqrt{n}}\Z'\X\right)\\
&\xrightarrow{d}(\Q_{ZZ}\mathbold{C}+\xi_2)'\Q_{ZZ}^{-1}(\Q_{ZZ}\mathbold{C}+\xi_2)
\end{align*}
还有
$$\X'\mathbold{P}_Ze=\left(\frac{1}{\sqrt{n}}\X'\Z\right)\left(\frac{1}{n}\Z'\Z\right)^{-1}\left(\frac{1}{\sqrt{n}}\Z'e\right)\xrightarrow{d}(\Q_{ZZ}\mathbold{C}+\xi_2)'\Q_{ZZ}^{-1}\xi_e$$
最后得到
\begin{align*}
\hb_{\text{2SLS}}-\beta&=(\X'\mathbold{P}_Z\X)^{-1}\X'\mathbold{P}_Ze \\
&\xrightarrow{d}[(\Q_{ZZ}\mathbold{C}+\xi_2)'\Q_{ZZ}^{-1}(\Q_{ZZ}\mathbold{C}+\xi_2)]^{-1}(\Q_{ZZ}\mathbold{C}+\xi_2)'\Q_{ZZ}^{-1}\xi_e
\end{align*}
由于固定矩阵$\mathbold{C}$的存在, 此时2SLS估计量$\hb_{\text{2SLS}}$不是一致的, 并且渐近分布也是非正态的. 因此, 即使拥有相当大的样本量, 使用弱工具变量也难以获得含有较小偏误的估计量.

关于弱工具变量的检验, 参考Staiger and Stock (1997), Stock and Yogo (2005), 以及Kleibergen and Paap (2006). 目前关于弱工具变量的许多开放性问题, 例如过度识别模型在异方差下的稳健性推断程序尚未达成一致.

\chapter{广义矩方法}
目前在应用计量经济学领域内最流行的估计方法之一就是Hansen (1982)建立的的广义矩方法 (Generalized Method of Moments, GMM), 它提供了一个统一的计量经济学分析框架, 许多估计量都可以视作是GMM框架下的一个特例, 包括之前的OLS估计量和2SLS估计量, 甚至ML估计量也是GMM估计量的特例.
\section{矩方程与GMM估计量}
假设$\theta \in \Theta$是$P\times1$维向量, 这里$\Theta$为$P\times1$维参数空间. 再定义$W_i$为一个$d\times1$维随机向量, 它的支集为$\mathscr{W}$, 并且其分布的某一特征可以由$\theta$刻画, 并且存在一个$L\times1$维矩函数$g(W_i,\theta)$和参数$\theta_o\in\Theta$, 使得矩方程
\begin{equation}\label{eq7.1}
  \E[g(W_i,\theta_o)]=0,\quad i=1,2,\cdots,n
\end{equation}
成立, 下标$o$表示该参数是$\Theta$内的某个我们感兴趣的真值. 在IV模型中, $g(W_i,\theta)=Z_i(Y_i-X_i'\theta)$, 这里$W_i=[Y_i,Z_i',X_i']'$.

总体而言, 我们称$\theta_o$是可识别的, 如果存在一个从数据分布到$\theta_o$的唯一映射, 故而在矩方程(\ref{eq7.1})的背景下, 这意味着满足它的$\theta_o$是唯一的. 由于矩方程(\ref{eq7.1})是一个由$L$个方程组成的含$P$个未知量的系统, 沿用上一章的说法, 我们称$L=P$时模型恰好识别, $L>P$时模型过度识别, $L<P$时模型不可识别.

在恰好识别的情况下, 可以通过(\ref{eq7.1})直接解出$\theta$. 定义样本矩为
$$\hat{g}(\theta)=n^{-1}\sum_{i=1}^{n}g(W_i,\theta)$$
再定义矩方法(Method of Moments, MM)估计量 为使得$\hat{g}(\theta)=0$的那个参数, 例如在IV模型中, MM估计量为
$$\hat{\theta}_{{\text{MM}}}=(\Z'\X)^{-1}\Z'Y$$

然而在过度识别的情况下, 由于方程数大于未知数的个数, 所以$\theta$无解, 此时无法通过矩估计直接解出$\theta$. 但是从这个角度出发, 我们期望$\hat{g}(\theta)$尽可能小, 由此找到一个合适的估计量. 例如, 通过选择某个$\hat{\theta}$来最小化二次型
$$\hat{g}(\theta)'\hat{g}(\theta)=||\hat{g}(\theta)||^2$$
然而考虑到$\hat{g}(\theta)$内的元素可能存在相关性, 因此引入一个权重矩阵$\hat{\W}$, 并选择$\hat{\theta}$以最小化二次型
\begin{align*}
J_n(\theta)&=\hat{g}(\theta)'\hat{\W}\hat{g}(\theta) \\
&=\left[\sum_{i=1}^{n}g(W_i,\theta)\right]'\hat{\W}\left[\sum_{i=1}^{n}g(W_i,\theta)\right]
\end{align*}
显然, 当$\hat{\W}=\mathbold{I}_L$时, 赋予这$L$个样本矩的权重是相同的.

\begin{definition}\label{def:def7.1}
广义矩方法估计量为
\begin{align*}\hat{\theta}_{\text{GMM}}=\arg\min_{\theta\in \Theta}\, J_n(\theta) \end{align*}
其中$\hat{\W}$是正定的.
\end{definition}
 在恰好识别的情况下, GMM估计量还原为不依赖于$\hat{\W}$的矩估计量. 而在过度识别的情况下, 权重矩阵$\hat{\W}$一般会影响到估计的精度, 一个性质良好的$\hat{\W}$要对方差较大的样本矩赋予小的权重, 并且消除各样本矩之间的相关性. 如果权重矩阵$\hat{\W}$是固定的, 则通常称$\hat{\theta}_{\text{GMM}}$为一步GMM估计量 (one-step GMM estimator).


通常而言, 如果矩函数$g(W_i,\theta)$是总体参数$\theta$的非线性函数, 那么通常无法获得$\hat{\theta}$的显式解, 但线性工具变量估计是一个例外. 考虑矩函数
$$g(W_i,\theta)=Z_i(Y_i-X_i'\theta)$$
以及对应的矩条件
$$\E[Z_i(Y_i-X_i'\theta)]=0$$
其中$Y_i$是标量, $X_i$和$\theta$是$K\times1$维向量, $Z_i$是$L\times1$维向量, 并且$L\geq K$. 此时
$$\hat{g}(\theta)=n^{-1}\sum_{i=1}^{n}Z_i(Y_i-X_i'\theta)=\frac{\Z'(Y-\X\theta)}{n}$$
从而$J_n(\theta)=n^{-2}(Y-\X\theta)'\Z\hat{\W}\Z'(Y-\X\theta)$, 于是可以找到FOC
$$\frac{\partial}{\partial\theta}J_n(\theta)=-2\X'\Z\hat{\W}\Z'(Y-\X\theta)=0$$
如果$\Z'\X$满秩, 则可以找到GMM估计量
$$\hat{\theta}_{\text{GMM}}=(\X'\Z\hat{\W}\Z'\X)^{-1}\X'\Z\hat{\W}\Z'Y$$
特别地, 如果模型恰好识别且$\X'\Z$满秩, 那么
\begin{align*}
\hat{\theta}_{\text{GMM}}&=(\Z'\X)^{-1}\hat{\W}^{-1}(\X'\Z)^{-1}\X'\Z\hat{\W}\Z'Y\\
&=(\Z'\X)^{-1}\Z'Y=\hat{\theta}_{\text{IV}}
\end{align*}
而在恰好识别的情况下, 如果将权重矩阵选为$\hat{\W}=c(\Z'\Z)^{-1}$, 其中$c\neq0$, 那么$\hat{\theta}_{\text{GMM}}=\hat{\theta}_{\text{2SLS}}$. 由此我们看到, 2SLS估计量是线性IV估计量的一个特例, 并且后面将会证明, 在条件同方差的假设下, 2SLS估计量在线性IV估计量类中是渐近有效的.
\section{GMM的渐近性质}
\subsection{一致性}
为了研究GMM估计量的渐近性质, 我们首先要给出新的定义和一些正则条件.
\begin{definition}
设$\{g_n(\theta)\}$为随机向量构成的非负序列, 如果当$n\to\infty$时有
$$\sup_{\theta\in\Theta}||g_n(\theta)-g(\theta)||=\text{o}_p(1)$$
则称$g_n(\theta)$在$\theta\in\Theta$中依概率一致收敛于$g(\theta)$.
\end{definition}
\begin{proposition}\label{pro:pro7.1}
(1) $\{g(W_i,\theta)\}_{i=1}^n$是可观测的i.i.d.随机样本.

(2) $P\times1$维参数空间$\Theta$是紧集.

(3) 对于任意给定的$\theta\in\Theta$, 矩函数$g(\cdot,\theta)$是Borel可测的, 且对于任意的$w\in\mathscr{W}$, $g(w,\cdot)$在$\Theta$上为连续函数.

(4) 样本矩$\hat{g}(\theta)$在$\Theta$上依概率一致收敛于$g(\theta)=\E[g(W_i,\theta)]$, 也即当$n\to\infty$时有
$$\sup_{\theta\in\Theta}\left|\left|\hat{g}(\theta)-g(\theta)\right|\right|\xrightarrow{p}0$$

(5) $\theta$在$\Theta$上可识别, 也即存在唯一的$\theta_o\in\Theta$使得$\E[g(W_i,\theta_o)]=0$.

(6) $\E\left[\sup_{\theta\in\Theta}||g(W_i,\theta)||\right]<\infty$, 又称占优条件.

(7) $\hat{\W}\xrightarrow{p}\W$, 这里$\W$是$L\times L$维非随机的对称、有限且非奇异的矩阵.
\end{proposition}
集合的紧性是一个拓扑概念\footnote{设$X$为一个拓扑空间, 并且$K\subset X$, 如果$K$的每个开覆盖都有有限子覆盖, 那么$K$为紧集.}, 它可以极大简化渐近分析. 根据Heine-Borel定理\footnote{有限维$\R^P$空间的有界闭子集等价于紧集.}, 我们可以将紧参数空间$\Theta$定义为$\R^P$空间里的某个非常大的有界闭集, 这样做并不会令人担心. 然而以上假设的第(5)点和第(6)点通常极难满足, 一般都直接假设其成立.

为了使得一致收敛的假设成立, 我们需要合适的一致弱大数定律 (Uniform Weak Law of Large Numbers, UWLLN).
\begin{lemma}[一致弱大数定律]\label{lem:lem7.1}
  设$\{W_i\}_{i=1}^n$是由定义在$\mathscr{W}\subset\R^d$上的i.i.d.随机向量构成的序列, $\Theta$是$\R^P$的紧子集, $q:\mathscr{W}\times\Theta\to\R$为一个实值函数. 如果以下条件成立: (1) 对于任意$\theta\in\Theta$, $q(\cdot,\theta)$是Borel可测的; (2) 对于任意$w\in\mathscr{W}$, $q(w,\cdot)$在$\Theta$上是连续的; (3) 对于一切$\theta\in\Theta$, 存在Borel可测函数$D:\R^+\to\R^+$, 使得对于一切$w\in\mathscr{W}$和$\theta\in\Theta$都有$|q(w,\theta)|\leq |D(w)|$及$\E[D(W_i)]<\infty$. 那么

  (1) $Q(\theta)=\E[q(W_i,\theta)]$在$\Theta$上连续.

  (2) 当$n\to\infty$时有$\sup_{\theta\in\Theta}|\hat{Q}(\theta)-Q(\theta)|\xrightarrow{p}0$, 其中$\hat{Q}(\theta)=n^{-1}\sum_{i=1}^{n}q(W_i,\theta)$.
\end{lemma}
\begin{proof}
  见Jennrich (1969).
\end{proof}
\begin{remark}
由于$|g(w_i,\theta)|\leq \sup_{\theta\in\Theta}|g(w_i,\theta)|$, 故而可以直接将$D(W_i)$替换为$\sup_{\theta\in\Theta}|g(W_i,\theta)|$.
\end{remark}

\begin{lemma}[极值估计量的一致性]\label{lem:lem7.2}
设$\Theta$是$\R^P$的紧子集, $\hat{Q}:\Theta\to\R$是随机实值函数, $Q: \Theta\to\R$是非随机实值连续函数. 假定对于一切$\theta\in\Theta$, $\hat{Q}(\cdot)$是Borel可测函数, $\hat{Q}(\cdot)$在$\Theta$上是连续的, 并且$\sup_{\theta\in\Theta}|\hat{Q}(\theta)-Q(\theta)|\xrightarrow{p}0$. 令$\hat{\theta}=\arg\max_{\theta\in\Theta}\,\hat{Q}(\theta)$, 如果$Q(\theta)$只在唯一的$\theta_o\in\Theta$处取得在$\Theta$上的最大值, 那么$\hat{\theta}\xrightarrow{p}\theta_o$.
\end{lemma}
\begin{proof}
设$\mathscr{N}\subset\R^P$是一个包含$\theta_o$的开集, 由于它的补集$\mathscr{N}^c$是闭的且$\Theta$是紧的, 故而$\mathscr{N}^c\cap \Theta$也是紧的. 又因为$Q:\Theta\to\R$是连续函数, 因此由最值定理可以证得$\max_{\theta\in \mathscr{N}^c\cap\Theta}\,Q(\theta)$的存在性. 令
\begin{equation}\label{eq7.5}
  \varepsilon=Q(\theta_o)-\max_{\theta\in \mathscr{N}^c\cap\Theta}\,Q(\theta)
\end{equation}
由于$\mathscr{N}$可以选取得任意小, 所以$\varepsilon>0$也可以任意小.

定义事件$A_n$为: 对于一切$\theta\in\Theta$都有$|\hat{Q}(\theta)-Q(\theta)|<\varepsilon/2$, 于是
\begin{equation}\label{eq7.2}
  A_n\Rightarrow Q(\hat{\theta})>\hat{Q}(\hat{\theta})-\varepsilon/2
\end{equation}
以及
\begin{equation}\label{eq7.3}
  A_n\Rightarrow \hat{Q}(\theta_o)>Q(\theta_o)-\varepsilon/2
\end{equation}
根据定义有$\hat{Q}(\hat{\theta})=\max_{\theta\in\Theta}\,Q(\theta)$, 故而$\hat{Q}(\hat{\theta})\geq\hat{Q}(\theta_o)$, 从(\ref{eq7.2})可以推知
\begin{equation}\label{eq7.4}
  A_n\Rightarrow Q(\hat{\theta})>\hat{Q}(\theta_o)-\varepsilon/2
\end{equation}
将(\ref{eq7.3})和(\ref{eq7.4})相加得到
\begin{equation}\label{eq7.6}
  A_n\Rightarrow Q(\hat{\theta})>Q(\theta_o)-\varepsilon
\end{equation}
从(\ref{eq7.5})和(\ref{eq7.6})可得
$$A_n\Rightarrow\max_{\theta\in \mathscr{N}^c\cap\Theta}\,Q(\theta)<Q(\hat{\theta})$$
也即$A_n\Rightarrow \hat{\theta}\in \mathscr{N}$, 这同时意味着$\PP[A_n]\leq \PP[\hat{\theta}\in \mathscr{N}]$, 根据$\sup_ {\theta\in\Theta}|\hat{Q}(\theta)-Q(\theta)|\xrightarrow{p}0$可知
$$\limn \PP[A_n]=1$$
因此$\displaystyle\limn \PP[\hat{\theta}\in\mathscr{N}]=1$, 也即$\hat{\theta}=\theta_o+\text{o}_p(1)$.
\end{proof}

\begin{theorem}\label{thm:thm7.2}
  在假设\ref{pro:pro7.1}下, 当$n\to\infty$时有$\hat{\theta}\xrightarrow{p}\theta_o$.
\end{theorem}
\begin{proof}
  首先设
  \begin{align*}
  \hat{Q}(\theta)&=-\hat{g}(\theta)'\hat{\W}\hat{g}(\theta) \\
  Q(\theta)&=-g(\theta)'\W g(\theta)
  \end{align*}
  根据三角不等式可知
  \begin{align*}
  |\hat{Q}(\theta)-Q(\theta)|&=|\hat{g}(\theta)'\hat{\W}\hat{g}(\theta)-g(\theta)'\W g(\theta)| \\
  &=|[\hat{g}(\theta)-g(\theta)+g(\theta)]'\hat{\W}[\hat{g}(\theta)-g(\theta)+g(\theta)]-g(\theta)'\W g(\theta)| \\
  &\leq |[\hat{g}(\theta)-g(\theta)]'\hat{\W}[\hat{g}(\theta)-g(\theta)]| \\
  &\quad +2|g(\theta)'\hat{\W}[\hat{g}(\theta)-g(\theta)]|+|g(\theta)'(\hat{\W}-\W)g(\theta)|
  \end{align*}
  假设\ref{pro:pro7.1}中的占优条件保证了UWLLN的条件(3)成立, 于是当$n\to\infty$时有
  $$\sup_{\theta\in\Theta}|\hat{Q}(\theta)-Q(\theta)|\xrightarrow{p}0$$
  再由Cholesky分解可知存在矩阵$\mathbold{C}$使得$\W=\mathbold{C}'\mathbold{C}$, 如果$\theta\neq\theta_o$, 那么由识别条件可知
  $$0\neq \W g(\theta)=\mathbold{C}'\mathbold{C}g(\theta)$$
  这同时意味着$\mathbold{C}g(\theta)\neq0$, 从而
  $$Q(\theta)=-[\mathbold{C}g(\theta)]'[\mathbold{C}g(\theta)]<Q(\theta_o)=0$$
  于是$\theta_o$是$\Theta$上唯一使得$Q(\theta)$最大化的参数. 又因为$Q(\theta)$是连续的, 根据引理\ref{lem:lem7.2}即可推知$\hat{\theta}\xrightarrow{p}\theta_o$.
\end{proof}

\subsection{渐近正态性}
和之前类似, 我们还需要在假设\ref{pro:pro7.1}上做出增加额外条件才能得出GMM估计量的渐近正态性.
\begin{proposition}\label{pro:pro7.2}
在假设\ref{pro:pro7.1}的基础上, 以下额外条件成立:

(1) $\theta_o\in\text{int}\,(\Theta)$.

(2) 对于一切$w_i\in \mathscr{W}\subset\R^d$, $g(w_i,\cdot)$在包含$\theta_o$的一个邻域$\mathscr{N}$内连续可微.

(3) $\E[\sup_{\theta\in \mathscr{N}}||\nabla_\theta g(W_i,\theta)||]<\infty$.

(4) $\E[\nabla_\theta g(W_i,\theta_o)]$为$L\times P$维满秩矩阵.

(5) $n^{-\frac{1}{2}}\sum_{i=1}^{n}g(W_i,\theta_o)\xrightarrow{d} N(0,\BO_o)$.
\end{proposition}
\begin{theorem}\label{thm:thm7.3}
  在假设\ref{pro:pro7.2}下, 当$n\to\infty$时有
  $$\sqrt{n}(\hat{\theta}-\theta_o)\xrightarrow{d}N[0,(\mathbold{G}_o'\W\mathbold{G}_o)^{-1}\mathbold{G}_o'\W\BO_o\W\mathbold{G}_o(\mathbold{G}_o'\W\mathbold{G}_o)^{-1}]$$
  其中$\mathbold{G}_o=\E[\nabla_\theta g(W_i,\theta_o)]$, 并且$\BO_o=\E[g(W_i,\theta_o)g(W_i,\theta_o)']$.
\end{theorem}
\begin{proof}
  因为$\theta_o\in\text{int}(\Theta)$, 且当$n\to\infty$时有$\hat{\theta}\xrightarrow{p}\theta_o$, 故而此时$\hat{\theta}$在$\Theta$的内部的概率接近于1. 当$n$充分大时, 最大化$\hat{Q}(\theta)=-\hat{g}(\theta)'\hat{\W}\hat{g}(\theta)$的FOC为
  $$\left[\sum_{i=1}^{n}\nabla_\theta g(W_i,\hat{\theta})\right]'\hat{\W}\left[\sum_{i=1}^{n} g(W_i,\hat{\theta})\right]=0$$
  等价地有
  $$[\nabla_\theta \hat{g}(\hat{\theta})]'\hat{\W}\sqrt{n}\hat{g}(\hat{\theta})=0$$

  定义$L\times P$维矩阵
  $$G(\theta)=\E[\nabla_\theta g(W_i,\theta)]$$
  利用三角不等式可知
  \begin{align*}
  ||\nabla_\theta \hat{g}(\hat{\theta})-\mathbold{G}_o||&=||\nabla_\theta\hat{g}(\hat{\theta})-G(\hat{\theta})+G(\hat{\theta})-\mathbold{G}_o|| \\
  &\leq ||\nabla_\theta \hat{g}(\hat{\theta})-G(\hat{\theta})||+||G(\hat{\theta})-\mathbold{G}_o|| \\
  &\leq \sup_{\theta\in\Theta}||\nabla_\theta \hat{g}(\hat{\theta})-G(\hat{\theta})||+||G(\hat{\theta})-\mathbold{G}_o|| \xrightarrow{p}0
  \end{align*}
  其中UWLLN和$\hat{\theta}\xrightarrow{p}\theta_o$分别保证了上式最后一行的两项趋于0. 再根据假设\ref{pro:pro7.2}(5)可得
  \begin{equation}\label{eq7.7}
    \sqrt{n}\hat{g}(\theta_o)=n^{-\frac{1}{2}}\sum_{i=1}^{n}g(W_i,\theta_o)\xrightarrow{d} N(0,\BO_o)
  \end{equation}
  其中$\BO_o=\E[g(W_i,\theta_o)g(W_i,\theta_o)']$. 现在将样本矩函数$\hat{g}(\hat{\theta})$在$\theta_o$处进行一阶Taylor展开得
  \begin{equation}\label{eq7.10}
    \sqrt{n}\hat{g}(\hat{\theta})=\sqrt{n}\hat{g}(\theta_o)+[\nabla_\theta\hat{g}(\overbar{\theta})]\sqrt{n}(\hat{\theta}-\theta_o)
  \end{equation}
  其中$\overbar{\theta}$在$\hat{\theta}$和$\theta_o$之间, 根据$\hat{\theta}\xrightarrow{p}\theta_o$可知$\text{plim}\,\overbar{\theta}=\theta_o$, 于是同理可得
  $$\nabla_\theta \hat{g}(\overbar{\theta})\xrightarrow{p} \mathbold{G}_o$$
  现在将条件(\ref{eq7.10})代入FOC可知
  $$[\nabla_\theta \hat{g}(\hat{\theta})]'\hat{\W}[\sqrt{n}\hat{g}(\theta_o)+[\nabla_\theta\hat{g}(\overbar{\theta})]\sqrt{n}(\hat{\theta}-\theta_o)]=0$$
  也即
  $$\sqrt{n}(\hat{\theta}-\theta_o)=-\{[\nabla_\theta \hat{g}(\hat{\theta})]'\hat{\W}[\nabla_\theta\hat{g}(\overbar{\theta})]\}^{-1}[\nabla_\theta \hat{g}(\hat{\theta})]'\hat{\W}\sqrt{n}\hat{g}(\theta_o)$$
  因此当$n\to\infty$时, 根据(\ref{eq7.7})和Slutsky定理即可证得结论.
\end{proof}
一旦获得了GMM估计量$\hat{\theta}$, 我们就可以对它的渐近方差进行估计, 于是$\mathbold{\Omega}_o$的一致估计量可以由
$$\hat{\mathbold{\Omega}}=n^{-1}\sum_{i=1}^{n}g(W_i,\hat{\theta})g(W_i,\hat{\theta})'$$
给出, 而$\mathbold{G}_o$的一致估计量为
$$\hat{\mathbold{G}}=n^{-1}\sum_{i=1}^{n}\nabla_\theta g(W_i,\hat{\theta})$$
于是$\sqrt{n}\hat{\theta}$的渐近协方差矩阵估计量为
$$(\hat{\mathbold{G}}'\hat{\W}\hat{\mathbold{G}})^{-1}\hat{\mathbold{G}}'\hat{\W}\hat{\BO}\hat{\W}\hat{\mathbold{G}}(\hat{\mathbold{G}}'\hat{\W}\hat{\mathbold{G}})^{-1}$$

特别地, 如果权重矩阵$\W=\BO_o^{-1}$, 此时
$$\sqrt{n}(\hat{\theta}-\theta_o)\xrightarrow{d} N[0, (\mathbold{G}_o'\BO_o^{-1}\mathbold{G}_o)^{-1}]$$
下面我们证明, 这样的$\W$是最优权重矩阵, 也即对于任意权重矩阵$\W$, 以下渐近方差之差是半正定的
\begin{equation}\label{eq7.8}
  (\mathbold{G}_o'\W\mathbold{G}_o)^{-1}\mathbold{G}_o'\W\BO_o\W\mathbold{G}_o(\mathbold{G}_o'\W\mathbold{G}_o)^{-1}-(\mathbold{G}_o'\BO_o^{-1}\mathbold{G}_o)^{-1}
\end{equation}

\begin{theorem}\label{thm:thm7.4}
  在假设\ref{pro:pro7.2}下, 通过选取$\W=\BO_o^{-1}$得到的GMM估计量是渐近有效的.
\end{theorem}
\begin{proof}
  令$\V=(\mathbold{G}_o'\W\mathbold{G}_o)^{-1}\mathbold{G}_o'\W\BO_o\W\mathbold{G}_o(\mathbold{G}_o'\W\mathbold{G}_o)^{-1}$, 以及$\V_o=(\mathbold{G}_o'\BO_o^{-1}\mathbold{G}_o)^{-1}$. 为了证明(\ref{eq7.8})成立, 只需证明$\V_o^{-1}-\V^{-1}$半正定即可. 令
  $$\mathbold{D}=\mathbold{I}_L-\BO_o^{\frac{1}{2}}\W\mathbold{G}_o(\mathbold{G}_o'\W\BO_o\W\mathbold{G}_o)^{-1}\mathbold{G}_o'\W\BO_o^{\frac{1}{2}}$$
  显然$\mathbold{D}$是一个对称幂等矩阵. 进一步
  \begin{align*}
  \V_o^{-1}-\V^{-1}&=\G_o'\BO_o^{-1}\G_o-\G_o'\W\G_o(\G_o'\W\BO_o\W\G_o')^{-1}\G_o'\W\G_o \\
  &=\G_o'\BO_o^{-\frac{1}{2}}\mathbold{D}\BO_o^{-\frac{1}{2}}\G_o=(\mathbold{D}\BO_o^{-\frac{1}{2}}\G_o)'(\mathbold{D}\BO_o^{-\frac{1}{2}}\G_o)
  \end{align*}
  证毕.
\end{proof}
\begin{remark}
最优权重矩阵$\W$并不唯一, 对于任意$c\neq0$, $c\BO_o^{-1}$也是最优的.
\end{remark}
现在我们回到线性工具变量估计, 也即矩函数
$$g(W_i,\theta)=Z_i(Y_i-X_i'\theta)$$
如果假设\ref{pro:pro6.2}成立, 那么可以假设\ref{pro:pro7.2}也成立. 根据定理\ref{thm:thm7.3}, 线性GMM估计量
$$\hat{\theta}_{\text{GMM}}=(\X'\Z\hat{\W}\Z'\X)^{-1}\X'\Z\hat{\W}\Z'Y$$
在假设\ref{pro:pro6.2}下具有渐近正态性. 当权重矩阵$\displaystyle \hat{\W}=(\Z'\Z)^{-1}/n$时, GMM估计量变为2SLS估计量, 此时$\hat{\W}\xrightarrow{p}\W=\E[Z_iZ_i']^{-1}$, 以及$\BO_o=\E[Z_iZ_i'e_i^2]$.

如果条件同方差假设$\E[e_i^2|Z_i]=\sigma^2$成立, 那么$\BO_o=\sigma^2\E[Z_iZ_i']$, 我们可以不失一般性地令$\sigma^2=1$, 于是根据定理\ref{thm:thm7.4}可知, 此时2SLS估计量在所有线性工具变量估计量类中是渐近有效的. 然而, 如果误差项存在条件异方差, 那么2SLS不是渐近有效估计.

\subsection{二阶段GMM估计量}
定理\ref{thm:thm7.4}表明, 在假设\ref{pro:pro7.2}成立的情况下, 按照以下步骤获取的二阶段GMM估计量是渐近最优的. 与一步GMM估计量不同, 二阶段GMM估计量的权重矩阵$\W$不是固定的.

\textbf{第一阶段}: 首先获得某个一致的初始GMM估计量$\tilde{\theta}$, 这里
$$\tilde{\theta}=\arg\max_{\theta\in\Theta}\,\hat{g}(\theta)'\tilde{\W}\hat{g}(\theta)$$
并且权重矩阵$\tilde{\W}$依概率收敛于某个有限、对称和正定的矩阵$\W$, 为简便起见, 可以取$\tilde{\W}=\mathbold{I}_L$. 通常而言, $\tilde{\theta}$不是渐近最优的估计量, 但它仍是$\theta_o$的一致估计量.

然后, 我们可以构造$\BO_o=\text{avar}[\sqrt{n}\hat{g}(\theta_o)]$的一致估计量
$$\tilde{\BO}=n^{-1}\sum_{i=1}^{n}g(W_i,\tilde{\theta})g(W_i,\tilde{\theta})'$$
并选择权重矩阵$\hat{\W}=\tilde{\BO}^{-1}$.

\textbf{第二阶段}: 用权重矩阵$\hat{\W}=\tilde{\BO}^{-1}$可以得到一个新的GMM估计量
$$\hat{\theta}=\arg\max_{\theta\in\Theta}\,\hat{g}(\theta)'\hat{\W}\hat{g}(\theta)$$
这里权重矩阵$\hat{\W}$不涉及未知参数$\theta$, 它是一个随机权重矩阵. 这样得到的二阶段GMM估计量是渐近最优的, 这是因为$\hat{\W}\xrightarrow{p}\W=\BO_o^{-1}$. 此时
$$\sqrt{n}(\hat{\theta}-\theta_o)\xrightarrow{d}N[0,(\G_o'\BO_o^{-1}\G_o)^{-1}]$$

值得注意的是, 尽管二阶段GMM估计量是渐近有效的, 但在实际应用中可能需要不断重复估计, 知道GMM参数估计值和最小目标函数值收敛为止, 才能消除GMM估计量对初始权重矩阵$\tilde{\W}$的依赖.

最后来看参数检验. 按照之前的做法, 如果原假设$\HH_0:R(\theta_o)=r$成立, 我们可以构造稳健的Wald检验统计量
$$W=n[R(\hat{\theta})-r]'[\hat{\RH}\hat{\V}_\theta\hat{\RH}']^{-1}[R(\hat{\theta})-r]$$
在假设\ref{pro:pro7.2}成立的条件下, 当$n\to\infty$时有$W\xrightarrow{d}\chi^2_J$. 其中$R:\Theta\to \R^J$连续可微, $J\times P$维矩阵$\hat{\RH}=\nabla_\theta R(\hth)$的秩为$J$并且$J\leq P$, 协方差矩阵估计量$\hat{\V}_\theta$是基于渐近最优GMM估计量来选取的.
\section{过度识别检验}
现在我们想要检验矩条件$\E[g(W_i,\theta_o)]=0$是否成立, 一个基本思想是构建$L\times L$维样本矩
$$\hat{g}(\hat{\theta})=n^{-1}\sum_{i=1}^{n}g(W_i,\hat{\theta})$$
并判断它是否显著不等于0.

首先将$\sqrt{n}\hat{g}(\hat{\theta})$在$\theta_o$处的一阶Taylor展开得到
\begin{equation}\label{eq7.9}
  \sqrt{n}\hat{g}(\hat{\theta})=\sqrt{n}\hat{g}(\theta_o)+\overbar{\G}\sqrt{n}(\hat{\theta}-\theta_o)
\end{equation}
根据之前的结论又有
$$\sqrt{n}(\hat{\theta}-\theta_o)=-\{[\nabla_\theta \hat{g}(\hat{\theta})]'\hat{\W}\overbar{\mathbold{G}}\}^{-1}[\nabla_\theta \hat{g}(\hat{\theta})]'\hat{\W}\sqrt{n}\hat{g}(\theta_o)$$
将其代入到(\ref{eq7.9})中, 并且左乘矩阵$\hat{\W}^{\frac{1}{2}}$得到
\begin{align*}
\hat{\W}^{\frac{1}{2}}\sqrt{n}\hat{g}(\hat{\theta})&=\hat{\W}^{\frac{1}{2}}\sqrt{n}\hat{g}(\theta_o)+\hat{\W}^{\frac{1}{2}}\overbar{\G}\sqrt{n}(\hat{\theta}-\theta_o) \\
&=\hat{\mathbold{\Pi}}\hat{\W}^{\frac{1}{2}}\sqrt{n}\hat{g}(\theta_o)
\end{align*}
这里的权重矩阵$\hat{\W}=\tilde{\BO}^{-1}$出自第一阶段的渐近最优GMM估计, 并且
$$\hat{\mathbold{\Pi}}=\mathbold{I}_L-\hat{\W}^{\frac{1}{2}}\overbar{\G}\{[\nabla_\theta \hat{g}(\hat{\theta})]'\hat{\W}\overbar{\G}\}^{-1}[\nabla_\theta\hat{g}(\hat{\theta})]'\hat{\W}^{\frac{1}{2}}$$
根据(\ref{eq7.7}), 由Slutsky定理可知
$$\hat{\W}^{\frac{1}{2}}\sqrt{n}\hat{g}(\theta_o)\xrightarrow{d} \mathbold{B}$$
这里$\mathbold{B}\sim N(0,\mathbold{I}_L)$. 当$n\to\infty$时有
$$\hat{\mathbold{\Pi}}\xrightarrow{p}\mathbold{I}_L-\W^{\frac{1}{2}}\G_o(\G_o'\W\G_o)^{-1}\G_o'\W^{\frac{1}{2}}=\mathbold{\Pi}$$
其中$\W=\BO_o^{-1}$. 容易验证, $\mathbold{\Pi}$是一个$L\times L$维对称幂等矩阵, 并且$\text{trace}\,(\mathbold{\Pi})=L-P$.

在原假设$\HH_0: \E[g(W_i,\theta_o)]=0$成立的情况下, 根据引理\ref{lem:lem2.1}可知
\begin{align*}
n[\hat{g}(\hat{\theta})'\hat{\W}\hat{g}(\hat{\theta})]&=[\hat{\W}^{\frac{1}{2}}\sqrt{n}\hat{g}(\theta_o)]'\hat{\mathbold{\Pi}}^2[\hat{\W}^{\frac{1}{2}}\sqrt{n}\hat{g}(\theta_o)] \\
&=[\hat{\W}^{\frac{1}{2}}\sqrt{n}\hat{g}(\theta_o)]'\hat{\mathbold{\Pi}}[\hat{\W}^{\frac{1}{2}}\sqrt{n}\hat{g}(\theta_o)] \\
&\xrightarrow{d} \mathbold{B}'\mathbold{\Pi B}\sim \chi^2_{L-P}
\end{align*}
以上检验方法称为Hansen检验.
\begin{theorem}\label{thm:thm7.5}
  在假设\ref{pro:pro7.2}下, 如果模型是过度识别的且当$n\to\infty$时有$\hat{\W}\xrightarrow{d}\BO_o^{-1}$. 那么当原假设$\HH_0:\E[g(W_i,\theta_o)]=0$成立时, 当$n\to\infty$时有
  $$J=n\cdot \hat{g}(\hat{\theta})'\hat{\W}\hat{g}(\hat{\theta}) \xrightarrow{d}\chi_{L-P}^2$$
\end{theorem}

Hansen检验基于渐近最优GMM估计量, 如果选取的是其它GMM估计量, 则无法得到上述结果. 此时需要使用另一个权重矩阵$\tilde{\W}$来构造Wald检验统计量, 它的渐近分布为$\chi_L^2$, 由于$\chi^2_{L-P}$的临界值更小, 因此使用渐近最优GMM估计量的Hansen检验更容易拒绝原假设.

另一方面, Hansen检验只能在模型过度识别的情况下进行, 如果模型是恰好识别的, 那么无论原假设$\HH_0:\E[g(W_i,\theta_o)]=0$是否成立, 样本矩$\hat{g}(\hat{\theta})$等于零向量, 从而导致$J=0$.

特别地, 我们可以利用Hansen检验来推导定理\ref{thm:thm6.4}. 设矩函数为
$$g(W_i,\theta)=Z_i(Y_i-X_i'\theta)$$
2SLS残差为$\hat{e}_i=Y_i-X_i'\hat{\theta}_{\text{2SLS}}$, 选取权重矩阵
$$\hat{\W}=ns^{-2}(\Z'\Z)^{-1}$$
这里$s^2=n^{-1}\sum_{i=1}^{n}\hat{e}_i^2$, 由此得到的2SLS估计量是渐近最优GMM估计量, 于是Hansen检验统计量为
$$J=n\cdot\hat{g}(\hat{\theta})'\hat{\W}\hat{g}(\hat{\theta})=\frac{\hat{e}'\Z(\Z'\Z)^{-1}\Z'\hat{e}}{\hat{e}'\hat{e}/n}$$
当条件同方差$\E[e_i^2|Z_i]=\sigma^2$成立时, $\BO_o=\sigma^2\E[Z_iZ_i']$, 并且$$\hat{\W}\xrightarrow{p}\BO_o^{-1}=\sigma^{-2}\E[Z_iZ_i']^{-1}$$
因此从定理\ref{thm:thm7.5}即可推得定理\ref{thm:thm6.4}, 也即Hansen检验统计量变为Sargan检验统计量.

\section{系统工具变量}
本节将处理SUR方程组的内生性问题, 这是线性GMM的一个应用. 考虑如下线性回归
\begin{equation}\label{eq7.11}
  Y_i=\overbar{X}_i\beta_o+e_i
\end{equation}
其中$Y_i$是$m\times 1$维向量, $\OX_i$为$m\times \overbar{K}$维矩阵, $e_i$为$m\times1$维误差向量. 可以将其展开为$j$个方程构成的SUR方程组
\begin{align*}
Y_1&=X_1'\beta_{1o}+e_1 \\
&\vdots \\
Y_m&=X_m'\beta_{mo}+e_m
\end{align*}
对于每个$j=1,\cdots,m$, $X_j$表示一个既包含内生变量又包含外生变量的$K_j\times1$维向量. 并且对于每个$j$, 我们还有一个$L_j\times1$维的工具变量集合$Z_j$, 观测值$i$上的工具变量矩阵可以记为
$$\overbar{Z}_i=\begin{bmatrix}
                   Z_{i1}' & 0 & \cdots & 0 \\
                   0 & Z_{i2}' & \cdots & 0 \\
                   \vdots & \vdots &  & \vdots \\
                   0 & 0 & \cdots & Z_{im}'
                 \end{bmatrix}$$
它的维数为$m\times\overbar{L}$, 其中$\overbar{L}=\sum_{j=1}^{m}L_j$.

现在我们给出以下假设, 并假设相关的矩条件成立.
\begin{proposition}\label{pro:pro7.3}
(1) $\E[\overbar{Z}_i'e_i]=0$.

(2) $\text{rank}\,(\E[\overbar{Z}_i'\OX_i])=\overbar{K}$.
\end{proposition}
上述假设意味着对于每个$j=1,\cdots,m$, 总有$\E[Z_j'e_j]=0$和$\text{rank}\,(\E[Z_{ij}X_{ij}'])=K_j$. 类似于第六章的讨论, 假设\ref{pro:pro7.3}(2)是识别的秩条件, 它的必要要求为$\overbar{L}\geq \overbar{K}$.

在假设\ref{pro:pro7.3}下, $\beta_o$是求解以下线性矩条件的唯一$\overbar{K}\times1$维向量
\begin{equation}\label{eq7.12}
  \E[\overbar{Z}_i'(Y_i-\overbar{X}_i\beta)]=0
\end{equation}
换言之, 对于任意$\beta\neq\beta_o$, 总有$\E[\overbar{Z}_i'(Y_i-\overbar{X}_i\tilde{\beta})]\neq0$. 当$\overbar{L}=\overbar{K}$时, 模型是恰好识别的, 此时$\beta$的系统IV估计量为
$$\hb_{\text{IV}}=\left(\sum_{i=1}^{n}\overbar{Z}_i'\overbar{X}_i\right)^{-1}\left(\sum_{i=1}^{n}\overbar{Z}_i'Y_i\right)$$

当$\overbar{L}>\overbar{K}$时, 模型是过度识别的, 可以得到$\beta$的GMM估计量
$$\hb_\text{GMM}=(\overbar{\X}'\overbar{\Z}\hat{\W}\overbar{\Z}'\overbar{\X})^{-1}\overbar{\X}'\overbar{\Z}\hat{\W}\overbar{\Z}'Y$$
其中$\overbar{\X}$和$\overbar{\Z}$分别为$\overbar{X}_i$和$\overbar{Z}_i$是在观测值$i$上堆叠起来的矩阵, 并且当$n\to\infty$时有$\hat{\W}\xrightarrow{p}\W$, 这里的$\W$是一个$\overbar{L}\times \overbar{L}$维非随机的对称正定矩阵. 在此基础上还有
$$\sqrt{n}(\hb_\text{GMM}-\beta_o)\xrightarrow{d}N(0,\V_\beta)$$
其中
$$\V_\beta=(\Q_{XZ}\W\Q_{ZX})^{-1}\Q_{XZ}\W\BO\W\Q_{ZX}(\Q_{XZ}\W\Q_{ZX})^{-1}$$
以及
\begin{align*}
\Q_{XZ}&=\E[\overbar{X}_i'\overbar{Z}_i] \\
\Q_{ZX}&=\E[\overbar{Z}_i'\overbar{X}_i] \\
\BO&=\E[\overbar{Z}_i'e_ie_i'\overbar{Z}_i]
\end{align*}
如果选取$\hat{\W}=(\overbar{\Z}'\overbar{\Z}/n)^{-1}$, 那么此时GMM估计量为
$$\hb_\text{S2SLS}=[\overbar{\X}'\overbar{\Z}(\overbar{\Z}'\overbar{\Z})^{-1}\overbar{\Z}'\overbar{\X}]^{-1}\overbar{\X}'\overbar{\Z}(\overbar{\Z}'\overbar{\Z})^{-1}\overbar{\Z}'Y$$
称为系统2SLS估计量. 由此可以得到残差$\check{e}_i=Y_i-\OX_i\hb_{\text{S2SLS}}$, 进而可以选取最优权重矩阵
\begin{equation}\label{eq7.13}
  \hat{\W}=\left(n^{-1}\sum_{i=1}^{n}\overbar{Z}_i'\check{e}_i\check{e}_i'\overbar{Z}_i\right)^{-1}
\end{equation}
根据(\ref{eq7.13})得到GMM估计量$\hb_\text{GMM}$是渐近最优的, 它的协方差矩阵估计量为
\begin{equation}\label{eq7.14}
  \left[\overbar{\X}'\overbar{\Z}\left(\sum_{i=1}^{n}\overbar{Z}_i'\hat{e}_i\hat{e}_i'\overbar{Z}_i\right)^{-1}\overbar{\Z}'\overbar{\X}\right]^{-1}
\end{equation}
其中$\hat{e}_i=Y_i-\overbar{X}_i\hb_\text{GMM}$. 从渐近观点上看, 使用第一阶段残差$\check{e}_i$来替代$\hat{e}_i$不会产生差异, 这个矩阵对角元素的平方根最优GMM估计量的渐近标准误.

特别地, 如果$\overbar{Z}_i=\overbar{X}_i$且$\hat{e}_i$是SOLS残差, 那么表达式(\ref{eq7.14})变为SOLS的稳健协方差矩阵估计量; 而当$\overbar{Z}_i=\hat{\BS}^{-1}\overbar{X}_i$, 且$\hat{e}_i$是SUR中的FGLS残差时, 那么表达式(\ref{eq7.14})变为FGLS的稳健协方差矩阵估计量.

为了检验原假设$\HH_0:\RH\beta_o=r$, 除了使用传统的Wald检验外, 还可以使用GMM距离检验. 假设$\hat{\W}$是一致估计出$\W$的最优加权矩阵, $\hb$是利用$\W$获得的无约束GMM估计量, 而$\tilde{\beta}$表示利用相同的$\hat{\W}$获得的具有$J$个线性约束的GMM估计量, 可以证明在$\HH_0$下, GMM距离检验统计量
$$n^{-1}\left[\left(\sum_{i=1}^{n}\overbar{Z}_i'\tilde{e}_i\right)'\hat{\W}\left(\sum_{i=1}^{n}\overbar{Z}_i'\tilde{e}_i\right)-\left(\sum_{i=1}^{n}\overbar{Z}_i'\hat{e}_i\right)'\hat{\W}\left(\sum_{i=1}^{n}\overbar{Z}_i'\hat{e}_i\right)\right]\xrightarrow{d}\chi_J^2$$
其中残差为$\tilde{e}_i=Y_i-\overbar{X}_i\tilde{\beta}$, 以及$\hat{e}_i=Y_i-\OX_i\hb$. 而在另一个假设$\HH_0:\E[\overbar{Z}_i'e_i]=0$下, 利用Hansen检验可以得到
$$n^{-1}\left(\sum_{i=1}^{n}\overbar{Z}_i'\hat{e}_i\right)'\hat{\W}\left(\sum_{i=1}^{n}\overbar{Z}_i'\hat{e}_i\right)\xrightarrow{d}\chi^2_{\overbar{L}-\overbar{K}}$$
这里要求模型是过度识别的.

\chapter{面板数据模型}
之前我们讨论的内容都是基于截面数据, 本章讨论面板数据的计量模型. 面板数据既包括了截面维度, 也包含了时间维度, 可以在一定程度上缓解由不可观测的时变因素造成的内生性问题, 并且捕捉到更多信息以提高估计的精确度.
\section{面板数据}
面板数据 (panel data)是在一段时期内追踪同一组个体的数据, 我们用下标$i$表示第$i$个体, 下标$t$表示个体位于第$t$期, 下标 $it$可以唯一识别个体$i$在第$t$期的情况. 此外, 我们$T$表示个体$i$被观测到的期数, 而用$S_i$来表示观测到个体$i$的时期组成的集合.

如果在同一时期内, 所有个体数据都能观测到, 则称面板数据是平衡的 (balanced). 假设我们有$N$个截面, 以及$T$个时期, 那么平衡面板数据共计有$n=NT$个观测值. 反之, 如果存在个体在某个时期观测不到的情况, 则称面板数据是非平衡的 (unbalanced), 共计有$n=\sum_{i=1}^{N}T$个观测值. 如果数据是随机缺失的, 则非平衡面板不会对应用产生影响, 只会让推导的符号变复杂, 以下仅讨论平衡面板数据.

除了这种上述方式外, 还可以按短面板和长面板进行分类, 其中短面板数据中的个体维度$N$比时间维度$T$更大, 而长面板数据的个体维度$N$比时间维度$T$更小. 一般而言, 微观层面的面板数据都是短面板.

现在来看具体的符号表示. 我们用序对$(Y_{it},X_{it})$表示观测, 其中$Y_{it}$是因变量, $X_{it}$是$K\times1$维自变量. 此外, 用$Y_i$表示由观测值$Y_{it}$堆叠起来的$T\times 1$维向量, $X_i$表示由观测值$X_{it}'$堆叠起来的$T\times K$维矩阵. 最后, 用记号$Y=[Y_1',\cdots,Y_N']'$表示$Y_i$堆叠起来的$n\times 1$维向量, $\X=[X_1',\cdots,X_N']'$的含义相似.
\section{混合回归}
最简单的面板数据模型为混合回归 (pooled regression). 考虑以下模型
\begin{align}
Y_{it}&=X_{it}'\beta+e_{it} \nonumber \\
\E[X_{it}e_{it}]&=0 \label{eq8.1}
\end{align}
其中$\beta$是$K\times1$维参数向量, $e_{it}$为随机扰动项. 这个模型也可以写作
\begin{align*}
Y_i&=X_i\beta+e_i \\
\E[X_i'e_i]&=0
\end{align*}
这里的$e_i$是$T\times1$维的. 最后, 全样本回归可以记作$Y=\X\beta+e$.

在以上模型, 每一个体的误差项都和解释变量不相关, 这是相当严苛的条件. 如果解释变量包含了因变量$Y_{it}$的滞后项, 则(\ref{eq8.1})一定不成立. 在混合回归模型中, 估计$\beta$的标准方法是最小二乘, 可以写作
\begin{align*}
\hb_{\text{POOL}}&=\left(\sum_{i=1}^{N}\sum_{t=1}^T X_{it}X_{it}'\right)^{-1}\left(\sum_{i=1}^{N}\sum_{t=1}^TX_{it}Y_{it}\right) \\
&=\left(\sum_{i=1}^{N}X_i'X_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'Y_i \right) \\
&=(\X'\X)^{-1}\X'Y
\end{align*}
称$\hb_{\text{POOL}}$为混合OLS估计量, 第$i$个个体的残差向量为$\hat{e}_i=Y_i-X_i\hb_{\text{POOL}}$.

如果比投影条件(\ref{eq8.1})更强的均值独立
\begin{equation}\label{eq8.2}
  \E[e_{it}|X_i]=0
\end{equation}
成立, 那么由
$$\hb_{\text{POOL}}=\beta+\left(\sum_{i=1}^{N}X_i'X_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'e_i\right)$$
可知$\E[\hb_{\text{POOL}}|\X]=\beta$, 也即$\hb_{\text{POOL}}$是$\beta$的无偏估计量.

如果$\{e_{it}\}$满足条件同方差和序列无关, 那么可以$\hb_{\text{POOL}}$的协方差矩阵估计量具有经典形式. 如果$\{e_{it}\}$存在异方差而无序列相关, 那么可以使用异方差稳健的协方差矩阵估计量. 但总体而言, 不同个体的误差项可以假设为不相关, 而同一个体在不同时期的误差项往往相关, 此时应该使用聚类稳健的协方差矩阵估计量
$$\hat{\V}_{\text{POOL}}=a_n(\X'\X)^{-1}\left(\sum_{i=1}^{N}X_i'\hat{e}_i\hat{e}_i'X_i\right)(\X'\X)^{-1}$$
其中
$$a_n=\left(\frac{n-1}{n-K}\right)\left(\frac{N}{N-1}\right)$$
为有限样本下的自由度调整.

\section{随机效应模型}
\subsection{RE估计量}
显然, 对面板数据进行混合回归要求过于严苛, 并且没有充分利用面板数据的优势. 现在考察面板数据的综合误差项$e_{it}$的误差成分结构 (error-components structure), 它的最简形式为
\begin{equation}\label{eq8.3}
  e_{it}=u_i+\varepsilon_{it}
\end{equation}
其中$u_i$称为个体效应 (individual fixed effects), $\varepsilon_{it}$是i.i.d.的特质误差项 (idiosyncratic error). 根据(\ref{eq8.3}), 我们可以建立单向误差成分模型  (one-way error components model)
\begin{equation}\label{eq8.10}
  Y_{it}=X_{it}'\beta+u_i+\varepsilon_{it}
\end{equation}
上式的堆叠形式为
$$Y_i=X_i\beta+\mathbf{1}_iu_i+\varepsilon_i$$
对它的分析依赖于误差项$u_i$和$\varepsilon_{it}$的结构.

我们首先给出以下随机效应 (random effects)假设, 即假定误差结构(\ref{eq8.3})具有零条件均值, 并且$u_i$和$\varepsilon_{it}$都满足条件同方差.
\begin{proposition}\label{pro:pro8.1}
对于误差结构(\ref{eq8.3}), 以下条件成立:
\begin{align}
\E[\varepsilon_{it}|X_i,u_i]&=0 \label{eq8.4} \\
\E[\varepsilon_{it}^2|X_i]&=\sigma_\varepsilon^2 \label{eq8.5} \\
\E[\varepsilon_{it}\varepsilon_{js}|X_i]&=0 \label{eq8.6} \\
\E[u_i|X_i]&=0 \label{eq8.7} \\
\E[u_i^2|X_i]&=\sigma^2_u \label{eq8.8} \\
\text{rank}\,(\E[X_i'\BO^{-1}X_i])&=K
\end{align}
\end{proposition}
根据假设以上假设, 通过混合OLS方法得到的POOL估计量是一致的, 但由于误差项不是球型扰动项, 因而混合回归不是最有效率的方法.

此外, 我们还可以从这些假设中得出误差向量$e_i$满足$\E[e_i|X_i]=0$, 以及
\begin{align}
\E[e_ie_i'|X_i]&=\BO=\sigma_u^2\mathbf{1}_i\mathbf{1}_i'+\sigma_\varepsilon^2\mathbold{I}_i \nonumber \\
&=\begin{bmatrix}
   \sigma_u^2+\sigma_\varepsilon^2 & \sigma_u^2 & \cdots & \sigma_u^2 \\
   \sigma_u^2 & \sigma_u^2+\sigma_\varepsilon^2 & \cdots & \sigma_u^2 \\
   \vdots & \vdots &  & \vdots \\
   \sigma_u^2 & \sigma_u^2 & \cdots & \sigma_u^2+\sigma_\varepsilon^2
 \end{bmatrix} \label{eq8.14}
\end{align}
其中$\mathbf{1}_i$是元素全为1的$T\times1$维向量, $\mathbold{I}_i$为$T\times T$维单位矩阵. 对于个体$i$, 它的误差项在第$t$和第$s$期的相关系数为$\rho=\sigma_u^2/(\sigma_u^2+\sigma_\varepsilon^2)$.

在假设\ref{pro:pro8.1}下, 称单向误差成分模型(\ref{eq8.10})为随机效应模型. 由于给定了误差项的成分, 如果已知$\sigma_u^2$和$\sigma_\varepsilon^2$, 那么估计回归系数$\beta$的方法为广义最小二乘, 也即
$$\hb_{\text{GLS}}=\left(\sum_{i=1}^{N}X_i'\BO^{-1}X_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\BO^{-1}Y_i\right)$$
此时
$$\hb_{\text{GLS}}-\beta=\left(\sum_{i=1}^{N}X_i'\BO^{-1}X_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\BO^{-1}e_i\right)$$
根据假设\ref{pro:pro8.1}可知$\E[\hb_{\text{GLS}}|\X]=\beta$, 也即$\hb_{\text{GLS}}$是$\beta$的无偏估计量. 此外还能得到$\hb_{\text{GLS}}$的条件方差为
\begin{equation}\label{eq8.13}
  \V_{\text{GLS}}=\left(\sum_{i=1}^{N}X_i'\BO^{-1}X_i\right)^{-1}
\end{equation}

现在来比较估计量$\hb_{\text{GLS}}$和$\hb_{\text{POOL}}$, 在假设\ref{pro:pro8.1}下可知$\hb_{\text{POOL}}$的条件方差为
$$\V_{\text{POOL}}=\left(\sum_{i=1}^{N}X_i'X_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\BO X_i\right)\left(\sum_{i=1}^{N}X_i'X_i\right)^{-1}$$
根据Gauss-Markov定理可知$\V_{\text{GLS}}\leq\V_{\text{POOL}}$, 因此在假设\ref{pro:pro8.1}下, $\hb_{\text{GLS}}$比$\hb_{\text{POOL}}$更有效. 此外, 当个体效应不存在时有$\sigma_u^2=0$, 于是$\BO=\sigma_\varepsilon^2\mathbold{I}_i$, 随机效应模型变为混合回归模型, 故而$\V_{\text{GLS}}=\V_{\text{POOL}}=\sigma_\varepsilon^2(\X'\X)^{-1}$.

以上的GLS分析建立在已知$\sigma_u^2$和$\sigma_\varepsilon^2$的基础上, 然而在实际情况中, 二者通常都是未知的, 此时仍需使用FGLS估计. 假定我们有$\sigma_u^2$和$\sigma_\varepsilon^2$的一致估计量, 那么可以构建
$$\hat{\BO}=\hat{\sigma}_u^2\mathbf{1}_i\mathbf{1}_i'+\hat{\sigma}_\varepsilon^2\mathbold{I}_i$$
于是可行的随机效应估计量为
$$\hb_{\text{RE}}=\left(\sum_{i=1}^{N}X_i'\hat{\BO}^{-1}X_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\hat{\BO}^{-1}Y_i\right)$$
在假设\ref{pro:pro8.1}下, 由FGLS方法推导而来的随机效应估计量$\hb_{\text{RE}}$也是一致估计量.

现在的问题是如何获取一致估计量$\hat{\sigma}_u^2$和$\hat{\sigma}_\varepsilon^2$. 首先定义$\E[e_{it}^2]=\sigma_e^2$, 根据误差结构(\ref{eq8.3})和假设\ref{pro:pro8.1}可知$\sigma^2_e=\sigma_u^2+\sigma_\varepsilon^2$, 于是我们只需得到$\hat{\sigma}_e^2$和$\hat{\sigma}_\varepsilon^2$即可. 记$\check{e}_{it}$是出自混合OLS的残差, 容易得到
\begin{equation}\label{eq8.11}
  \hat{\sigma}_e^2=\frac{1}{NT-K}\sum_{i=1}^{N}\sum_{t=1}^T\check{e}_{it}
\end{equation}
另一方面, 我们还需要得出$\hat{\sigma}_u^2$. 注意到当$t\neq s$时有$\E[e_{it}e_{is}]=\sigma_u^2$, 故而
\begin{align*}
\E\left[\sum_{t=1}^{T-1}\sum_{s=t+1}^{T} e_{it}e_{is}\right]=\sum_{t=1}^{T-1}\sum_{s=t+1}^{T}\E[e_{it}e_{is}]=\frac{T(T-1)}{2}\sigma_u^2
\end{align*}
从而
\begin{equation}\label{eq8.12}
  \hat{\sigma}_u^2=\frac{1}{NT(T-1)/2-K}\sum_{i=1}^{N}\sum_{t=1}^{T-1}\sum_{s=t+1}^{T}\check{e}_{it}\check{e}_{is}
\end{equation}
根据(\ref{eq8.11})和(\ref{eq8.12})即可得到我们需要的估计量$\hat{\sigma}_\varepsilon^2=\hat{\sigma}_e^2-\hat{\sigma}_u^2$. 如果面板数据是平衡的, 那么(\ref{eq8.12})为
$$\hat{\sigma}_u^2=\frac{1}{NT(T-1)/2-K}\sum_{i=1}^{N}\sum_{t=1}^{T-1}\sum_{s=t+1}^{T}\check{e}_{it}\check{e}_{is}$$
在假设(\ref{pro:pro8.1})下, (\ref{eq8.11})和(\ref{eq8.12})是一致估计量. 除了上述方法外, 还可以通过固定效应来估计特质误差方差$\sigma_\varepsilon^2$, 以及通过组间估计来获取$\sigma_u^2$的估计量.

最后来看$\hb_{\text{RE}}$的协方差矩阵估计量. 如果假设\ref{pro:pro8.1}确实完全成立, 可以直接由(\ref{eq8.13})构造出
$$\hat{\V}_{\text{RE}}^0=\left(\sum_{i=1}^{N}X_i'\hat{\BO}^{-1}X_i\right)^{-1}$$
然而假设\ref{pro:pro8.1}毕竟是很强的条件, 对于$T$固定且$N$相当大的面板数据而言, 使用以下聚类稳健的估计量通常也不会损失什么
$$\hat{\V}_{\text{RE}}=\left(\sum_{i=1}^{N}X_i'\hat{\BO}^{-1}X_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\hat{\BO}^{-1}\hat{e}_i\hat{e}_i'\hat{\BO}^{-1}X_i\right)\left(\sum_{i=1}^{N}X_i'\hat{\BO}^{-1}X_i\right)^{-1}$$
其中$\hat{e}_i=Y_i-X_i\hat{\beta}_{\text{RE}}$.

如果标准的随机效应假设\ref{pro:pro8.1}成立, 但实际上模型中并没有包括不可观测的个体效应$u_i$, 那么混合OLS是有效的, 并且所有与混合OLS相关的检验统计量也是渐近有效的. 基于此, 如果不可观测效应$u_i$不存在, 那么我们可以检验原假设
$$\HH_0: \sigma_u^2=0$$
基于此, Wooldridge (2010)介绍了一种统计量
$$Z=\frac{\displaystyle\sum_{i=1}^{N}\sum_{t=1}^{T-1}\sum_{s=t+1}^{T}\hat{e}_{it}\hat{e}_{is}}{\displaystyle\left[\sum_{i=1}^{N}\left(\sum_{t=1}^{T-1}\sum_{s=t+1}^{T}\hat{e}_{it}\hat{e}_{is}\right)^2\right]^{\frac{1}{2}}}$$
这里的$\hat{e}_{it}$为混合OLS残差. 在$\{e_{it}\}$不存在序列相关的情况下, 这个统计量服从渐近标准正态分布. 然而, 对原假设的拒绝并不意味着随机效应结构是正确的.


\subsection{一般FGLS分析}
实际上, 随机效应估计是一般FGLS分析的一个特例. 如果特质误差$\{\varepsilon_{it}\}$存在异方差性且对于不同的$t$是序列相关的, 那么在FGLS中应该使用更一般的估计量
$$\hat{\BO}=N^{-1}\sum_{i=1}^{N}\check{e}_{i}\check{e}_i'$$
这里的$\check{e}_i$是混合OLS残差. 只要条件(\ref{eq8.4})和(\ref{eq8.7})成立, 那么FGLS估计量是一致的.

对于$N$很大的面板数据而言, 如果假设\ref{pro:pro8.1}完全成立, 那么一般FGLS估计量和RE估计量一样渐近有效. 而如果$\E[e_ie_i'|X_i]=\BO$, 并且$\BO$不具有随机效应结构(\ref{eq8.14}), 那么一般FGLS估计量更为渐近有效. 那为何还要使用RE估计量?

原因在于, 如果$N$比$T$大不了多少倍, 那么无约束的FGLS分析的有限样本性质欠佳, 因为$\hat{\BO}$共计有$T(T+1)/2$个待估参数, 而RE估计量只需要两个待估参数.

既然一般FGLS分析的待估参数太多, 而传统的RE分析需要的假设条件过强, 故而可以考虑二者的中间形式. 例如, 特质误差$\varepsilon_{it}$服从自相关系数为$\rho$且方差为$\sigma_\varepsilon^2$的平稳AR(1)过程, 那么
$$\BO=\sigma^2_u\mathbf{1}_i\mathbf{1}_i'+\E[\varepsilon_i\varepsilon_i']$$
只依赖于参数$\sigma_\varepsilon^2$, $\sigma_u^2$以及$\rho$, 这些参数通过混合回归就可以得到估计, 于是就可以很容易地使用FGLS方法.

\section{固定效应模型}
继续考虑具有单向误差结构的回归模型
\begin{equation}\label{eq8.15}
  Y_{it}=X_{it}'\beta+u_i+\varepsilon_{it}
\end{equation}
或者
\begin{equation}\label{eq8.16}
  Y_i=X_i\beta+\mathbf{1}_iu_i+\varepsilon_i
\end{equation}
 在许多情况下, 我们可以将个体效应$u_i$解释为不可观测的非时变遗漏变量 (例如在工资方程中, $u_i$可能包含不可观测的个体$i$的能力), 此时称(\ref{eq8.15})和(\ref{eq8.16})为固定效应模型 (fixed effects model).

 由于$u_i$可以被解释为遗漏变量, 自然可以将其视作与解释变量$X_{it}$相关, 此时混合OLS估计量和FGLS估计量均是不一致的.
\subsection{组内估计}
\begin{proposition}\label{pro:pro8.2}
$\E[\varepsilon_{it}|X_i, u_i]=0$.
\end{proposition}
这个假设称为严格外生性 (strictly exogeneity), 由它可以推出特质误差$\varepsilon_{it}$与回归元$X_{it}$及个体固定效应$u_i$均不相关, 但是反之不行.  由于允许$\E[u_i|X_i]$为$X_i$的任意函数, 因此FE分析比RE分析更加稳健.

在假设\ref{pro:pro8.2}下, 估计$\beta$的方法是使用组内变换 (within transformation), 以消去不可观测的个体固定效应$u_i$. 首先定义
\begin{equation}\label{eq8.17}
  \overbar{Y}_i=T^{-1}\sum_{t=1}^{T}Y_{it}
\end{equation}
它是在给定个体$i$的情况下, 对$Y_{it}$在时间上取平均. 再定义
$$\dot{Y}_{it}=Y_{it}-\overbar{Y}_i$$
称为组内变换, 也即离差形式的$Y_{it}$. 继续考虑堆叠形式, 可以将(\ref{eq8.17})写为$\overbar{Y}_i=(\mathbf{1}_i\mathbf{1}_i')^{-1}\mathbf{1}_i'Y_i$, 从而
\begin{align*}
\dot{Y}_i&=Y_i-\mathbf{1}_i\overbar{Y}_i \\
&=Y_i-\mathbf{1}_i(\mathbf{1}_i\mathbf{1}_i')^{-1}\mathbf{1}_i'Y_i \\
&=\mathbold{M}_iY_i
\end{align*}
其中$\mathbold{M}_i=\mathbold{I}_i-\mathbf{1}_i(\mathbf{1}_i\mathbf{1}_i')^{-1}\mathbf{1}_i'$, 它是一个$T\times T$维对称幂等矩阵. 类似可以定义
\begin{align*}
\overbar{X}_i&=T^{-1}\sum_{t=1}^{T}X_{it} \\
\dot{X}_{it}&=X_{it}-\overbar{X}_i \\
\dot{X}_i&=\mathbold{M}_iX_i
\end{align*}
最后可以得到全样本的堆叠表示, 定义$NT\times NT$维矩阵$\mathbold{D}=\text{diag}\,\{\mathbold{I}_i,\cdots,\mathbold{I}_i\}$以及$\mathbold{M}_D=\text{diag}\,\{\mathbold{M}_i,\cdots,\mathbold{M}_i\}$, 于是
$$\mathbold{M}_DY=\dot{Y}=\begin{bmatrix}
                            \dot{Y}_1 \\
                            \vdots \\
                            \dot{Y}_N
                          \end{bmatrix},\quad \mathbold{M}_D\X=\dot{\X}=\begin{bmatrix}
                                                                          \dot{X}_1 \\
                                                                          \vdots \\
                                                                          \dot{X}_N
                                                                        \end{bmatrix}$$

现在我们对方程(\ref{eq8.15})在$t=1,\cdots,T$上取平均, 由此得到截面方程
\begin{equation}\label{eq8.18}
  \overbar{Y}_i=\overbar{X}_i'\beta+u_i+\overbar{\varepsilon}_i
\end{equation}
其中$\overbar{\varepsilon}_i=T^{-1}\sum_{t=1}^{T}\varepsilon_{it}$. 使用(\ref{eq8.15})减去(\ref{eq8.18})可得
$$\dot{Y}_{it}=\dot{X}_{it}'\beta+\dot{\varepsilon}_{it}$$
其中$\dot{\varepsilon}_{it}=\varepsilon_{it}-\overbar{\varepsilon}_i$. 或者将方程写为
$$\dot{Y}_i=\dot{X}_i\beta+\dot{\varepsilon}_i$$
利用最小二乘即可得到固定效应估计量
\begin{align*}
\hb_{\text{FE}}&=\left(\sum_{i=1}^{N}\sum_{t=1}^{T}\dot{X}_{it}\dot{X}_{it}'\right)^{-1}\left(\sum_{i=1}^{N}\sum_{t=1}^{T}\dot{X}_{it}\dot{Y}_{it}\right) \\
&=\left(\sum_{i=1}^{N}\dot{X}_i'\dot{X}_i\right)^{-1}\left(\sum_{i=1}^{N}\dot{X}_i'\dot{Y}_i\right) \\
&=\left(\sum_{i=1}^{N}X_i'\mathbold{M}_iX_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\mathbold{M}_iY_i\right)
\end{align*}
以及固定效应残差$\hat{\varepsilon}_{it}=\dot{Y}_{it}-\dot{X}_{it}'\hb_{\text{FE}}$. 由组内变换得到的FE估计量又称组内估计量 (within estimator).

\begin{remark}
在进行组内变换的时候, 不可观测的个体效应$u_i$都被消去, 其本质是消去非时变因素, 因此$X_{it}$中不能包含非时变的回归元, 例如截距项、人的性别等变量. 因此在组内估计中, $X_{it}$的维数$K$是时变因素的数量.

然而我们仍然可以对截距项进行估计, 在模型$Y_{it}=\alpha+X_{it}'\beta+u_i+\varepsilon_{it}$中, 估计量$\hat{\alpha}_{\text{FE}}=\overbar{Y}-\overbar{X}'\hb_{\text{FE}}$, 其中$\overbar{Y}$和$\overbar{X}$为对应的全样本均值. 此外, 个体效应估计量可以写为$\hat{u}_i=\overbar{Y}_i-\overbar{X}_i'\hb_{\text{FE}}-\hat{\alpha}_{\text{FE}}$.
\end{remark}

利用假设\ref{pro:pro8.2}可以得到FE估计量的有限样本性质, 首先注意到
$$\hb_{\text{FE}}-\beta=\left(\sum_{i=1}^{N}X_i'\mathbold{M}_iX_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\mathbold{M}_i\varepsilon_i\right)$$
立即可得$\E[\hb_{\text{FE}}|\X]=\beta$, 以及\footnote{夹心估计量的中间部分实际为$\sum_{i=1}^{N}\dot{X}_i'\E[\dot{\varepsilon}_i\dot{\varepsilon}_i'|X_i]\dot{X}_i$, 但它在代数上等同于$\sum_{i=1}^{N}\dot{X}_i'\E[\varepsilon_i\varepsilon_i'|X_i]\dot{X}_i$.}
\begin{equation}\label{eq8.19}
  \V_{\text{FE}}=\left(\sum_{i=1}^{N}\dot{X}_i'\dot{X}_i\right)^{-1}\left(\sum_{i=1}^{N}\dot{X}_i'\mathbold{\Sigma}_i\dot{X}_i\right)\left(\sum_{i=1}^{N}\dot{X}_i'\dot{X}_i\right)^{-1}
\end{equation}
其中$\mathbold{\Sigma}=\E[\varepsilon_i\varepsilon_i'|X_i]$. 它的完全稳健估计量为
\begin{equation}\label{eq8.34}
  \hat{\V}_{\text{FE}}=(\dot{\X}'\dot{\X})^{-1}\left(\sum_{i=1}^{N}\dot{X}_i'\hat{\varepsilon}_i\hat{\varepsilon}_i'\dot{X}_i\right)\left(\dot{\X}'\dot{\X}\right)^{-1}
\end{equation}

如果特质误差$\{\varepsilon_{it}\}$满足条件同方差和序列无关
\begin{align}
\E[\varepsilon_{it}^2|X_i]&=\sigma_\varepsilon^2 \label{eq8.20} \\
\E[\varepsilon_{it}\varepsilon_{is}|X_i]&=0 \label{eq8.21}
\end{align}
这里$t\neq s$. 此时(\ref{eq8.19})简化为
$$\V_{\text{FE}}^0=\sigma_\varepsilon^2\left(\sum_{i=1}^{N}\dot{X}_i'\dot{X}_i\right)^{-1}$$
容易证明, 如果(\ref{eq8.20})和(\ref{eq8.21})对混合回归成立, 那么
$$\V_{\text{FE}}^0=\sigma_\varepsilon^2\left(\sum_{i=1}^{N}\dot{X}_i'\dot{X}_i\right)\geq \sigma_\varepsilon^2\left(\sum_{i=1}^{N}X_i'X_i\right)=\V_{\text{POOL}}$$
表明使用更稳健的FE估计的代价是降低了估计量的有效性, 因为在(\ref{eq8.20})和(\ref{eq8.21})下, 混合OLS会利用更多信息. 最后, $\V_{\text{FE}}^0$的估计量可以表述为
\begin{equation}\label{eq8.35}
  \hat{\V}_{\text{FE}}^0=\hat{\sigma}_\varepsilon^2(\dot{\X}'\dot{\X})^{-1}
\end{equation}
其中
\begin{equation}\label{eq8.24}
  \hat{\sigma}_\varepsilon^2=\frac{1}{N(T-1)-K}\sum_{i=1}^{N}\hat{\varepsilon}_i'\hat{\varepsilon}_i
\end{equation}
这里的$\hat{\varepsilon}_i=\dot{Y}_i-\dot{X}_i\hb_{\text{FE}}$为固定效应残差.
\subsection{虚拟变量回归}
另一种得到FE估计量的方法是使用$Y_{it}$对$X_{it}$以及许多虚拟变量进行最小二乘回归. 首先来看没有回归元的方程
\begin{equation}\label{eq8.29}
  Y_{it}=u_i+\varepsilon_{it}
\end{equation}
考虑对固定效应向量$u=[u_1,\cdots,u_N]'$的OLS估计量$\hat{u}$, 由于数据矩阵是$\mathbf{1}_{T}$, 易知$\hat{u}_i=\overbar{Y}_i$, 以及OLS残差$\hat{\varepsilon}_{it}=Y_{it}-\overbar{Y}_i=\dot{Y}_{it}$.

再令$d_i$为$N$个虚拟变量构成的向量, 它的第$i$个元素为1而其它元素为0. 注意到$u_i=d_i'u$, 于是(\ref{eq8.29})变为$Y_{it}=d_i'u+\varepsilon_{it}$, 可以将其写为堆叠形式$Y_i=\mathbf{1}_id_i'u+\varepsilon_i$, 以及$Y=\mathbold{D}u+\varepsilon$, 其中$\mathbold{D}=\text{diag}\,\{\mathbf{1}_T,\cdots,\mathbf{1}_T\}$. 由此得到OLS估计量
\begin{align*}
\hat{u}&=(\mathbold{D}'\mathbold{D})^{-1}\mathbold{D}'Y=\text{vec}(\overbar{Y}_i)
\end{align*}
于是可得$NT\times 1$维残差向量
$$\hat{\varepsilon}=[\mathbold{I}_{NT}-\mathbold{D}(\mathbold{D}'\mathbold{D})^{-1}\mathbold{D}']Y=\dot{Y}$$
类似地, $\X$对$u$回归得到的残差为$\dot{\X}$.

最后来看带回归元的单向误差成分模型
$$Y_{it}=X_{it}'\beta+d_i'u+\varepsilon_{it}$$
它的矩阵形式为
$$Y=\X\beta+\mathbold{D}u+\varepsilon$$
现在考虑$(\beta,u)$的最小二乘估计$(\hb,\hat{u})$, 称为固定效应模型的最小二乘虚拟变量 (Least Squares Dummy Variable, LSDV)估计量. 根据FWL定理, LSDV估计量$\hb_{\text{LSDV}}$可以通过残差$\dot{Y}$对$\dot{\X}$回归得到, 也即$\hb_{\text{LSDV}}=\hb_{\text{FE}}$.

当$N$相当大时, 最好使用组内估计而非LSDV方法, 这是因为后者的计算需要消耗大量计算力, 例如在$T=10$和$N=10000$的情况下, 矩阵$\mathbold{D}$有10亿个元素.

\subsection{一阶差分估计}
除了组内变换和虚拟变量回归, 另一种重要的方法为一阶差分 (first differencing). 具体而言, 一阶差分变换为
$$\Delta Y_{it}=Y_{it}-Y_{it-1}$$
也即用第$t$期的观测$Y_{it}$减去第$t-1$期的观测$Y_{it-1}$, 这对于$t=1$之外的观测都适用. 用矩阵可以表示为$\Delta Y_i=\mathbold{D}_iY_i$, 其中$\mathbold{D}_i$是一个$(T-1)\times T$维矩阵差分算子
$$\mathbold{D}_i=\begin{bmatrix}
                   -1 & 1 & 0 & \cdots & 0 & 0 \\
                   0 & -1 & 1 & \cdots & 0 & 0 \\
                   \vdots & \vdots & \vdots &  & \vdots & \vdots \\
                   0 & 0 & 0 & \cdots & -1 & 1
                 \end{bmatrix}$$
对(\ref{eq8.15})和(\ref{eq8.16})实施差分变换得到
$$\Delta Y_{it}=\Delta X_{it}'\beta+\Delta \varepsilon_{it}$$
以及
\begin{equation}\label{eq8.27}
  \Delta Y_i=\Delta X_i\beta+\Delta \varepsilon_i
\end{equation}
经过差分变换后, 不可观测的$u_i$已经被消去.

现在对回归方程(\ref{eq8.27})应用最小二乘可得
\begin{align}
\hb_{\text{FD}}&=\left(\sum_{i=1}^{N}\sum_{t\geq2}\Delta X_{it}\Delta X_{it}'\right)^{-1}\left(\sum_{i=1}^{N}\sum_{t\geq2}\Delta X_{it}\Delta Y_{it}\right) \nonumber \\
&=\left(\sum_{i=1}^{N}\Delta X_i'\Delta X_i\right)^{-1}\left(\sum_{i=1}^{N}\Delta X_i'\Delta Y_i\right) \nonumber \\
&=\left(\sum_{i=1}^{N}X_i'\mathbold{D}_i'\mathbold{D}_iX_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\mathbold{D}_i'\mathbold{D}_iY_i\right) \label{eq8.28}
\end{align}
我们称(\ref{eq8.28})为一阶差分估计量 (first-difference estimator). 此外, 当$T=2$时有$\hb_{\text{FD}}=\hb_{\text{FE}}$, 这是因为
$$\mathbold{D}_i'\mathbold{D}_i=2\mathbold{M}_i=\begin{bmatrix}
                                                  1 & -1 \\
                                                  -1 & 1
                                                \end{bmatrix}$$

当特质误差$\{\varepsilon_{it}\}$是条件同方差的且序列无关, 那么$\Delta\varepsilon_i=\mathbold{D}_i\varepsilon_i$具有协方差矩阵$\sigma_\varepsilon^2\mathbold{H}$, 其中$\mathbold{H}=\mathbold{D}_i'\mathbold{D}_i$. 另一方面, 此时我们可以用GLS来估计方程(\ref{eq8.27})并得到
\begin{align*}
\tilde{\beta}_{\text{FD}}&=\left(\sum_{i=1}^{N}\Delta X_i'\mathbold{H}^{-1}\Delta X_i\right)^{-1}\left(\sum_{i=1}^{N}\Delta X_i'\mathbold{H}^{-1}\Delta Y_i\right) \\
&=\left[\sum_{i=1}^{N}X_i'\mathbold{D}_i'(\mathbold{D}_i\mathbold{D}_i')^{-1}\mathbold{D}_iX_i\right]^{-1}\left[\sum_{i=1}^{N}X_i'\mathbold{D}_i'(\mathbold{D}_i\mathbold{D}_i')^{-1}\mathbold{D}_iY_i\right] \\
&=\left(\sum_{i=1}^{N}X_i'\mathbold{M}_iX_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\mathbold{M}_iY_i\right)
\end{align*}
其中$\mathbold{M}_i=\mathbold{D}_i'(\mathbold{D}_i\mathbold{D}_i')^{-1}\mathbold{D}_i$, 这可以由线性代数直接验证.

也就是说, 如果特质误差是i.i.d.的, 那么对一阶差分方程使用GLS方法得到的估计量是FE估计量. 由于Gauss-Markov定理表明GLS估计量具有渐近有效的方差, 故而在i.i.d.特质误差下, FE估计量比FD估计量更有效.

\subsection{组间估计}
所谓的组间估计量 (between estimator)来源于均值化的回归方程
\begin{equation}\label{eq8.22}
  \overbar{Y}_i=\overbar{X}_i\beta+u_i+\overbar{\varepsilon}_i
\end{equation}
对(\ref{eq8.22}施加最小二乘可得
\begin{align*}
\hat{\beta}_{\text{BE}}=\left(\sum_{i=1}^{N}\overbar{X}_i\overbar{X}_i'\right)^{-1}\left(\sum_{i=1}^{N}\overbar{X}_i\overbar{Y}_i\right)
\end{align*}


在假设\ref{pro:pro8.1}下, $\hb_{\text{BE}}$是$\beta$的无偏估计量, 并且它的方差为
$$\V_{\text{BE}}=\sigma^2\left(\sum_{i=1}^{N}\overbar{X}_i\overbar{X}_i'\right)^{-1}$$
其中
$$\sigma^2=\text{var}(u_i+\overbar{\varepsilon}_i)=\sigma_u^2+\frac{\sigma^2_\varepsilon}{T}$$
是回归(\ref{eq8.22})的误差方差. 然而, $\hb_{\text{BE}}$的有效性不及由FGLS估计得到的$\hb_{\text{RE}}$. 此外, 如果$u_i$和$X_i$任意相关, 那么BE估计量必然不是一致的.

根据上述讨论, 我们似乎没有直接使用组间估计的动机, 但组间估计对于构造$\sigma_u^2$的估计量很有用. 首先考虑
\begin{equation}\label{eq8.25}
  \sigma_b^2=N^{-1}\sum_{i=1}^{N}\sigma_i^2=\sigma_u^2+\frac{\sigma^2_\varepsilon}{T}
\end{equation}
它的估计量可以很自然地考虑为
\begin{equation}\label{eq8.23}
  \hat{\sigma}_b^2=\frac{1}{N-K}\sum_{i=1}^{N}\hat{e}_{bi}^2
\end{equation}
其中$\hat{e}_{bi}=\overbar{Y}_i-\overbar{X}_i'\hb_{\text{BE}}$为组间残差.

我们在之前已经得到了$\hat{\sigma}_\varepsilon^2$的表达式(\ref{eq8.24}), 既然它在条件更弱的FE估计中适用, 那么它也在RE估计中适用. 基于(\ref{eq8.25})可得$\sigma_u^2$的估计量
$$\hat{\sigma}_u^2=\hat{\sigma}_b^2-\frac{\hat{\sigma}_\varepsilon^2}{T}$$
因为它可能是负的, 我们通常使用受限的估计量
\begin{equation}\label{eq8.26}
  \hat{\sigma}_u^2=\max\,\left\{0,\hat{\sigma}_b^2-\frac{\hat{\sigma}_\varepsilon^2}{T}\right\}
\end{equation}
由此得到的估计量$\hat{\sigma}_\varepsilon^2$和$\hat{\sigma}_u^2$可用于构建RE估计量中的$\hat{\BO}$.
\section{广义离差模型}
以上章节介绍了POOL估计量、RE估计量和FE估计量, 实际上三者都可以使用OLS对广义离差模型 (quasi-demeaned model)进行估计得到.

考虑具有随机效应结构的综合误差$e_i$的协方差矩阵
$$\BO=\begin{bmatrix}
   \sigma_u^2+\sigma_\varepsilon^2 & \sigma_u^2 & \cdots & \sigma_u^2 \\
   \sigma_u^2 & \sigma_u^2+\sigma_\varepsilon^2 & \cdots & \sigma_u^2 \\
   \vdots & \vdots &  & \vdots \\
   \sigma_u^2 & \sigma_u^2 & \cdots & \sigma_u^2+\sigma_\varepsilon^2
 \end{bmatrix}$$
定义矩阵$\mathbold{P}_i=\mathbf{1}_i(\mathbf{1}_i'\mathbf{1}_i)^{-1}\mathbf{1}_i'$, 于是$\mathbold{M}_i=\mathbold{I}_i-\mathbold{P}_i$以及$\mathbold{P}_iY_i=\mathbf{1}_i\overbar{Y}_i$, 此时
$$\BO=\sigma_\varepsilon^2\mathbold{I}_i+\sigma_u^2\mathbf{1}_i\mathbf{1}_i'=\sigma_\varepsilon^2\left(\mathbold{I}_i+\frac{\sigma_u^2T}{\sigma_\varepsilon^2}\mathbold{P}_i\right)=\sigma_\varepsilon^2(\mathbold{M}_i+\rho^{-2}\mathbold{P}_i)$$
其中
\begin{equation}\label{eq8.30}
  \rho=\frac{\sigma_\varepsilon}{\sqrt{\sigma_\varepsilon^2+\sigma_u^2T}}
\end{equation}
于是$\BO^{-1}=\sigma_\varepsilon^{-2}(\M_i+\rho^2\mathbold{P}_i)$, 并且
$$\BO^{-\frac{1}{2}}=\sigma_\varepsilon^{-1}(\M_i+\rho\mathbold{P}_i)=\sigma_\varepsilon^{-1}[\mathbold{I}_i-(1-\rho)\mathbold{P}_i]$$

再令$\tilde{X}_i=\BO^{-\frac{1}{2}}X_i$, $\tilde{Y}_i=\BO^{-\frac{1}{2}}Y_i$, 于是GLS估计量为
\begin{align}
\hb_{\text{GLS}}&=\left(\sum_{i=1}^{N}X_i'\BO^{-1}X_i\right)^{-1}\left(\sum_{i=1}^{N}X_i'\BO^{-1}Y_i\right) \nonumber \\
&=\left(\sum_{i=1}^{N}\tilde{X}_i'\tilde{X}_i\right)^{-1}\left(\sum_{i=1}^{N}\tilde{X}_i'\tilde{Y}_i\right) \label{eq8.31}
\end{align}
其中
\begin{equation}\label{eq8.32}
  \tilde{Y}_i=\sigma_\varepsilon^{-1}[Y_i-(1-\rho)\mathbf{1}_i\overbar{Y}_i]
\end{equation}
同理可得
\begin{align}
\tilde{X}_i&=\sigma_\varepsilon^{-1}[X_i-(1-\rho)\mathbf{1}_i\overbar{X}_i'] \label{eq8.33} \\
\tilde{e}_i&=\sigma_\varepsilon^{-1}[e_i-(1-\rho)\mathbf{1}_i\overbar{e}_i] \label{eq8.39}
\end{align}
然而它们是不可行的, 因为$\rho$不可观测.

为了得到FGLS估计量, 考虑使用估计量$\hat{\sigma}_\varepsilon^2$和$\hat{\sigma}_u^2$分别替代(\ref{eq8.30})中的$\sigma_\varepsilon^2$和$\sigma_u^2$, 得到估计量
$$\hat{\rho}=\frac{\hat{\sigma}_\varepsilon}{\sqrt{\hat{\sigma}_\varepsilon^2+\hat{\sigma}_u^2T}}$$
以及
$$\hat{\theta}=1-\frac{\hat{\sigma}_\varepsilon}{\sqrt{\hat{\sigma}_\varepsilon^2+\hat{\sigma}_u^2T}}$$
将其代入到(\ref{eq8.32}), (\ref{eq8.33})和(\ref{eq8.39})中, 于是要估计的方程变为
$$Y_{it}-\hat{\theta}\overbar{Y}_i=(X_{it}-\hat{\theta}\overbar{X}_i)'\beta+e_{it}-\hat{\theta}\overbar{e}_i$$
通过对上式进行OLS回归来得到FGLS估计量$\hb_{\text{FGLS}}$.

根据之前的讨论, 如果$\hat{\theta}=0$, 那么有$\tilde{X}_i=X_i$以及$\tilde{Y}_i=Y_i$, 因此$\hb_{\text{FGLS}}=\hb_{\text{POOL}}$. 而当$\hat{\theta}=1$时有, $\tilde{X}_i=\dot{X}_i$以及$\tilde{Y}_i=\dot{Y}_i$, 故而$\hb_{\text{FGLS}}=\hb_{\text{FE}}$. 最后, 当$0<\hat{\theta}<1$时, $\hb_{\text{FGLS}}$就是RE估计量$\hb_\text{RE}$.
\section{FE的渐近性质}
之前的部分已经讨论了, 在假设\ref{pro:pro8.1}成立的条件下, RE估计量具有一致性, 本节主要讨论FE估计量的渐近性质, 为此先给出以下假设.
\begin{proposition}\label{pro:pro8.3}
(1) 回归模型为$Y_{it}=X_{it}'\beta+u_i+\varepsilon_{it}$, $i=1,\cdots,N$, $t=1,\cdots,T$且$T\geq2$.

(2) $(\varepsilon_i, X_i)$是i.i.d.的, 其中$i=1,\cdots,N$.

(3) 对所有的$s=1,\cdots,T$都有$\E[X_{is}\varepsilon_{it}]=0$.

(4) $\Q_T=\E[\dot{X}_i'\dot{X}_i]>0$.

(5) $\E[\varepsilon_{it}^4]<\infty$.

(6) $\E||X_{it}^4||<\infty$.
\end{proposition}
\begin{theorem}\label{thm:thm8.1}
  在假设\ref{pro:pro8.3}下, 当$N\to\infty$时有
  $$\sqrt{N}(\hb_{\text{FE}}-\beta)\xrightarrow{d}N(0,\V_\beta)$$
  其中$\V_\beta=\Q_T^{-1}\BO_T\Q^{-1}_T$, 以及$\BO_T=\E[\dot{X}_i'\varepsilon_i\varepsilon_i'\dot{X}_i]$.
\end{theorem}
\begin{proof}
  假设\ref{pro:pro8.2}意味着$(\dot{X}_i,\varepsilon_i)$也是i.i.d.的, 并且具有有限四阶矩, 根据WLLN可知
  $$N^{-1}\sum_{i=1}^{N}\dot{X}_i'\dot{X}_i\xrightarrow{p}\E[\dot{X}_i'\dot{X}_i]=\Q_T$$
  另一方面
  $$\E[\dot{X}_i'\varepsilon_i]=\sum_{t=1}^{T}\E[\dot{X}_{it}\varepsilon_{it}]=\sum_{t=1}^{T}\E[X_{it}\varepsilon_{it}]-\sum_{t=1}^{T}\sum_{j=1}^{T}\E[X_{ij}\varepsilon_{it}]=0$$
  于是由CLT可知
  $$N^{-\frac{1}{2}}\sum_{i=1}^{N}\dot{X}_i'\varepsilon_i\xrightarrow{d} N(0,\BO_T)$$
  其中假设\ref{pro:pro8.3}的第(5)点和第(6)点保证了使用CLT的前提条件成立. 进一步
  $$\sqrt{N}(\hb_{\text{FE}}-\beta)=\left(N^{-1}\sum_{i=1}^{N}\dot{X}_i'\dot{X}_i\right)^{-1}\left(N^{-\frac{1}{2}}\sum_{i=1}^{N}\dot{X}_i'\varepsilon_i\right)\xrightarrow{d} N(0,\V_\beta)$$
  证毕.
\end{proof}

之前已经介绍了$\V_{\text{FE}}$的完全稳健的估计量(\ref{eq8.34}), 以及在特质误差$\varepsilon_{it}$满足同方差和无序列相关时的估计量(\ref{eq8.35}). 现在来看$\{\varepsilon_{it}\}$仅存在异方差而无序列相关时的情形, 此时$\E[\varepsilon_{it}|X_i]=0$以及
\begin{equation}\label{eq8.36}
  \E[\varepsilon_{it}^2|X_i]=\sigma_{it}^2
\end{equation}
故而$\mathbold{\Sigma}_i=\E[\varepsilon_i\varepsilon_i'|X_i]=\text{diag}\,\{\sigma_{it}^2\}$, 因此协方差矩阵(\ref{eq8.19})变为
$$\V_{\text{FE}}=(\dot{\X}'\dot{\X})^{-1}\left(\sum_{i=1}^{N}\sum_{t=1}^{T}\dot{X}_{it}\dot{X}_{it}'\sigma_{it}^2\right)(\dot{\X}'\dot{\X})^{-1}$$
使用$\hat{\varepsilon}_{it}^2$替代上式中的$\sigma_{it}^2$, 再做一个自由度调整可得异方差稳健的协方差矩阵估计量
\begin{equation}\label{eq8.37}
  \hat{\V}_{\text{FE}}=\frac{NT}{N(T-1)-K}(\dot{\X}'\dot{\X})^{-1}\left(\sum_{i=1}^{N}\sum_{t=1}^{T}\dot{X}_{it}\dot{X}_{it}'\hat{\varepsilon}_{it}^2\right)(\dot{\X}'\dot{\X})^{-1}
\end{equation}
直觉上看, White (1980)形式的$\hat{\V}_{\text{FE}}$是$\V_{\text{FE}}$合理的估计量, 然而 Stock and Waston (2008)指出了这种错误.

考虑一种特殊情况, $\hat{\varepsilon}_{it}=\dot{Y}_{it}-\dot{X}_{it}'\beta$, 也即残差$\hat{\varepsilon}_{it}$通过真实参数$\beta$构成, 此时
$$\hat{\varepsilon}_{it}=\dot{\varepsilon}_{it}=\varepsilon_{it}-T^{-1}\sum_{j=1}^{T}\varepsilon_{ij}$$
在$\varepsilon_{it}$仅存在异方差而无自相关的情况下, 根据(\ref{eq8.21})和(\ref{eq8.36})可得
$$\E[\hat{\varepsilon}_{it}^2|X_i]=\left(\frac{T-2}{T}\right)\sigma_{it}^2+\frac{\sigma_i^2}{T}$$
其中$\sigma_i^2=T^{-1}\sum_{t=1}^{T}\sigma_{it}^2$. 假设$K=0$, 将上式代入到(\ref{eq8.37})得到
\begin{align*}
\E[\hat{\V}_{\text{FE}}|\X]&=\frac{T}{T-1}(\dot{\X}'\dot{\X})^{-1}\left(\sum_{i=1}^{N}\sum_{t=1}^{T}\dot{X}_{it}\dot{X}_{it}'\E[\hat{\varepsilon}_{it}^2|X_i]\right)(\dot{\X}'\dot{\X})^{-1} \\
&=\left(\frac{T-2}{T-1}\right)\V_{\text{FE}}+\frac{1}{T-1}(\dot{\X}'\dot{\X})^{-1}\left(\sum_{i=1}^{N}\dot{X}_{it}\dot{X}_{it}'\overbar{\sigma}_i^2\right)(\dot{\X}'\dot{\X})^{-1}
\end{align*}
当$T>2$且固定时, 随着$N\to\infty$, $\hat{\V}_{\text{FE}}$仍是有偏估计. Stock and Waston (2008)还表明了它不是$\V_{\text{FE}}$的一致估计量 (除非$T\to\infty$),  并且提出了
\begin{align*}
\tilde{\V}_{\text{FE}}&=\left(\frac{T-1}{T-2}\right)\hat{\V}_{\text{FE}}-\frac{1}{T-1}\hat{\mathbold{B}}_{\text{FE}} \\
\hat{\mathbold{B}}_{\text{FE}}&=(\dot{\X}'\dot{\X})^{-1}\left(\sum_{i=1}^{N}\dot{X}_i'\dot{X}_i\hat{\sigma}_i^2\right)(\dot{\X}'\dot{\X})^{-1} \\
\hat{\sigma}_i^2&=\frac{1}{T-1}\sum_{t=1}^{T}\hat{\varepsilon}_{it}^2
\end{align*}
这里的$\tilde{\V}_{\text{FE}}$是$\V_{\text{FE}}$的无偏估计, 并且在$N\to\infty$但$T$固定的情况下也是$\V_{\text{FE}}$的一致估计.

\begin{remark}
尽管$\hat{\V}_{\text{FE}}$在上述情况下不是一致估计量, 但如果$\{\varepsilon_{it}\}$满足条件同方差和无序列相关, 那么$\hat{\V}_{\text{FE}}^0$仍是一致估计. 此外, 对于异方差和序列相关完全稳健的$\hat{\V}_{\text{FE}}$也是一致的.
\end{remark}
\section{RE与FE的比较}
在选择RE和FE的关键考虑是, 不可观测的个体效应$u_i$和解释变量$X_{it}$是否相关. 如果二者是相关的, 那么FE估计量是一致的, 而RE估计量不一致. 如果二者不相关, 那么RE估计量同样是一致的, 并且渐近有效, 这是因为RE估计量是通过FGLS获得的.

为了比较这两种估计量, 考虑之前在工具变量章节中使用过的Hausman检验, 在此之前先介绍一个引理, 它仍然出自于 Hausman (1978).
\begin{lemma}\label{lem:lem8.1}
设$\hb_0$和$\hb_1$都是$\beta$的一致估计量, 并且都服从渐近正态分布
\begin{align*}
&\sqrt{N}(\hb_0-\beta)\xrightarrow{d} N(0,\V_0) \\
&\sqrt{N}(\hb_1-\beta)\xrightarrow{d} N(0,\V_1)
\end{align*}
定义$\hat{q}=\hb_0-\hb_1$, 如果$\hb_0$是渐近有效的, 那么$\sqrt{N}(\hb_0-\beta)$和$\sqrt{N}\hat{q}$的极限分布的协方差为0, 也即$\text{cov}(\hb_0,\hat{q})=0$.
\end{lemma}
\begin{proof}
假设$\mathbold{C}=\text{cov}(\hb_0,\hat{q})\neq0$, 定义估计量$\hb_2=\hb_0+r\mathbold{A}\hat{q}$, 其中$r$是一个标量, $\mathbold{A}$为任意$K\times K$维矩阵, 于是
$$\var(\hb_2)=\var(\hb_0)+r\mathbold{AC}+r\mathbold{C}'\mathbold{A}'+r^2\mathbold{A}\var(\hat{q})\mathbold{A}'$$
设函数
$$F(r)=\var(\hb_2)-\var(\hb_0)=r\mathbold{AC}+r\mathbold{C}'\mathbold{A}'+r^2\mathbold{A}\var(\hat{q})\mathbold{A}'$$
它的一阶导数为
$$F'(r)=\mathbold{AC}+\mathbold{C}'\mathbold{A}'+2r\mathbold{A}\var(\hat{q})\mathbold{A}'$$
选取$\mathbold{A}=-\mathbold{C}'$, 注意到协方差矩阵$\mathbold{C}$是对称的, 因此
$$F'(r)=-2\mathbold{C}'\mathbold{C}+2r\mathbold{C}'\var(\hat{q})\mathbold{C}$$
当$r=0$时, $F'(0)=-2\mathbold{C}'\mathbold{C}$为半负定矩阵, 又因为$F(0)=0$, 因此存在某个充分小的$r>0$, 使得$F(r)<0$. 但由于$\hb_0$是渐近有效的, 故而$F(r)\geq0$, 产生矛盾, 因此必有$\mathbold{C}=0$.
\end{proof}
\begin{theorem}\label{thm:thm8.2}
  在假设\ref{pro:pro8.1}和秩条件$\text{rank}\,(\E[\dot{X}_i'\dot{X}_i])=K$成立的条件下, 当$N\to\infty$时有
  $$H=(\hb_{\text{FE}}-\hb_{\text{RE}})'(\hat{\V}_{\text{FE}}-\hat{\V}_{\text{RE}})^{-1}(\hb_{\text{FE}}-\hb_{\text{RE}})\xrightarrow{d}\chi_{K}^2$$
\end{theorem}
\begin{proof}
  当$N\to\infty$时有$\sqrt{N}(\hb_{\text{FE}}-\hb_{\text{RE}})\xrightarrow{d}N(0,\V)$, 其中
  $$\V/n=\text{avar}(\hb_{\text{FE}})+\text{avar}(\hb_{\text{RE}})-\text{cov}(\hb_{\text{FE}},\hb_{\text{RE}})-\text{cov}(\hb_{\text{RE}},\hb_{\text{FE}})$$
  由于在假设\ref{pro:pro8.1}下, RE估计量是渐近有效的, 因此根据引理\ref{lem:lem8.1}可知
  $$\text{cov}(\hb_{\text{FE}}-\hb_\text{RE},\hb_\text{RE})=\text{cov}(\hb_{\text{FE}},\hb_\text{RE})-\text{avar}(\hb_\text{RE})=0$$
  从而$\V=n\V_{\text{FE}}-n\V_{\text{RE}}$. 根据Wald检验原理可知
  \begin{align*}
  H&=n(\hb_{\text{FE}}-\hb_{\text{RE}})'\hat{\V}^{-1}(\hb_{\text{FE}}-\hb_{\text{RE}}) \\
  &=(\hb_{\text{FE}}-\hb_{\text{RE}})'(\hat{\V}_{\text{FE}}-\hat{\V}_{\text{RE}})^{-1}(\hb_{\text{FE}}-\hb_{\text{RE}})
  \end{align*}
  因此当$N\to\infty$时有$H\xrightarrow{d}\chi^2_K$.

\end{proof}

\begin{remark}
下面给出关于Hausman检验的一些评注.
\begin{itemize}
  \item Hausman检验只在假设\ref{pro:pro8.1}完全成立的情况下才能使用, 也即特质误差$\{\varepsilon_{it}\}$不能存在异方差和序列相关, 否则$\hb_{\text{RE}}$不再是渐近有效的估计量, 导致无法使用引理\ref{lem:lem8.1}构造$H$统计量.
  \item 此外, 由于组内变换只能估计时变解释变量的回归系数, 而RE方法没有这个限制, 因此$\hb_{\text{RE}}$的维数可能多于$\hb_{\text{FE}}$的维数, 此时Hausman检验只能对时变的回归元上进行.

  \item 有时候, Stata之类的计量软件会给出负的$H$统计量, 这可能是由于对$\hat{\sigma}_\varepsilon^2$的不同估计导致的, 此时$H$统计量中间的那个矩阵不一定正定, 因此最好在$\hat{\V}_{\text{FE}}$和$\hat{\V}_{\text{RE}}$中都使用同一个估计量$\hat{\sigma}_\varepsilon^2$.
\end{itemize}
\end{remark}
如果特质误差$\varepsilon_{it}$存在条件异方差, 那么可以考虑广义离差模型
$$Y_{it}-\hat{\theta}\overbar{Y}_i=(X_{it}-\hat{\theta}\overbar{X}_i)'\beta+(X_{it}-\overbar{X}_i)'\gamma+e_{it}-\hat{\theta}\overbar{e}_i$$
对上式进行OLS估计后可以获得聚类稳健协方差矩阵估计量, 然后利用Wald检验原理即可检验原假设$\HH_0: \gamma=0$, 倘若我们拒绝原假设, 则应该拒绝随机效应.

目前的计量分析更倾向于稳健性而非有效性, 通常在线性面板数据模型中直接使用FE分析, 而只有在非线性模型中才使用RE分析, 因为在非线性模型中通常难以估计出$\hb_{\text{FE}}$.
\section{双向误差成分}
之前讨论的面板数据模型仅将综合误差$e_{it}$分解为个体效应$u_i$和特质误差$\varepsilon_{it}$, 然而实际上可能存在仅随时间变化而不随个体变化的不可观测效应, 由此给出双向误差成分模型 (two-way error component model)
\begin{equation}\label{eq8.38}
  Y_{it}=X_{it}'\beta+u_i+v_t+\varepsilon_{it}
\end{equation}
其中$u_i$是不可观测的个体效应, $v_t$是不可观测的时间效应, $\varepsilon_{it}$为特质误差项. 模型(\ref{eq8.38})既可以使用RE估计也可以使用FE估计.

在随机效应框架下, 需要将假设\ref{pro:pro8.1}拓展到$v_t$上, 由于$e=v\otimes \mathbf{1}_N+\mathbf{1}_T\otimes u+\varepsilon$, 于是
$$\BO=(\mathbf{1}_T\otimes \mathbf{1}_N\mathbf{1}_N')\sigma_v^2+(\mathbf{1}_T\mathbf{1}_T'\otimes\mathbf{1}_N)\sigma_u^2+\sigma_\varepsilon^2\mathbf{I}_{NT}$$
它可以在GLS中被用于估计$\beta$.

在固定效应框架下, 可以使用双向组内变换
$$\ddot{Y}_{it}=Y_{it}-\overbar{Y}_i-\tilde{Y}_t+\overbar{Y}$$
其中
$$\tilde{Y}_t=N^{-1}\sum_{i=1}^{N}Y_{it},\quad \overbar{Y}=(NT)^{-1}\sum_{i=1}^{N}\sum_{t=1}^{T}Y_{it}$$
类似可以定义$\ddot{X}_{it}=X_{it}-\overbar{X}_i-\tilde{X}_t+\overbar{X}$. 如果$Y_{it}$满足(\ref{eq8.38}), 那么
\begin{align*}
\overbar{Y}_i&=\OX_i'\beta+u_i+\overbar{v}+\overbar{\varepsilon}_i \\
\tilde{Y}_t&=\tilde{X}_t'\beta+\overbar{u}+v_t+\tilde{\varepsilon}_t \\
\overbar{Y}&=\OX'\beta+\overbar{u}+\overbar{v}+\overbar{\varepsilon}
\end{align*}
从而
$$\ddot{Y}_{it}=\ddot{X}_{it}'\beta+\ddot{\varepsilon}_{it}$$
\nocite{abadie2023should}
对上式使用最小二乘即可得到出双向组内估计量.

除了使用双向组内变换外, 还可以使用LSDV方法来估计(\ref{eq8.38}). 设$\tau_t$为$T\times1$维向量, 它的第$t$个元素为1, 而其它元素为0, 再定义时间固定效应向量$v=[v_1,\cdots,v_T]'$, 于是双向成分模型可以写为
$$Y_{it}=X_{it}'\beta+u_i+\tau_t'v+\varepsilon_{it}$$
注意, 为了使得模型可以识别, 避免完全多重共线性, 需要在$\tau_t$中剔除某个基期时间虚拟变量, 然后使用组内估计即可.

如果使用了双向组内变换, 那么$X_{it}$中就不能包括任意非时变的回归元$X_i$, 以及任意时间序列变量$X_t$, 它们在双向组内变换过程中都会被消去. 由于出自(\ref{eq8.38})的固定效应估计量关于$u_i$和$v_t$是不变的, 因此无需新增假设即可分析其渐近性质.
\section{工具变量}
考虑固定效应模型
\begin{equation}\label{eq8.40}
  Y_{it}=X_{it}'\beta+u_i+\varepsilon_{it}
\end{equation}
我们称$X_{it}$是内生的, 如果$\E[X_{it}X_{is}]\neq0$, 这将导致$\hb_{\text{FE}}$是结构参数$\beta$的非一致估计量, 我们可以使用工具变量解决内生性问题.

首先令$Z_{it}$为$L\times 1$维工具向量, 并且$L\geq K$. 在截面数据的情况下, $Z_{it}$既包括$X_{it}$的外生部分, 又包括来自$X_{it}$外的排他性外生变量. 然后再定义$Z_i$为在个体$i$上堆叠起来的$T\times L$维工具矩阵, $\mathbold{Z}$为全样本堆叠起来的工具矩阵. 结构参数$\beta$的识别仍依赖于矩条件$\text{rank}\,(\E[\dot{Z}_i'\dot{X}_i])=K$, 以及排除完全多重共线性所需要的$\E[\dot{Z}_i'\dot{Z}_i]>0$.

再来考虑如下有关固定效应的虚拟变量回归
$$Y_{it}=X_{it}'\beta+d_i'u+\varepsilon_{it}$$
其中$d_i$为$N\times1$维虚拟变量向量, 它的全样本形式为
\begin{equation}\label{eq8.41}
  Y=\X\beta+\mathbold{D}u+\varepsilon
\end{equation}
根据之前的讨论, 固定效应估计量$\hb_{\text{FE}}$可以由对上式的OLS回归获得, 并且还可以将$\mathbold{D}$视为外生变量. 于是, 固定效应模型(\ref{eq8.40})的2SLS估计在代数上, 等于在(\ref{eq8.41})中使用$[\mathbold{Z},\mathbold{D}]$作为工具的$Y$对$[\X,\mathbold{D}]$的2SLS估计. 根据(\ref{eq6.34})可知
\begin{equation}\label{eq8.42}
  \hb_\text{FE2SLS}=[\X'\M_D\Z(\Z'\M_D\Z)^{-1}\Z'\M_D\X]^{-1}\X'\M_D\Z(\Z'\M_D\Z)^{-1}\Z'\M_DY
\end{equation}
其中$\M_D=\mathbold{I}_{NT}-\mathbold{D}(\mathbold{D}'\mathbold{D})^{-1}\mathbold{D}'$. 注意到$\M_DY=\dot{Y}$, $\M_D\X=\dot{\X}$, 以及$\M_D\Z=\dot{\Z}$, 于是
$$\hb_{\text{FE2SLS}}=[\dot{\X}'\dot{\Z}(\dot{\Z}'\dot{\Z})^{-1}\dot{\Z}'\dot{\X}]^{-1}\dot{\X}'\dot{\Z}(\dot{\Z}'\dot{\Z})^{-1}\dot{\Z}'\dot{Y}$$
如果是双向固定效应模型
$$Y_{it}=X_{it}'\beta+u_i+v_t+\varepsilon_{it}$$
只需要添加$T-1$个时间虚拟变量到回归中, 然后将所有的虚拟变量组合成前面提到的矩阵$\mathbold{D}$, 然后利用(\ref{eq8.42})即可得到2SLS估计量.

为了得到FE2SLS估计量的渐近性质, 这里给出一系列正则条件.
\begin{proposition}\label{pro:pro8.4}
(1) 回归模型为$Y_{it}=X_{it}'\beta+u_i+\varepsilon_{it}$, $i=1,\cdots,N$, $t=1,\cdots,T$且$T\geq2$.

(2) $(\varepsilon_i, X_i, Z_i)$是i.i.d.的, 其中$i=1,\cdots,N$.

(3) 对所有的$s=1,\cdots,T$都有$\E[Z_{is}\varepsilon_{it}]=0$.

(4) $\Q_{ZZ}=\E[\dot{Z}_i'\dot{Z}_i]>0$.

(5) $\text{rank}\,(\Q_{ZX})=K$, 其中$\Q_{ZX}=\E[\dot{Z}_i'\dot{X}_i]$.

(6) $\E[\varepsilon_{it}^4]<\infty$.

(7) $\E||X_{it}^4||<\infty$.

(8) $\E||Z_{it}^4||<\infty$.
\end{proposition}
\begin{theorem}
  在假设\ref{pro:pro8.4}下, 当$N\to\infty$时有$\sqrt{N}(\hb_\text{FE2SLS}-\beta)\xrightarrow{d} N(0,\V_\beta)$, 其中
  \begin{align*}
  \V_\beta=(\Q_{ZX}'\Q_{ZZ}^{-1}\Q_{ZX})^{-1}(\Q_{ZX}'\Q_{ZZ}^{-1}\BO_{Z\varepsilon}\Q_{ZZ}^{-1}\Q_{ZX})(\Q_{ZX}'\Q_{ZZ}^{-1}\Q_{ZX})^{-1}
  \end{align*}
  以及$\BO_{Z\varepsilon}=\E[\dot{Z}_i'\varepsilon_i\varepsilon_i'\dot{Z}_i]$.
\end{theorem}
它的证明与定理\ref{thm:thm8.1}的类似, 在此略过. 如果特质误差$\{\varepsilon_{it}\}$满足条件同方差和无序列相关, 那么可以将$\V_\beta$简化为
$$\V_\beta=\sigma_\varepsilon^2(\Q_{ZX}'\Q_{ZZ}^{-1}\Q_{ZX})^{-1}$$
遵循标准步骤, $\hb_{\text{FE2SLS}}$的完全稳健协方差矩阵估计量为
\begin{align*}
\hat{\V}_\text{FE2SLS}&=[\dot{\X}'\dot{\Z}(\dot{\Z}'\dot{\Z})^{-1}\dot{\Z}'\dot{\X}]^{-1}\dot{\X}'\dot{\Z}(\dot{\Z}'\dot{\Z})^{-1}\left(\sum_{i=1}^{N}\dot{Z}_i'\hat{\varepsilon}_i\hat{\varepsilon}_i'\dot{Z}_i\right)^{-1} \\
&\quad\times (\dot{\Z}'\dot{\Z})^{-1}\dot{\Z}'\dot{\X}[\dot{\X}'\dot{\Z}(\dot{\Z}'\dot{\Z})^{-1}\dot{\Z}'\dot{\X}]^{-1}
\end{align*}
其中$\hat{\varepsilon}_i=\dot{Y}_i-\dot{X}_i\hb_\text{FE2SLS}$为FE2SLS残差. 根据Stock and Waston (2008)的分析, 仍不建议使用异方差稳健的协方差矩阵估计量, 特别是当$T$很小的时候.

\section{Hausman-Taylor模型}
FE分析无法囊括非时变的回归元, 而RE分析尽管可以将其纳入, 但所需条件太强, Hausman and Taylor (1981)提出了介于二者之间的模型, 该模型关于时变变量的假设与FE的假设相同, 而对于非时变变量的假设则更强一些. 考虑模型
$$Y_{it}=X_{1it}'\beta_1+X_{2it}'\beta_2+Z_{1i}'\gamma_1+Z_{2i}'\gamma_2+u_i+\varepsilon_{it}$$
其中$X_{1it}$和$X_{2it}$为时变变量, $Z_{1i}$和$Z_{2i}$为非时变变量. 回归元$X_{1it}$, $X_{2it}$, $Z_{1i}$, $Z_{2i}$的维数分别为$K_1$, $K_2$, $L_1$, $L_2$. 上述模型的全样本形式为
\begin{equation}\label{eq8.43}
  Y=\X_1\beta_1+\X_2\beta_2+\Z_1\gamma_1+\Z_2\gamma_2+u+\varepsilon
\end{equation}
再设$\overbar{\X}_1$和$\overbar{\X}_2$表示在个体上的平均, $\dot{\X}_1=\X_1-\overbar{\X}_1$和$\dot{\X}_2=\X_2-\overbar{\X}_2$为组内变换.

Hausman-Taylor模型假设所有回归元在每个时间$t$上和特质误差$\varepsilon_{it}$无关, $X_{1it}$和$Z_{1i}$关于个体效应$u_i$是外生的, 也即
\begin{align*}
\E[X_{1it}u_i]&=0 \\
\E[Z_{1i}u_i]&=0
\end{align*}
而回归元$X_{2it}$和$Z_{2i}$可以与个体效应$u_i$产生相关性. 定义矩阵$\X=[\X_1,\X_2,\Z_1,\Z_2]$, $\beta=[\beta_1',\beta_2',\gamma_1',\gamma_2']'$, 可以将以上假设写为总体矩条件
\begin{align*}
\E[\dot{\X}_1'(Y-\X\beta)]&=0 \\
\E[\dot{\X}_2'(Y-\X\beta)]&=0 \\
\E[\overbar{\X}_1'(Y-\X\beta)]&=0 \\
\E[\Z_1'(Y-\X\beta)]&=0
\end{align*}
这里的矩条件的个数为$2K_1+K_2+l_1$, 而回归系数共计$K_1+K_2+l_1+l_2$个, 因此为了模型可以识别, 必须保证$K_1\geq l_2$.

由于模型将$\X_1$和$\Z_1$视为外生的, 而将$\X_2$和$\Z_2$视为内生的, 故而可以使用$\dot{\X}_2$和$\X_1$作为它们的工具, 因此使用$\Z=[\dot{\X}_1,\dot{\X}_2,\overbar{\X}_1,\dot{\Z}_1]$为工具的2SLS方法即可估计出$\hb_{\text{2SLS}}$, 并且它是一致的.

Hausman and Taylor (1981)在更强的假设条件下获得估计量, 将$\varepsilon_{it}$设置为与$u_i$均值独立, $\BO_i$具有随机效应结构$\BO_i=\sigma_\varepsilon^2\mathbf{I}_n+\sigma_u^2\mathbf{1}_i\mathbf{1}_i'$, 并且$\BO=\mathbold{I}_N\otimes \BO_i$.

首先做组内变换得到
$$\dot{Y}_{it}=\dot{X}_{1it}'\beta_1+\dot{X}_{2it}'\beta_2+\dot{\varepsilon}_{it}$$
取得$[\beta_1',\beta_2']'$的固定效应估计量$[\hb_{\text{FE1}}',\hb_{\text{FE2}}']'$, 以及残差$\hat{\varepsilon}_{i}=\dot{Y}_{i}-\dot{X}_{1i}\hb_{\text{FE1}}-\dot{X}_{2i}\hb_\text{FE2}$, 并且可以构造$\sigma_\varepsilon^2$的一致估计量
$$\hat{\sigma}_\varepsilon^2=\frac{1}{N(T-1)}\sum_{i=1}^{N}\hat{\varepsilon}_i'\hat{\varepsilon}_i$$
然后定义
$$\tilde{\varepsilon}_i=Y_i-\overbar{X}_{1i}\hb_\text{FE1}-\overbar{X}_{2i}\hb_\text{FE2}$$
使用$\tilde{\varepsilon}_i$对$\Z_{1i}$和$\Z_{2i}$进行2SLS回归, 工具变量为$\X_1$和$\Z_1$, 由此得到估计量$\hat{\gamma}_{\text{IV1}}$和$\hat{\gamma}_\text{IV2}$. 定义
$$\check{\varepsilon}_{it}=Y_{it}-X_{1it}'\hb_{\text{FE1}}-X_{2it}'\hb_\text{FE2}-Z_{1it}'\hat{\gamma}_{\text{IV1}}-Z_{2it}'\hat{\gamma}_{\text{IV2}}$$
以及
$$s^2=(NT)^{-1}\sum_{i=1}^{N}\sum_{t=1}^{T}\check{\varepsilon}_{it}^2$$
根据$\text{plim}\,s^2=\sigma_\varepsilon^2+\sigma_u^2T$就能得到估计量$\hat{\sigma}_u^2=s^2-\hat{\sigma}_\varepsilon^2/T$, 因此可以定义
$$\hat{\theta}_i=1-\frac{\hat{\sigma}_\varepsilon}{\sqrt{\hat{\sigma}_\varepsilon^2+\hat{\sigma}_u^2T}}$$
进一步定义
\begin{align*}
Y_{it}^\ast&=Y_{it}-\hat{\theta}_i\overbar{Y}_i \\
X_{it}^\ast&=X_{it}-\hat{\theta}_i\overbar{X}_i \\
Z_{it}&=[(X_{1it}-\overbar{X}_{1i})',(X_{2it}-\overbar{X}_{2i})',Z_{1i}',\overbar{X}_{1i}']' \\
X_{it}&=[X_{1it}',X_{2it}',Z_{1i}',Z_{2i}']'
\end{align*}
通过工具变量$Z_{it}$, 实施$Y_{it}^\ast$对$X_{it}^\ast$的2SLS回归, 即可获得Hausman-Taylor估计量$\hb_{\text{HT}}$. 最后定义全样本数据矩阵$Y^\ast$, $\X^\ast$, 以及$\Z$, 那么
$$\hb_{\text{HT}}=[\X^\ast{'}\Z(\Z'\Z)^{-1}\Z{'}\X^\ast]^{-1}\X^\ast{'}\Z(\Z'\Z)^{-1}\Z{'}Y^\ast$$
可以证明, 当模型恰好识别的时候, HT估计量等同于上述提到的2SLS估计量. 而当模型过度识别的时候, 如果更强的假设条件成立, 那么HT估计量比2SLS估计量更加渐近有效, 而Amemiya and MaCurdy (1986)则将HT估计量的有效性进一步完善了.
\section{动态面板数据模型}
之前考虑的面板数据模型均为静态模型, 也即回归元中没有包括$Y_{it}$的滞后项, 然而在许多经济模型中, 当期的决策取决于过去的决策.

面板数据框架下的动态模型由$\text{AR}(p)$过程和带有单向误差结构的回归元构成, 也即
\begin{equation}\label{eq8.44}
  Y_{it}=\alpha_1Y_{i,t-1}+\cdots+\alpha_pY_{i,t-p}+X_{it}'\beta+u_i+\varepsilon_{it}
\end{equation}
其中$\alpha_j$为自回归系数, $X_{it}$为$K\times1$维回归向量, $u_i$为个体效应, 而$\varepsilon_{it}$为特质误差. 假定$u_i$和$\varepsilon_{it}$相互独立, $\{\varepsilon_{it}\}$无序列相关且是零均值的, 并且满足$\E[X_{is}\varepsilon_{it}]=0$.
\subsection{FE估计的偏误}
对于一个$\text{AR}(1)$过程
\begin{equation}\label{eq8.45}
  Y_{it}=\alpha Y_{i,t-1}+u_i+\varepsilon_{it}
\end{equation}
对方程(\ref{eq8.45})使用组内变换可得
$$\dot{Y}_{it}=\alpha\dot{Y}_{i,t-1}+\dot{\varepsilon}_{it},\quad t\ge2$$
于是个体效应$u_i$被消去, 问题的难点在于$\E[\dot{Y}_{i,t-1}\dot{\varepsilon}_{it}]\neq0$.

为了看清FE估计的偏误, 考虑一个简单的例子. 假设$T=3$, 于是每个个体$i$有两个可观测序对$[Y_{it},Y_{i,t-1}]$, 于是组内估计量为差分估计量. 当$t=3$时, 对(\ref{eq8.45})使用差分算子得到
\begin{equation}\label{eq8.46}
  \Delta Y_{i3}=\alpha\Delta Y_{i2}+\Delta\varepsilon_{i3}
\end{equation}
对上式使用最小二乘可知
\begin{align*}
\hat{\alpha}_{\text{FE}}&=\left(\sum_{i=1}^{N}\Delta Y_{i2}^2\right)^{-1}\left(\sum_{i=1}^{N}\Delta Y_{i2}\Delta Y_{i3}\right)=\alpha+\left(\sum_{i=1}^{N}\Delta Y_{i2}^2\right)^{-1}\left(\sum_{i=1}^{N}\Delta Y_{i2}\Delta \varepsilon_{i3}\right)
\end{align*}
注意到
\begin{align*}
\E[\Delta Y_{i2}\Delta \varepsilon_{i3}]&=\E[(Y_{i2}-Y_{i1})(\varepsilon_{i3}-\varepsilon_{i2})] \\
&=\E[Y_{i2}\varepsilon_{i3}]-\E[Y_{i1}\varepsilon_{i3}]-\E[Y_{i2}\varepsilon_{i2}]+\E[Y_{i1}\varepsilon_{i2}]=-\sigma_\varepsilon^2
\end{align*}
当$|\alpha|<1$时, 根据AR(1)的方差公式可知
$$\E[(\Delta Y_{i2})^2]=\frac{2\sigma_\varepsilon^2}{1-\alpha^2}-\frac{2\alpha\sigma_\varepsilon^2}{1-\alpha^2}=\frac{2\sigma_\varepsilon^2}{1+\alpha}$$
于是
$$\text{plim}\,(\hat{\alpha}_{\text{FE}}-\alpha)=\frac{\E[\Delta Y_{i2}\Delta \varepsilon_{i3}]}{\E[(\Delta Y_{i2})^2]}=-\frac{1+\alpha}{2}$$
当$T>3$时, 根据Nickell (1981)的结论可以得出, 当$|\alpha|<1$时, 固定效应估计量的概率极限为
$$\text{plim}\,(\hat{\alpha}_{\text{FE}}-\alpha)=\frac{1+\alpha}{\displaystyle\frac{2\alpha}{1-\alpha}-\frac{T-1}{1-\alpha^{T-1}}}$$
例如当$\alpha=0.5$, $T=30$时, 偏误大概为$-0.056$.

根据以上分析, 当回归元包括$Y_{it}$的滞后项时, FE估计量不是一致的, 即使时间维度$T$非常大.
\subsection{Anderson-Hsiao估计量}
Anderson-Hsiao (1982)取得了重要突破, 证明了一个简单的工具变量估计量在模型(\ref{eq8.44})中是一致的. 首先对(\ref{eq8.44})中$t\geq p+1$的部分做一阶差分
\begin{equation}\label{eq8.48}
  \Delta Y_{it}=\alpha_1\Delta Y_{i,t-1}+\alpha_2\Delta Y_{i,t-2}+\cdots+\alpha_p\Delta Y_{i,t-p}+\Delta X_{it}'\beta+\Delta\varepsilon_{it}
\end{equation}
同样有
$$\E[\Delta Y_{i,t-1}\Delta\varepsilon_{it}]=-\sigma_\varepsilon^2$$
但当$s>1$时, $\E[\Delta Y_{i,t-s}\Delta \varepsilon_{it}]=0$, 并且$\E[\Delta X_{it}\Delta\varepsilon_{it}]=0$的严格外生也成立.

由于$\Delta Y_{i,t-1}$与$\Delta \varepsilon_{it}$之间的相关性导致了内生性问题, 一种解决方法是使用工具变量. Anderson and Hsiao (1982)指出$Y_{i,t-2}$是一个有效工具, 这是因为在$\{\varepsilon_{it}\}$序列无关的条件下有
\begin{equation}\label{eq8.47}
  \E[Y_{i,t-2}\Delta\varepsilon_{it}]=\E[Y_{i,t-2}\varepsilon_{it}]-\E[Y_{i,t-2}\varepsilon_{i,t-1}]=0
\end{equation}
使用$Y_{i,t-2}$作为$\Delta Y_{i,t-1}$的工具, 由此得到的IV估计量即为Anderson-Hsiao估计量. 最终, 使用$[Y_{i,t-2},\cdots,Y_{i,t-p-1}]$作为$[\Delta Y_{i,t-1},\cdots,\Delta Y_{i,t-p}]$的工具, 就能一致估计出$[\alpha_1,\cdots,\alpha_p]$, 这要求$T\geq p+2$. 为了看到这一点, 我们同样假定$T=3$, $p=1$, 并且没有其它回归元$X_{it}$, 此时Anderson-Hsiao IV估计量为
$$\hat{\alpha}_{\text{IV}}=\left(\sum_{i=1}^{N}Y_{i1}\Delta Y_{i2}\right)^{-1}\left(\sum_{i=1}^{N}Y_{i1}\Delta Y_{i3}\right)=\alpha+\left(\sum_{i=1}^{N}Y_{i1}\Delta Y_{i2}\right)^{-1}\left(\sum_{i=1}^{N}Y_{i1}\Delta \varepsilon_{i3}\right)$$
如果$\{\varepsilon_{it}\}$是序列无关的, 那么(\ref{eq8.47})成立, 通常而言$\E[Y_{i1}\Delta Y_{i2}]\neq0$, 于是当$N\to\infty$时有
$$\hat{\alpha}_{\text{IV}}\xrightarrow{p}\alpha-\frac{\E[Y_{i1}\Delta\varepsilon_{i3}]}{\E[Y_{i1}\Delta Y_{i2}]}=\alpha$$
从而$\hat{\alpha}_{\text{IV}}$是$\alpha$的一致估计量.

简单总结一下, Anderson-Hsiao估计量的一致性依赖于两个关键假设, 一是误差项$\{\varepsilon_{it}\}$序列无关, 也即$\Delta\varepsilon_{it}$不存在二阶自相关, 因此工具变量才能与其无关; 二是工具变量与内生变量存在相关, 在上例中就是$\E[Y_{i1}\Delta Y_{i2}]\neq0$.

\subsection{GMM估计量}
正交条件(\ref{eq8.47})是动态面板模型所隐含的众多条件之一, 事实上滞后项$Y_{i,t-2},Y_{i,t-3},\cdots$都是$\Delta Y_{i,t-1}$的有效工具变量. 当$T>p+2$时, 尽管Anderson-Hsiao估计量是一致的, 但它是缺乏效率的 (受时代所限, GMM估计在1982年才提出), 而这些工具变量可以用来提高估计效率, 这项工作主要由 Arellano and Bond (1991)发展壮大.

首先将差分回归元$[\Delta Y_{i,t-1},\cdots,\Delta Y_{i,t-p},\Delta X_{it}']$堆叠为$T\times (p+K)$维矩阵$\Delta X_i$, 系数向量堆叠为$\alpha$, 从而(\ref{eq8.48})可以写为$\Delta Y_i=\Delta X_i\alpha+\Delta\varepsilon_i$, 全样本回归为$\Delta Y=\Delta \X\alpha+\Delta\varepsilon$. 可以证明, 当$t\geq p+2$时, $[Y_{i1},\cdots,Y_{i,t-2},\Delta X_{it}]$都是有效的工具变量\footnote{例如对于滞后阶数$p=2$, $t=5$的模型, 共有$K+3$个工具变量, 它们是$[Y_{i1},Y_{i2},Y_{i3},\Delta X_{it}']$.}.  定义矩阵
\begin{equation*}
  Z_i=\begin{bmatrix}
        [Y_{i1},\cdots,Y_{ip},\Delta X_{i,p+2}'] & 0 & \cdots & 0 \\
        0 & [Y_{i1},\cdots,Y_{i,p+1},\Delta X_{i,p+3}'] & \cdots & 0 \\
        \vdots & \vdots &  & \vdots \\
        0 & 0 & \cdots & [Y_{i1},\cdots,Y_{i,T-2},\Delta X_{i,T}']
      \end{bmatrix}
\end{equation*}
它的维数为$(T-p-1)\times L$, 其中$L=K(T-p-1)+[(T-2)(T-1)-(p-2)(p-1)]/2$. 以上工具矩阵包含了所有的滞后项$Y_{i,t-2}, Y_{i,t-3},\cdots$\footnote{严格外生条件$\E[X_{is}\varepsilon_{it}]=0$可能限制太强, 一个限制性更少的条件是假定回归元都是前定的(predetermined), 也即对于一切$s\geq0$, 都有$\E[X_{i,t-s}\varepsilon_{it}]=0$, 此时$X_{it}$可以与$\varepsilon_{it}$的滞后项相关, 由此$\E[\Delta X_{it}\Delta \varepsilon_{it}]\neq0.$, 故而应该使用工具$[X_{1i},X_{i2},\cdots,X_{i,t-1}]$替换$\Delta X_{it}$并纳入到$Z_i$中.}.

根据假设, 我们可以得到$L$个矩条件
$$\E[\Z'(\Delta Y_i-\Delta X_i\alpha)]=0$$
如果$T>p+2$, 那么$L>p$, 此时模型是过度识别的. 再定义$L\times L$维协方差矩阵
$$\BO=\E[Z_i'\Delta \varepsilon_i\Delta\varepsilon_i'Z_i]$$
令$\Z$是$Z_i$堆叠起来的$(T-p-1)N\times L$维矩阵, 那么一个不可行的有效GMM估计量为
$$\hat{\alpha}_\text{GMM}=(\Delta \X'\Z\BO^{-1}\Z'\Delta\X)^{-1}\Delta \X'\Z\BO^{-1}\Z'\Delta Y$$
如果$\{\varepsilon_{it}\}$是条件同方差的且无序列相关, 那么
$$\BO=\sigma_\varepsilon^2\E[Z_i'\mathbold{H}Z_i]$$
这里$\mathbold{H}=\mathbold{D}_i\mathbold{D}_i'$. 此时
$$\hat{\BO}_1=N^{-1}\sum_{i=1}^{N}Z_i'\mathbold{H}Z_i$$
从而可以得到可行的渐近有效GMM估计量
$$\hat{\alpha}_1=(\Delta\X'\Z\hat{\BO}_1^{-1}\Z'\Delta\X)^{-1}\Delta\X'\Z\hat{\BO}_1^{-1}\Z'\Delta Y$$
称$\hat{\alpha}_1$为一步Arellano-Bond GMM估计量. 此时$\hat{\alpha}_1$的经典协方差矩阵估计量为
$$\hat{\V}_1^0=\hat{\sigma}_\varepsilon^2(\Delta\X'\Z\hat{\BO}_1^{-1}\Z'\Delta\X)^{-1}$$
这里的$\hat{\sigma}_\varepsilon^2$为一步GMM残差$\hat{\varepsilon}_i=\Delta Y_i-\Delta X_i\hat{\alpha}_1$的样本方差, 而聚类稳健的协方差矩阵估计量为
$$\hat{\V}_1=(\Delta\X'\Z\hat{\BO}_1^{-1}\Z'\Delta\X)^{-1}(\Delta\X'\Z\hat{\BO}_1^{-1}\Z'\hat{\BO}_2^{-1}\Z\hat{\BO}_1^{-1}\Z'\Delta\X)(\Delta\X'\Z\hat{\BO}_1^{-1}\Z'\Delta\X)^{-1}$$
其中
$$\hat{\BO}_2=N^{-1}\sum_{i=1}^{N}Z_i'\hat{\varepsilon}_i\hat{\varepsilon}_i'Z_i$$
是使用一步GMM残差构造的$\BO$的聚类稳健估计量. 由此还能得到允许$\varepsilon_{it}$存在异方差的两步Arellano-Bond GMM估计量
$$\hat{\alpha}_2=(\Delta\X'\Z\hat{\BO}_2^{-1}\Z'\Delta\X)^{-1}\Delta\X'\Z\hat{\BO}_2^{-1}\Z'\Delta Y$$
它的聚类稳健稳健估计量为
$$\hat{\V}_2=(\Delta\X'\Z\hat{\BO}_2^{-1}\Z'\Delta\X)^{-1}(\Delta\X'\Z\hat{\BO}_2^{-1}\Z'\hat{\BO}_3^{-1}\Z\hat{\BO}_2^{-1}\Z'\Delta\X)(\Delta\X'\Z\hat{\BO}_2^{-1}\Z'\Delta\X)^{-1}$$
其中
$$\hat{\BO}_3=N^{-1}\sum_{i=1}^{N}Z_i'\tilde{\varepsilon}_i\tilde{\varepsilon}_i'Z_i$$
这里的$\tilde{\varepsilon}_{i}=\Delta Y_i-\Delta X_i\hat{\alpha}_2$为两步GMM残差. GMM估计量可以这样被不断迭代, 直至收敛到一个迭代GMM估计量.

Arellano-Bond估计量相对于Anderson-Hsiao估计量的优点是, 当$T>p+2$时, 额外的矩条件可以用来降低估计量的渐近方差. 然而其缺点为当$T$很大时, 容易导致弱工具变量问题, 因此有必要限制作为工具变量的滞后项的数量.

对于一步Arellano-Bond估计量, 它的权重矩阵$\hat{\BO}_1$并不依赖于GMM残差, 也就是说它的随机性要弱于二步Arellano-Bond估计量中的权重矩阵$\hat{\BO}_2$, 因此在较小的样本中, 特别是当误差项满足条件同方差时, 一步估计量有更好的性质. 相对地, 在大样本和误差项存在异方差的情形下, 两步估计量具有更好的渐近有效性.

简单总结一下, Arellano-Bond估计量是通过将GMM应用到差分方程(\ref{eq8.48})中, 并且选取可用的滞后项$Y_{i,t-2},Y_{i,t-3},\cdots$作为工具变量来获得的一致估计量, 以上方法也叫做差分GMM (difference GMM). 尽管有了许多提高, 然而Blundell and Bond (1998)指出Anderson-Hsiao估计量与Arellano-Bond估计量都会面临弱工具变量问题.

考虑Anerson-Hsiao估计量的情形, 如果使用$Y_{i,t-2}$作为$\Delta Y_{i,t-1}$的工具, 我们可以写出简约式方程
$$\Delta Y_{i,t-1}=Y_{i,t-2}\gamma+v_{it}$$
其中简约系数$\gamma$可以由线性投影定义. 利用方程$\Delta Y_{i,t-1}=(\alpha-1)Y_{i,t-2}+u_i+\varepsilon_{i,t-1}$以及$\E[Y_{i,t-2}\varepsilon_{i,t-1}]=0$, 从而
$$\gamma=\frac{\E[Y_{i,t-2}\Delta Y_{i,t-1}]}{\E[Y_{i,t-2}^2]}=(\alpha-1)+\frac{\E[Y_{i,t-2}u_i]}{\E[Y_{i,t-2}^2]}$$
Blundell and Bond (1998)进一步证明了
$$\gamma=\frac{k(\alpha-1)}{k+\sigma_u^2/\sigma_\varepsilon^2}$$
其中$k=(1-\alpha)/(1+\alpha)$. 显然, 当$\gamma$接近于0时, $Y_{i,t-2}$是一个弱工具变量, 此时$\alpha$接近于1, 也即模型是一个随机游走.

对于弱工具变量问题, Arellano and Bover (1995)以及 Blundell and Bond (1998)提出了解决办法. 回到之前的水平方程(\ref{eq8.44})中, 如果$\{\varepsilon_{it}\}$不存在序列相关, 并且$[\Delta Y_{i,t-1},\Delta Y_{i,t-2},\cdots]$与个体效应$u_i$不相关, 那么可以使用$[\Delta Y_{i,t-1},\Delta Y_{i,t-2},\cdots]$作为工具变量对模型(\ref{eq8.44})进行GMM估计, 称为水平GMM.

进一步, Blundell-Bond估计量将差分GMM和水平GMM结合在一起, 在水平GMM的假设条件下, 可以得到更有效率的系统GMM估计量, 并且一定程度缓解弱工具变量问题.
\chapter{极大似然估计}
之前我们考察的全部模型, 没有对以外生变量为条件的内生变量的分布做出任何假设就能通过外生性做出一致估计, 倘若我们给定了内生变量的分布信息, 那么极大似然估计 (Maximum Likelihood Estimation, MLE)将特别有用, 特别是在许多 (单非全部)一致且渐近正态的估计量族中, ML估计量都是渐近有效的, GMM估计量就是这样的估计量族中的一员.
\section{预备内容}

关于$Y$的参数模型 (parametric model)是$X$的一个依赖于未知参数$\theta\in\Theta$的概率函数, 它表明了$Y$的总体分布是某个具体的分布族中的一员. 举例而言, 一个参数模型是$Y\sim N(\mu,\sigma^2)$, 它的条件概率密度为
$$f(y|\mu,\sigma^2)=\frac{1}{\sqrt{2\uppi\sigma^2}}\text{exp}\left[{-\frac{(y-\mu)^2}{2\sigma^2}}\right]$$
其中参数$\mu\in\R$, 并且$\sigma^2>0$. 这个模型刻画了$Y$服从由$\mu$和$\sigma^2$确定的正态分布.

在初等统计学中, 经典的MLE正是从对独立同分布的$\{Y_i\}_{i=1}^n$的密度族的设定出发, 根据从总体中得到的随机抽样, 就可以构造似然函数和得出一阶条件进行参数估计. 然而在几乎所有的经济应用中, 我们总是对估计条件分布的参数感兴趣.

假定从总体中得到的随机抽样被分割为$(Y_i,X_i)$, 其中$y_i\in\mathscr{Y}$而$x_i\in\mathscr{X}$, 我们对给定$X_i$时的$Y_i$的条件分布感兴趣, 并且对$X_i$的分布不感兴趣, 无需对$X_i$的分布做任何具体设定. 从这个角度看, 本章所使用的方法称为条件极大似然估计 (Conditional Maximum Likelihood Estimation, CMLE), 在不引起混淆的情况下简称为MLE. 通过对$X_i$取0, 就可以得到无条件MLE的特殊情形.

为了实施极大似然分析, 我们需要对基本的结构模型, 即给定$X_i$时的$Y_i$的密度进行设定或推导. 通常而言, 我们假定密度函数具有有限个未知的参数, 因此我们可以得到一个关于$Y_i$的参数模型, $Y_i$既可以是向量也可以是标量.

为了阐明极大似然的一般理论, 下面给出二值响应模型 (binary response model)的例子, 它的一个重要分支是概率单位模型 (probit model).
\begin{example}
假定随机变量$Y_i^\ast$服从
$$Y_i^\ast=X_i'\theta+e_i$$
其中$X_i$和$\theta$均为$K\times1$维向量, $e_i$与$X_i$独立, 并且$e_i\sim N(0,1)$. 我们无法观测到$Y_i^\ast$, 而只能观测到指示$Y_i^\ast$符号的二值变量
$$Y_i=\mathbbm{1}[Y_i^\ast>0]$$
因此可以写出给定$X_i$时$Y_i$的分布
\begin{align}
\PP[Y_i=1|X_i]&=\PP[Y_i^\ast>0|X_i]=\PP[X_i'\theta+e_i|X_i]  \nonumber \\
&=\PP[e_i>-X_i'\theta|X_i]=1-\Phi(-X_i'\theta)=\Phi(X_i'\theta) \label{eq9.1}
\end{align}
其中$\Phi(\cdot)$为标准正态的累积分布函数. 类似可以得出
\begin{equation}\label{eq9.2}
  \PP[Y_i=0|X_i]=1-\Phi(X_i'\theta)
\end{equation}
将(\ref{eq9.1})和(\ref{eq9.2})结合就能得到给定$X_i$时$Y_i$的条件密度
$$f(y|X_i)=[\Phi(X_i'\theta)]^y[1-\Phi(X_i'\theta)]^{1-y}$$
显然当$y\notin\{0,1\}$时, $f(y|X_i)=0$.
\end{example}

\section{CMLE的一般框架}
设$\mathscr{X}\subset\R^K$和$\mathscr{Y}\subset\R^G$分别是随机向量$X$和$Y$的支集, 再定义给定$X$时$Y$的条件分布为$D(Y|X)$, 对于每个$X$而言, 这一分布表示一个概率测度, 并且完整地描述了$X$取到某个特殊值时, 随机向量$Y$的行为, 这一分布几乎总是由条件密度来描述.

于是我们定义$p_o(y|X)$为给定$X$时$Y$的条件密度, 下标$o$表示它为真实的条件密度, 而非某个可能的条件密度, 并且对于所有$x\in\mathscr{X}$, $p_o(\cdot|X)$为关于$\sigma$-有限测度$\nu$\footnote{设$(\Omega,\mathscr{F},\mu)$为一个测度空间, 如果存在可数集族$A_1,A_2,\cdots\in\mathscr{F}$, 使得$\bigcup_{n=1}^\infty A_n=\Omega$, 并且对一切$n\ge1$都有$\mu(A_n)<\infty$, 那么$\mu$是$\sigma$-有限测度.}的密度. 如果$D(Y|X)$是离散的, 那么$\nu$为计数测度且积分转化为求和; 如果$D(Y|X)$是绝对连续的, 那么$\nu$为Lebesgue测度. 换言之, 如果$Y_i$是离散的, 那么$\nu(\text{d}y)$把积分转换为求和; 如果$Y_i$是连续的, 那么就得到了通常的Lebesgue积分.


在正式阐述MLE的原理前, 还需要用到统计推断领域中极为重要的Kullback-Leibler信息准则 (Kullback-Leibler Information Criterion, KLIC). 假定$f$和$g$是$\R^M$上的非负$\nu$-可测函数, 定义$\mathscr{S}_f=\{z\in\R^M: f(z)>0\}$, 并且
\begin{equation}\label{eq9.3}
  1=\int_{\mathscr{S}_f}f(z)\,\nu(\text{d}z)\geq\int_{\mathscr{S}_f}g(z)\,\nu(\text{d}z)
\end{equation}
式(\ref{eq9.3})中的等式表明$f$是$\R^M$上的密度, 当$g$也是$\R^M$上的密度时, 上述不等式成立. 由此产生了一个重要结果
\begin{equation}\label{eq9.4}
  \mathscr{I}(f;g)=\int_{\mathscr{S}_f}\log\left[\frac{f(z)}{g(z)}\right]f(z)\,\nu(\text{d}z)\geq 0
\end{equation}
其中$\mathscr{I}(f;g)$的数量大小即为KLIC. 式(\ref{eq9.4})的另一种表述为
$$\E[\log\{f(Z)\}]\geq \E[\log\{g(Z)\}]$$

CMLE需要用到条件版本的(\ref{eq9.4}). 设$\mathscr{Y}(X)=\{y: p(y|X)>0\}$为$Y$的条件支集, $\nu$是不依赖于$X$的$\sigma$-有限测度, 那么对于任意$g(\cdot|X)>0$都有
$$\mathscr{I}_X(p;g)=\int_{\mathscr{Y}(X)}\log\left[\frac{p(y|X)}{g(y|X)}\right]p(y|X)\,\nu(\text{d}y)\geq 0$$
上式也可以表述为
$$\E[\log\{p(Y|X)\}|X]\geq \E[\log\{g(Y|X)\}|X]$$
现在任意选取一个非负$\nu$-可测函数$f(\cdot|X)$, 使得
\begin{equation}\label{eq9.5}
  \int_\mathscr{Y}f(y|X)\,\nu(\text{d}y)=1,\quad \forall x\in\mathscr{X}
\end{equation}
根据条件KLIC可知, 对于任意$x\in\mathscr{X}$都有
\begin{equation}\label{eq9.6}
  \mathscr{K}(f;X)\equiv\int_\mathscr{Y}\log\left[\frac{p_o(y|X)}{f(y|X)}\right]p_o(y|X)\,\nu(\text{d}y)\geq0
\end{equation}
故而当$f=p_o$时, 积分恒为0, 也即(\ref{eq9.6})意味着$\mathscr{K}(f;X)$在$f=p_o$处最小化.

现在将(\ref{eq9.5})应用到关于$p_o(\cdot|x)$的参数模型上, 即
\begin{equation}\label{eq9.7}
  \{f(\cdot|X;\theta): \theta\in\Theta\subset\R^P\}
\end{equation}
假定对于一切$x\in\mathscr{X}$和$\theta\in\Theta$, $f(\cdot|X;\theta)$都满足条件(\ref{eq9.5}), 我们称条件密度模型(\ref{eq9.7})是正确设定的, 如果存在某个$\theta_o\in\Theta$\footnote{相对应地, 如果这样的$\theta_o\in\Theta$不存在, 则说明模型设定错误.}, 使得对任意$x\in\mathscr{X}$都有
$$f(\cdot|X;\theta_o)=p_o(\cdot|X)$$
对于每个$x\in\mathscr{X}$, $\mathscr{K}(f;X)$可以写为
$$\E[\log\{p_o(Y_i|X_i)\}|X_i=X]-\E[\log\{f(Y_i|X_i)\}|X_i=X]$$
因此如果条件密度模型被正确设定, 那么
\begin{equation}\label{eq9.8}
  \E[ l_i(\theta_o)|X_i]\geq\E[ l_i(\theta)|X_i],\quad \theta\in\Theta
\end{equation}
其中
$$ l_i(\theta)= l(Y_i,X_i,\theta)=\log f(Y_i|X_i;\theta)$$
表示关于观测值$i$的条件对数似然 (conditional log-likelihood), 它是$\theta$的一个随机函数. 例如在Probit模型中, 观测值$i$的对数似然为
$$ l_i(\theta)=Y_i\log\Phi(X_i'\theta)+(1-Y_i)\log\,[1-\Phi(X_i'\theta)]$$

在不等式(\ref{eq9.8})两端取期望并使用LIE可知
\begin{equation}\label{eq9.9}
  \theta_o=\arg\max_{\theta\in\Theta}\,\E[ l_i(\theta)]
\end{equation}
使用样本矩替代总体矩, 倘若
$$\hat{\theta}_{\text{ML}}=\arg\max_{\theta\in\Theta}\,n^{-1}\sum_{i=1}^{n}\log f(Y_i|X_i;\theta)$$
存在, 那么称$\hat{\theta}_{\text{ML}}$为$\theta_o$的条件极大似然估计量. 假如把$X_i$处理为常量, 那么就得到经典框架下的ML估计量
$$\hat{\theta}_\text{ML}=\max_{\theta\in\Theta}\,\prod_{i=1}^{n}f(Y_i|X_i;\theta)$$
而ML估计量的存在性类似于“紧集上的连续函数存在最值”, 可由以下定理保证.
\begin{theorem}
  设$\Theta\subset\R^P$为紧集, 假设对于一切$\theta\in\Theta$, $f(\cdot;\theta)$都是$(Y_i,X_i)\in \mathscr{Y}\times \mathscr{X}$的非负可测函数, 并且对于每个$i$, 条件密度$f(Y_i|X_i;\cdot)$关于$\theta\in\Theta$是连续的, 那么ML估计量存在.
\end{theorem}
一旦我们得到了MLE的存在性, 就可以继续讨论MLE的不变性 (invariance). 假设$\hth$是$\theta_o\in\Theta$的ML估计量, 我们通过一个定义在$\Theta$上的函数$\tau:\Theta\to\Lambda$对模型进行再参数化 (reparametrization), 这里的$\Lambda$为值域
$$\Lambda=\{\lambda: \tau(\theta)=\lambda, \theta\in\Theta \}$$
并且规定$\Lambda$非空. 如果函数$\tau:\Theta\to\Lambda$为单射, 则它称为再参数化过程. 根据单射的定义, 存在反函数$\tau^{-1}$, 使得对于任意$\theta\in\Theta$都有$\tau(\tau^{-1}(\theta))=\theta$.

为简单起见, 假设$Y_i$是一个随机变量. 定义$\lambda_o=\tau(\theta_o)$, 以及$l(\theta)=\sum_{i=1}^{n}l_i(\theta)$, 于是
$$l(\theta)=l(\tau^{-1}(\lambda))=l^\ast(\lambda)$$
其中
$$l^\ast(\lambda)=\sum_{i=1}^{n} \log f(y_i|X_i;\lambda)$$
由于$\hth$为$\theta_o$的ML估计量, 因此对于任意$\theta\in\Theta$都有$l(\hth)\geq l(\theta)$. 定义$\hat{\lambda}=\tau(\hth)$, 对于任意$\theta\in\Theta$, 可以得到
\begin{align*}
l(\hth)=l(\tau^{-1}(\hat{\lambda}))=l^\ast(\hat{\lambda})\geq l(\theta)=l(\tau^{-1}(\lambda))
\end{align*}
因此对于任意$\lambda\in\Lambda$都有
$$l^\ast(\hat{\lambda})\geq l^\ast(\lambda)$$
从而$\hat{\lambda}$是$\lambda_o\in\Lambda$的ML估计量.

\begin{remark}
上述证明过程要求$\tau$为单射, 这是一个限制性很强的条件. Zehna (1966)去掉了这一限制, 因此$\tau:\Theta\to\Lambda$可以为任意可测函数, 但是$\hat{\lambda}$不再是通常意义下的ML估计量.

Zehna (1966)定义了集合$\Theta_\lambda=\{\theta: \tau(\theta)=\lambda\}$, 以及$M(\lambda)=\sup_{\theta\in\Theta_\lambda}l(\theta)$, 称$M(\lambda)$为由$\tau$诱导的似然函数. 此时
$$M(\lambda)=\sup_{\theta\in\Theta_\lambda}l(\theta)\leq\sup_{\theta\in\Theta}l(\theta)=l(\hth)=M(\hat{\lambda})$$
对一切$\lambda\in\Lambda$成立. 因此$\hat{\lambda}=\tau(\hth)$最大化了由$\tau$诱导的似然函数$M(
\lambda)$. 然而, $M(\lambda)$在一般情况下似乎不是任何随机变量的似然函数.
\end{remark}
\section{CMLE的渐近性质}
\subsection{一致性}
在正式讨论ML估计量的渐近性质前, 我们先给出一系列正则假设, 最主要的还是需要用到UWLLN和极值估计量的一致性引理.
\begin{proposition}\label{pro:pro9.1}
 (1) $\{(X_i,Y_i)\}_{i=1}^n$是一个可观测的随机样本, $x_i\in\mathscr{X}\subset\R^K$, 并且$y_i\in\mathscr{Y}\subset\R^G$.

 (2) $P\times 1$维参数空间是紧集.

 (3) 对于任意$x\in\mathscr{X}$和$\theta\in\Theta$, $f(\cdot|X,\theta)$是关于$\sigma$-有限测度$\nu(\text{d}y)$的真实密度.

 (4) 存在唯一的$\theta_o\in\Theta$, 使得对于一切$x\in\mathscr{X}$都有$p_o(\cdot|X)=f(\cdot|X;\theta_o)$.

 (5) 对于任意$\theta\in\Theta$, 对数似然$l(\cdot,\theta)$是$\mathscr{Y}\times \mathscr{X}$上的Borel可测函数.

 (6) 对于任意$(y,x)\in\mathscr{Y}\times\mathscr{X}$, 对数似然$l(y,x,\cdot)$是$\Theta$上的连续函数.

 (7) $\E[\sup_{\theta\in\Theta}| l_i(\theta)|]<\infty$.

\end{proposition}
在假设\ref{pro:pro9.1}中, 尽管$\Theta$可以不是紧的, 但这就需要做出更多的讨论. 可测性则是技术性假定, 通常无需对其检验. 关键性假设仍是$\theta_o$可识别, 占优条件成立, 以及对数似然函数在$\theta\in\Theta$上连续.

\begin{theorem}
在假设\ref{pro:pro9.1}下, 当$n\to\infty$时有$\hat{\theta}_\text{ML}\xrightarrow{p}\theta_o$.
\end{theorem}
\begin{proof}
  定义$\hat{Q}(\theta)=n^{-1}\sum_{i=1}^{n} l_i(\theta)$, 以及$Q(\theta)=\E[ l_i(\theta)]$, 根据之前证明GMM估计量一致性的做法, 由UWLLN可以证明$\sup_{\theta\in\Theta}|\hat{Q}(\theta)-Q(\theta)|\xrightarrow{p}0$, 由于真实参数$\theta_o$可识别, 最后根据极值估计量一致性引理即可证得结论.
\end{proof}

我们将$\theta_o$设置为一个有限的向量, 因此可以将$\Theta$设置为一个包含$\theta_o$的有界闭集, 在Euclid空间的意义上, 有界闭集等价于紧集.

对于Probit模型, 它关于观测值$i$的条件对数似然为
$$l_i(\theta)=Y_i\log\Phi(X_i'\theta)+(1-Y_i)\log\,[1-\Phi(X_i'\theta)]$$
如果$\E[X_iX_i']>0$, 那么对于任意$\theta\neq\theta_o$, $X'\theta\neq X'\theta_o$, 这是因为
$$\E[(X_i'\theta-X_i'\theta_o)^2]=(\theta-\theta_o)'\E[X_iX_i'](\theta-\theta_o)>0$$
又由于$\Phi(\cdot)$是严格单调函数, 故而当$\E[X_iX_i']$正定时, 只有唯一的$\theta_o\in\Theta$, 使得$p_o(\cdot|X_i)=f(\cdot|X_i;\theta_o)$. 注意到对于任意$v\in\R$, 都有不等式
$$|\log \Phi(v)|\leq |\log\Phi(0)|+|v|+|v|^2$$
从而
\begin{align*}
|l_i(\theta)| &\leq |Y_i||\log \Phi(X_i'\theta)|+|1-Y_i||\log\Phi(-X_i'\theta)| \\
&\leq |\log \Phi(X_i'\theta)|+|\log \Phi(-X_i'\theta)| \\
&\leq 2[|\log\Phi(0)|+||X_i||\cdot||\theta||+||X_i||^2||\theta||^2]
\end{align*}
只要$\E[X_iX_i']$的非奇异性意味着$\E||X_i||^2<\infty$, 于是$\E[\sup_{\theta\in\Theta}|l_i(\theta)|]<\infty$也成立. 因此只要样本$\{(Y_i,X_i)\}_{i=1}^n$是i.i.d.的, 并且$\E[X_iX_i']$正定且有限, 则Probit模型的ML估计量是一致的.
\subsection{得分函数与条件信息矩阵}
首先我们给出一个技术性假设, 也即真实参数$\theta_o$位于$\Theta$内部, 并且现在对每个观测值$i$, 定义一个$P\times1$维向量
$$s_i(\theta)=\nabla_\theta l_i(\theta)$$
称为对数似然得分 (score of log-likelihood). 如果条件密度模型$f(\cdot|X_i;\theta)$是正确识别的, 那么在$\theta=\theta_o$处, 得分函数具有重要的零条件均值性质, 也即
\begin{equation}\label{eq9.10}
  \E[s_i(\theta_o)|X_i]=0
\end{equation}
换言之, 当我们在$\theta_o$处计算$P\times1$维得分时, 并且关于$f(\cdot|X_i;\theta_o)$取期望, 则期望值为0. 根据LIE可得$\E[s_i(\theta_o)]=0$, 因此ML估计量可以看作是恰好识别情况下的GMM估计量.

为了证明条件(\ref{eq9.10}), 对于任意$\theta\in\Theta$, 设$\E_\theta[\cdot|X_i]$表示相对于密度$f(\cdot|X_i;\theta)$的条件期望. 于是根据定义可得
$$\E_\theta[s_i(\theta)|X_i]=\int_\mathscr{Y}s(y,X_i,\theta)f(y|X_i;\theta)\,\nu(\text{d}y)$$
在一定条件下, 积分和微分在$\Theta$内部可交换顺序, 也即
\begin{equation}\label{eq9.11}
  \nabla_\theta\left[\int_{\mathscr{Y}}f(y|X_i;\theta)\,\nu(\text{d}y)\right]=\int_\mathscr{Y}\nabla_\theta f(y|X_i;\theta)\,\nu(\text{d}y)
\end{equation}
其中$x_i\in\mathscr{X}$, 并且$\theta$在参数空间$\Theta$内部. 根据条件(\ref{eq9.5})可知
$$\int_\mathscr{Y}\nabla_\theta f(y|X_i;\theta)\,\nu(\text{d}y)=0$$
上式等价于
$$\int_\mathscr{Y}[\nabla_\theta l(y,X_i,\theta)]f(y|X_i;\theta)\,\nu(\text{d}y)=0$$
使用$\theta_o$替代$\theta$即可推出(\ref{eq9.10})成立.

\begin{remark}
条件密度模型的正确设定是条件(\ref{eq9.10})成立的充分不必要条件, 也即(\ref{eq9.10})无法推出模型正确设定, 因为模型误设可能存在于高阶矩中.
\end{remark}

\begin{theorem}\label{thm:thm9.1}
  在假设\ref{pro:pro9.1}下, 如果对于任意$(y,x)\in\mathscr{Y}\times \mathscr{X}$, 对数似然$l(y,x,\cdot)$关于$\beta\in\text{int}\,(\Theta)$连续可微, 那么$\E[s_i(\theta_o)|X_i]=0$.
\end{theorem}

另一方面, 假定$\theta_o\in\text{int}\,(\Theta)$, 并且$l_i(\theta)$在包含$\theta_o$的一个邻域$\mathscr{N}$内二阶连续可微. 再设观测值$i$的Hessian矩阵是$ l_i(\theta)$的二阶偏导数矩阵
$$H_i(\theta)=\nabla_\theta s_i(\theta)=\nabla_\theta^2  l_i(\theta)$$
它是$P\times P$的对称矩阵. 由于ML估计量是最大化问题的解, 因此$H_i(\theta_o)$的期望是负定的. 定义矩阵
$$H(\theta)=\E[H_i(\theta)]$$
当真实参数$\theta_o$可识别时, $-\mathbold{H}_o=\E[H_i(\theta_o)]$通常是正定的. 可以证明, 矩阵$-\mathbold{H}_o$等价于$\BO_o=\E[s_i(\theta_o)s_i(\theta_o)']$, 这里的$\BO_o$称为Fisher条件信息矩阵.

在足够的光滑条件下, 积分和微分可交换顺序
\begin{equation}\label{eq9.12}
  \nabla_\theta\left[\int_\mathscr{Y}s_i(\theta)f(y|X_i;\theta)\,\nu(\text{d}y)\right]=\int_\mathscr{Y}\nabla_\theta[s_i(\theta)f(y|X_i;\theta)]\,\nu(\text{d}y)
\end{equation}
假定$\theta$位于$\Theta$内部, 根据这一性质, 对恒等式
$$\int_\mathscr{Y}s_i(\theta)f(y|X_i;\theta)\,\nu(\text{d}y)\equiv \E_\theta[s_i(\theta)|X_i]=0$$
求微分可得
$$-\E_\theta[H_i(\theta)|X_i]=\var_\theta[s_i(\theta)|X_i]$$
于是在$\theta_o$处有
$$-\E[H_i(\theta_o)|X_i]=\E[s_i(\theta_o)s_i(\theta_o)'|X_i]$$
根据LIE即可推知$-\mathbold{H}_o=\BO_o$.

\begin{theorem}\label{thm:thm9.2}
  在假设\ref{pro:pro9.1}下, 如果对于任意$(y,x)\in\mathscr{Y}\times \mathscr{X}$, 对数似然$l(y,x,\cdot)$关于$\beta\in\text{int}\,(\Theta)$二阶连续可微, 那么$\E[s_i(\theta_o)s_i(\theta_o)'+H_i(\theta_o)|X_i]=0$.
\end{theorem}
在Probit模型中, 得分函数为
\begin{align*}
s_i(\theta)&=Y_i\left[\frac{\phi(X_i'\theta)X_i}{\Phi(X_i'\theta)}\right]-(1-Y_i)\left[\frac{\phi(X_i'\theta)X_i}{1-\Phi(X_i'\theta)}\right] \\
&=\frac{[Y_i-\Phi(X_i'\theta)]\phi(X_i'\theta)}{\Phi(X_i'\theta)[1-\Phi(X_i'\theta)]}X_i
\end{align*}
其中$\phi(\cdot)$是标准正态的概率密度函数. 并且Hessian矩阵
\begin{align*}
H_i(\theta)&=\left\{-\left[\frac{Y_i-\Phi(X_i'\theta)}{\Phi(X_i'\theta)(1-\Phi(X_i'\theta))}\right][\phi(X_i'\theta)]^2\right.\\
&\quad +\left.\left[\frac{Y_i-\Phi(X_i'\theta)}{\Phi(X_i'\theta)(1-\Phi(X_i'\theta))}\right]\phi'(X_i'\theta)\right\}X_iX_i'
\end{align*}
对于Probit模型, 还可以证明
$$-\mathbold{H}_o=\BO_o=\E[\lambda(X_i'\theta_o)\lambda(-X_i'\theta_o)X_iX_i']$$
其中$$\lambda(v)=\frac{\phi(v)}{\Phi(v)}$$称为逆Mills比率 (Inverse Mills Ratio, IMR).
\subsection{渐近正态性}
跟之前一样, 为了推导ML估计量的渐近分布, 我们还需要在假设\ref{pro:pro9.1}上的基础上增加新的限制条件.

\begin{proposition}\label{pro:pro9.2}
在假设\ref{pro:pro9.1}的基础上, 以下额外条件成立:

(1) $\theta_o\in\text{int}\,(\Theta)$.

(2) 对于任意$(y,x)\in\mathscr{Y}\times\mathscr{X}$, 对数似然函数$l(y,x,\cdot)$在包含$\theta_o$的某个邻域$\mathscr{N}$上二阶连续可微.

(3) 对于一切$\theta\in\text{int}\,(\Theta)$, 式(\ref{eq9.11})和(\ref{eq9.12})中的微分与积分可交换顺序.

(4) $\E[\sup_{\theta\in\mathscr{N}}||H_i(\theta)||]<\infty$.

(5) $\E[H_i(\theta_o)]$为满秩矩阵.

(6) $n^{-\frac{1}{2}}\sum_{i=1}^{n}s_i(\theta_o)\xrightarrow{d} N(0,\BO_o)$.
\end{proposition}
\begin{theorem}
  在假设\ref{pro:pro9.2}下, 当$n\to\infty$时有
  $$\sqrt{n}(\hat{\theta}-\theta_o)\xrightarrow{d} N(0,-\mathbold{H}_o)$$
  其中$\mathbold{H}_o=-\E[H_i(\theta_o)]$.
\end{theorem}
\begin{proof}
  之前已经证明了当$n\to\infty$时有$\hat{\theta}\xrightarrow{p}\theta_o$, 因为$\theta_o\in\text{int}\,(\Theta)$, 故而当$n$充分大时, ML估计量$\hat{\theta}$也是$\Theta$的内点, 且最大化对数似然函数$n^{-1}\sum_{i=1}^{n}\log f(Y_i|X_i;\theta)$的FOC为
  \begin{equation}\label{eq9.28}
    \hat{s}(\hat{\theta})=n^{-1}\sum_{i=1}^{n}\nabla_\theta l_i(\hat{\theta})=n^{-1}\sum_{i=1}^{n}s_i(\hat{\theta})=0
  \end{equation}
  将$\hat{s}(\hat{\theta})$在$\theta_o$处一阶Taylor展开可得
  \begin{align*}
  \sqrt{n}\hat{s}(\theta_o)+\hat{H}(\overbar{\theta})\sqrt{n}(\hat{\theta}-\theta_o)=0
  \end{align*}
  其中$\overbar{\theta}$在$\hat{\theta}$和$\theta_o$之间, 并且由$\hat{\theta}\xrightarrow{p}\theta_o$可知$\overbar{\theta}-\theta_o\xrightarrow{p}0$. 现在定义Hessian矩阵
  $$\hat{H}(\theta)=n^{-1}\sum_{i=1}^{n}\nabla_\theta^2 l_i(\theta)=n^{-1}\sum_{i=1}^{n}H_i(\theta)$$
  于是
  \begin{align*}
  ||\hat{H}(\overbar{\theta})-\mathbold{H}_o||&=||\hat{H}(\overbar{\theta})-H(\overbar{\theta})+H(\overbar{\theta})-H(\theta_o)|| \\
  &\leq \sup_{\theta\in\Theta}||\hat{H}(\overbar{\theta})-H(\overbar{\theta})||+||H(\overbar{\theta})-H(\theta_o)||\xrightarrow{p}0
  \end{align*}
  其中UWLLN保证第一项趋于0, 而$\overbar{\theta}\xrightarrow{p}\theta_o$及$H(\cdot)$的连续性保证了第二项趋于0.

  由于$\mathbold{H}_o=H(\theta_o)$是非奇异的, 故而当$n$充分大时, 矩阵$\hat{H}(\overbar{\theta})$也是非奇异的. 根据极大似然的FOC可知
  $$\sqrt{n}(\hat{\theta}-\theta_o)=-\hat{H}^{-1}(\overbar{\theta})\sqrt{n}\hat{s}(\theta_o)$$
  根据假设\ref{pro:pro9.2}(6)又可知
  $$\sqrt{n}\hat{s}(\theta_o)\xrightarrow{d}N(0,\BO_o)$$
  其中$\BO_o=\E[s_i(\theta_o)s_i(\theta_o)']$. 由Slutsky定理推得
  $$\sqrt{n}(\hat{\theta}-\theta_o)\xrightarrow{d} N(0,\mathbold{H}_o^{-1}\BO_o\mathbold{H}_o^{-1})$$
  因为模型是正确设定的, 故而$-\mathbold{H}_o=\BO_o$, 因此$\sqrt{n}(\hat{\theta}-\theta_o)\xrightarrow{d} N(0,-\mathbold{H}_o)$.
\end{proof}
下一节将证明, 在条件密度模型正确设定的情况下, 在很大一类估计量中, $\hat{\theta}_\text{ML}$都是$\theta_o$的渐近有效一致估计量.

然而, 模型误设是一个普遍现象, 会导致ML估计量不一致, 因此尽管ML估计量更有效, 但是缺乏稳健性. 而之前介绍的GMM估计量并不依赖于任何概率分布的假设, 故而比ML估计量更稳健.

现在来看$\hth_\text{ML}$的协方差矩阵估计量, 由于在条件密度模型正确设定的情况下有
$$\text{avar}(\sqrt{n}\hat{\theta})=-\mathbold{H}_o^{-1}=\BO_o^{-1}$$
故而有两种方法可以估计$\text{avar}(\sqrt{n}\hat{\theta})$. 一种是
$$\hat{\V} _\text{ML1}=-\hat{H}^{-1}(\hat{\theta})$$
其中
$\hat{H}(\theta)=n^{-1}\sum_{i=1}^{n}\nabla_\theta^2 \log f(Y_i|X_i;\theta)$. 另一种则是
$$\hat{\V}_\text{ML2}=n^{-1}\sum_{i=1}^{n}s_i(\hat{\theta})s_i(\hat{\theta})'$$
估计量$\hat{\V}_\text{ML1}$的一致性可由$\hat{\theta}_\text{ML}\xrightarrow{p}\theta_o$, 以及假设\ref{pro:pro9.2}(4)保证. 而估计量$\hat{\V}_\text{ML2}$的一致性需要做出额外假设, 具体参考Newey and McFadden (1994).

此外, 估计量$\hat{\V}_\text{ML1}$具有更好的有限样本性质, 但$\hat{\V}_\text{ML2}$更加容易计算, 因此在不得不使用数值计算的时候会方便许多.

Newey and McFadden (1994)证明了在Probit模型中有
$$||H(\theta)||\leq 2||X_iX_i'||$$
如果$\E[X_iX_i']$正定且有限, 那么假设\ref{pro:pro9.2}的每一个条件都可以满足, 因此ML估计量具有渐近正态性.
\section{CMLE的有效性}
本节将证明模型正确设定下的ML估计量在相当大的一类渐近正态估计量中都是有效的, 尽管这样的估计量类并没有包括全部的渐近正态估计量. 一个这样的估计量类就包含了GMM估计量, 因此ML估计量在我们所感兴趣的估计量中都是有效的.

首先需要阐明一个概念, 在全体渐近正态估计量构成的大类中, 不存在渐近有效的估计量. 为了看清这一点, 假设$\theta_o$的估计量$\hth$是渐近正态的, 也即当$n\to\infty$时有
$$\sqrt{n}(\hth-\theta_o)\xrightarrow{d} N(0,\V)$$
再定义另一个估计量
$$\tilde{\theta}=\begin{cases}
    \hth, &  |\hth-\alpha|\geq n^{-\frac{1}{4}} \\
    \alpha, & |\hth-\alpha|<n^{-\frac{1}{4}}
  \end{cases}$$
可以证明, $\tilde{\theta}$也是$\theta_o$的一致估计量, 其中$\alpha$是某个给定的常数. 并且当$\theta_o\neq \alpha$时有
$$\sqrt{n}(\tilde{\theta}-\theta_o)\xrightarrow{d} N(0,\V)$$
而当$\theta_o=\alpha$时有
$$n^\beta (\tilde{\theta}-\theta_o)\xrightarrow{d} 0$$
其中$\beta$是任意的实数.

换言之, 当$\theta_o\neq\alpha$时, $\tilde{\theta}$和$\hth$具有相同的渐近分布, 而当$\theta_o=\alpha$时, $\tilde{\theta}$收敛到$\theta_o$的速率可以任意快, 并且渐近分布具有零方差, 称$\tilde{\theta}$关于$\hth$是超有效的 (superefficient). 总的来看, 超有效性可能只在参数空间$\Theta$的Lebesgue零测集上才能取得, 因此在实践上意义不大.

回到正题, ML估计量关于GMM估计量的有效性可以通过比较它们的渐近方差来证明. 设$W_i=[X_i,Y_i]$, 假定当模型正确设定时有
\begin{equation}\label{eq9.21}
  \E[g(W_i,\theta_o)]=0
\end{equation}
其中$g(W,\theta)$表示一个$L\times 1$维向量. 在一定正则条件下, 积分和微分可交换, 按照前面的方法可以证得
\begin{equation}\label{eq9.22}
  -\E[\nabla_\theta g_i(\theta_o)]=\E[g_i(\theta_o)s_i(\theta_o)']
\end{equation}
令$\hth_{\text{GMM}}$是与正交条件(\ref{eq9.21})相联系的GMM估计量, 根据之前的结论有
$$\text{avar}(\sqrt{n}\hth_\text{GMM})=\E[m_\theta]^{-1}\E[mm']\E[m_\theta]^{-1}$$
其中
\begin{align*}
m_\theta&=\E[\nabla_\theta g(W_i,\theta_o)]'\W \nabla_\theta g(W_i,\theta_o) \\
m&=\E[\nabla_\theta g(W_i,\theta_o)]'\W g(W_i,\theta_o)
\end{align*}
根据(\ref{eq9.21})可知$\E[m_\theta]=-\E[ms']$, 其中$s=s_i(\theta_o)$. 由于ML估计量 (在$\sqrt{n}$意义下的)的渐近方差为$\E[ss']^{-1}$, 故而
$$\text{avar}(\sqrt{n}\hth_\text{GMM})-\text{avar}(\sqrt{n}\hth_\text{ML})=\E[ms']^{-1}\E[UU']\E[ms']^{-1}$$
其中$U=m-\E[ms']\E[ss']^{-1}s$. 显然上式是半正定的, 因此在某些正则条件下, ML估计量比GMM估计量更加渐近有效. 事实上, 渐近方差$\E[ss']^{-1}$即为统计学中的Cramer-Rao下界.

注意, MLE的有效性比最优GMM的还要强. 最优GMM是在给定了矩条件$\E[g(W_i,\theta_o)]=0$的情况下, 通过选取最优权重矩阵获得的. 如果GMM要想达到Cramer-Rao下界, 则矩条件必须为$\E[s_i(\theta_o)]=0$, 从这个意义上看, 具有最优矩条件的GMM渐近等价于MLE. 此时GMM估计量$\hth$应该满足
$$n^{-1}\sum_{i=1}^{n}s_i(\hth)=0$$
这正是最大化对数似然函数$\sum_{i=1}^{n}\log f(Y_i|X_i;\theta)$的FOC.


\section{参数检验}
现在考虑当条件密度模型$f(y_i|X_i;\theta)$设定正确时, 如何检验原假设
$$\HH_0: R(\theta_o)=r$$
其中$R:\Theta \to \R^J$是非随机的连续可微函数, $\nabla_\theta R(\theta_o)$是$J\times P$维满秩矩阵, $r$是$J\times 1$维非随机向量, 并且还满足$J\leq P$.

下面介绍以ML估计量$\hth_{\text{ML}}$为基础的三大统计检验方法, 分别为Wald检验, 似然比 (Likelihood Ratio, LR)检验, 以及Lagrange乘子 (Lagrange Multiplier, LM)检验.

\subsection{Wald检验}
在原假设$\HH_0: R(\theta_o)=r$成立的情况下, 根据一阶Taylor展开, ML估计量的渐近正态性, 以及Slutsky定理可知
\begin{align*}
\sqrt{n}[R(\hat{\theta})-r]&=\sqrt{n}[R(\theta_o)-r]+\nabla_\theta R(\overbar{\theta})\sqrt{n}(\hat{\theta}-\theta_o) \\
&=\nabla_\theta R(\overbar{\theta})\sqrt{n}(\hat{\theta}-\theta_o) \\
&\xrightarrow{d} N\{0,-[\nabla_\theta R(\theta_o)]\mathbold{H}_o^{-1}[\nabla_\theta R(\theta_o)]'\}
\end{align*}
其中$\overbar{\theta}$位于$\hat{\theta}$与$\theta_o$之间. 于是二次型
$$n[R(\hth)-r]'\{-[\nabla_\theta R(\theta_o)]\mathbold{H}_o^{-1}[\nabla_\theta R(\theta_o)]'\}^{-1}[R(\hth)-r]\xrightarrow{d}\chi^2_J$$
由Slutsky定理可以得到Wald检验统计量
$$W=n[R(\hth)-r]'\{-[\nabla_\theta R(\hth)][\hat{H}^{-1}(\hth)][\nabla_\theta R(\hth)]'\}^{-1}[R(\hth)-r]\xrightarrow{d}\chi^2_J$$
其中
$$\hat{H}(\theta)=n^{-1}\sum_{i=1}^{n}\nabla_\theta^2 l_i(\theta)$$
可以看出, 只需使用无约束的ML估计量$\hth$即可构造Wald检验统计量, 这是Wald检验的一大优点.

为了使Wald检验统计量服从渐近$\chi^2$分布, 必须将$\theta$限制在$\Theta$内部, 而不能在它的边界上. 举例而言, 如果限制$\theta\in\Theta$中的全体元素非负, 并且会用到这个约束条件, 那么在$\HH_0: \theta_o=0$下, Wald检验统计量不会有极限$\chi^2$分布.

此外, Wald统计量的一大缺陷是, 对于以不同方式施加的非线性约束, 它不具有不变性. 考虑经典线性回归模型中的原假设$\HH_0: \theta_1=1$, 假定$\theta_1>0$, 渐近$T$统计量为$(\hat{\theta}_1-1)/\text{se}(\hth_1)$, 现在定义$\phi_1=\log\theta_1$, 那么原假设可以重新表述为$\HH_0:\phi_1=0$, 根据Delta法可知$\text{se}(\hat{\phi}_1)=\hat{\theta}_1^{-1}\text{se}(\hth_1)$, 从而基于$\hat{\phi}_1$的$T$统计量为$\hat{\phi}_1/\text{se}(\hat{\phi}_1)=\hth_1\log(\hth_1)/\text{se}(\hat{\phi}_1)\neq (\hth_1-1)/\text{se}(\hat{\phi}_1)$.

由于缺乏不变性, 因此对于非线性假设而言, Wald检验统计量的有限样本性质可能会很差, 研究者不得不去研究原假设的各种表述, 以便获得一个令人满意的结果. 为此, 有必要寻找其它的统计量来克服缺乏不变性这个问题.
\subsection{似然比检验}
\begin{theorem}
  在假设\ref{pro:pro9.2}下, 定义对数条件似然样本均值
  \begin{align*}
  \hat{l}(\hth)&=n^{-1}\sum_{i=1}^{n}l_i(\hth) \\ \hat{l}(\tilde{\theta})&=n^{-1}\sum_{i=1}^{n}l_i(\tilde{\theta})
  \end{align*}
  其中$\hth$是无约束ML估计量, 而$\tilde{\theta}$是约束条件$R(\tilde{\theta})=r$下的ML估计量. 如果原假设$\HH_0: R(\theta_o)=r$成立, 那么当$n\to\infty$时有
  $$LR=2n[\hat{l}(\hth)-\hat{l}(\tilde{\theta})]\xrightarrow{d}\chi^2_J$$
\end{theorem}
\begin{proof}
  根据之前的讨论, 由于无条件ML估计量$\hth$是问题$\max_{\theta\in\Theta}\, \hat{l}(\theta)$的解, 对应的的FOC为
  $$\hat{s}(\hth)=0$$
  其中$\hat{s}(\theta)=n^{-1}\sum_{i=1}^{n}\nabla_\theta l_i(\theta)$. 另一方面, 有约束ML估计量$\tilde{\theta}$是问题
  $$\max_{\theta\in\Theta}\,\hat{l}(\theta)+\lambda'[r-R(\theta)]$$
  的解, 其中$\lambda$为$J\times1$维Lagrange乘子向量. 可以找到FOC为
  \begin{align}
  \hat{s}(\tilde{\theta})-[\nabla_\theta R(\tilde{\theta})]'\tilde{\lambda}&=0 \label{eq9.13} \\
  R(\tilde{\theta})-r&=0 \nonumber
  \end{align}
  将$\hat{l}(\tilde{\theta})$在$\hth$处二阶Taylor展开可得
  \begin{align*}
  -LR&=2n[\hat{l}(\tilde{\theta})-\hat{l}(\hth)] \\
  &=2n[\hat{l}(\hth)-\hat{l}(\hat{\theta})]+2n[\hat{s}(\hth)]'(\tilde{\theta}-\hth)+\sqrt{n}(\tilde{\theta}-\hth)'\hat{H}(\overbar{\theta}_a)\sqrt{n}(\tilde{\theta}-\hth) \\
  &=\sqrt{n}(\tilde{\theta}-\hth)'\hat{H}(\overbar{\theta}_a)\sqrt{n}(\tilde{\theta}-\hth)
  \end{align*}
  其中$\overbar{\theta}_a$位于$\hat{\theta}$和$\tilde{\theta}$之间, 从而
  \begin{equation*}
    LR=\sqrt{n}(\tilde{\theta}-\hth)'[-\hat{H}(\overbar{\theta}_a)]\sqrt{n}(\tilde{\theta}-\hth)
  \end{equation*}

  现在将$\hat{s}(\tilde{\theta})$在无约束ML估计量$\hth$处一阶Taylor展开, 再将其代入到(\ref{eq9.13})得到
  $$\hat{s}(\hth)+\hat{H}(\overbar{\theta}_b)(\tilde{\theta}-\hth)-[\nabla_\theta R(\tilde{\theta})]'\tilde{\lambda}=0$$
  其中$\overbar{\theta}_b$也位于$\hat{\theta}$和$\tilde{\theta}$之间. 由于$\hat{s}(\hth)=0$, 故而
  $$\hat{H}(\overbar{\theta}_b)\sqrt{n}(\tilde{\theta}-\hth)-[\nabla_\theta R(\tilde{\theta})]'\sqrt{n}\tilde{\lambda}=0$$
  当$n$充分大时, 矩阵$\hat{H}(\overbar{\theta}_b)$可逆, 于是
  \begin{equation}\label{eq9.17}
    \sqrt{n}(\tilde{\theta}-\hth)=\hat{H}^{-1}(\overbar{\theta}_b)[\nabla_\theta R(\tilde{\theta})]'\sqrt{n}\tilde{\lambda}
  \end{equation}
  故而还需推导$\sqrt{n}\tilde{\lambda}$的渐近分布.

  然后将$\hat{s}(\tilde{\theta})$在真实参数$\theta_o$处一阶Taylor展开, 再将其代入到(\ref{eq9.13})得到
  $$[\nabla_\theta R(\tilde{\theta})]'\sqrt{n}\tilde{\lambda}=\sqrt{n}\hat{s}(\theta_o)+\hat{H}(\overbar{\theta}_c)\sqrt{n}(\tilde{\beta}-\beta_o)$$
  其中$\overbar{\theta}_c$位于$\tilde{\theta}$和$\theta_o$之间. 对于充分大的$n$有
  \begin{equation}\label{eq9.14}
    \hat{H}^{-1}(\overbar{\theta}_c)[\nabla_\theta R(\tilde{\theta})]'\sqrt{n}\tilde{\lambda}=\hat{H}^{-1}(\overbar{\theta}_c)\sqrt{n}\hat{s}(\theta_o)+\sqrt{n}(\tilde{\theta}-\theta_o)
  \end{equation}
  进一步将$R(\tilde{\beta})-r=0$在$\theta_o$处一阶Taylor展开得到
  \begin{equation}\label{eq9.15}
    \sqrt{n}[R(\theta_o)-r]+[\nabla_\theta R(\overbar{\theta}_d)]'\sqrt{n}(\tilde{\theta}-\theta_o)=0
  \end{equation}
  其中$\overbar{\theta}_d$也位于$\tilde{\theta}$和$\theta_o$之间. 如果原假设$\HH_0: R(\theta_o)=r$成立, 那么根据(\ref{eq9.15})可得
  \begin{equation}\label{eq9.16}
    [\nabla_\theta R(\overbar{\theta}_d)]'\sqrt{n}(\tilde{\theta}-\theta_o)=0
  \end{equation}
  在式(\ref{eq9.14})上左乘$\nabla_\theta R(\overbar{\theta}_d)$, 根据(\ref{eq9.16})可知
  $$[\nabla_\theta R(\overbar{\theta}_d)][\hat{H}^{-1}(\overbar{\theta}_c)][\nabla_\theta R(\tilde{\theta})]'\sqrt{n}\tilde{\lambda}=[\nabla_\theta R(\overbar{\theta}_d)]\hat{H}^{-1}(\overbar{\theta}_c)\sqrt{n}\hat{s}(\theta_o)$$
  当$n$充分大时, 根据假设\ref{pro:pro9.2}(6)以及Slutsky定理可得
  \begin{align}
  \sqrt{n}\tilde{\lambda}&=\{[\nabla_\theta R(\overbar{\theta}_d)][\hat{H}^{-1}(\overbar{\theta}_c)][\nabla_\theta R(\tilde{\theta})]'\}^{-1}[\nabla_\theta R(\overbar{\theta}_d)]\hat{H}^{-1}(\overbar{\theta}_c)\sqrt{n}\hat{s}(\beta_o) \nonumber \\
  &\xrightarrow{d}N(0,\{-[\nabla_\theta R(\theta_o)]\mathbold{H}_o^{-1}[\nabla_\theta R(\theta_o)]'\}^{-1}) \label{eq9.18}
  \end{align}

  在式(\ref{eq9.17})上左乘$[-\hat{H}(\overbar{\theta}_a)]^{\frac{1}{2}}$, 根据(\ref{eq9.18})可知
  \begin{align}
  [-\hat{H}(\overbar{\theta}_a)]^{\frac{1}{2}}\sqrt{n}(\tilde{\theta}-\hth)&=[-\hat{H}(\overbar{\theta}_a)]^{\frac{1}{2}}\hat{H}^{-1}(\overbar{\theta}_b)[\nabla_\theta R(\tilde{\theta})]'\sqrt{n}\tilde{\lambda} \nonumber \\
  &\xrightarrow{d} N(0,\mathbold{\Pi})\sim\mathbold{\Pi}^{\frac{1}{2}}N(0,\mathbold{I}_P) \label{eq9.19}
  \end{align}
  其中
  $$\mathbold{\Pi}=\mathbold{H}_o^{-\frac{1}{2}}[\nabla_\theta R(\theta_o)]'\{-[\nabla_\theta R(\theta_o)]\mathbold{H}_o^{-1}[\nabla_\theta R(\theta_o)]'\}^{-1}[\nabla_\theta R(\theta_o)]\mathbold{H}_o^{-\frac{1}{2}}$$
  为$P\times P$维幂等矩阵, 并且秩为$J$. 最后根据引理\ref{lem:lem2.1}可知
  \begin{align*}
  LR&=\sqrt{n}(\tilde{\theta}-\hat{\theta})'[-\hat{H}(\overbar{\theta}_a)]^{\frac{1}{2}}[-\hat{H}(\overbar{\theta}_a)]^{\frac{1}{2}}\sqrt{n}(\tilde{\theta}-\hth) \\
  &\xrightarrow{d}\chi_J^2
  \end{align*}
  证毕.
\end{proof}
LR检验是基于比较原假设$\HH_0: R(\theta_o)=r$下的对数似然函数$\hat{l}(\tilde{\theta})$和无约束条件下的对数似然函数$\hat{l}(\hth)$. 如果$\HH_0$成立, 那么无约束模型的$\hat{l}(\hth)$和有约束模型的$\hat{l}(\tilde{\theta})$应该相近. 反之, 如果$\hat{l}(\hth)$显著大于$\hat{l}(\tilde{\theta})$, 则原假设$\HH_0$应该为假.

相较于Wald检验来说, LR检验可能更难施行, 毕竟计算起来十分麻烦, 甚至有时无法解析地得到表达式, 但是LR检验具有不变性, 这是它的一大优点. 此外, Wald检验通常比LR检验更容易拒绝原假设.
\subsection{Lagrange乘子检验}
除了以上两种方法外, 还可以通过Lagrange乘子$\tilde{\lambda}$构造LM检验, 它又称为Rao有效得分检验. 考虑以下有约束的最大化问题
$$\max_{\theta\in\Theta}\,\hat{l}(\theta)+\lambda'[r-R(\theta)]$$
这里最优Lagrange乘子度量了约束条件$\HH_0: R(\theta_o)=r$对模型似然函数最大值的影响. 当$\HH_0$成立时, 施加该约束应该对似然函数最大值的影响不大, 也即$\tilde{\lambda}$应该很小. 反之, 如果$\tilde{\lambda}$的值很大, 则有理由拒绝原假设.

之前在推导LR检验统计量时就已得出
\begin{align*}
\sqrt{n}\tilde{\lambda}&=\{[\nabla_\theta R(\overbar{\theta}_d)][\hat{H}^{-1}(\overbar{\theta}_c)][\nabla_\theta R(\tilde{\theta})]'\}^{-1}[\nabla_\theta R(\overbar{\theta}_d)]\hat{H}^{-1}(\overbar{\theta}_c)\sqrt{n}\hat{s}(\beta_o) \nonumber \\
  &\xrightarrow{d}N(0,\{-[\nabla_\theta R(\theta_o)]\mathbold{H}_o^{-1}[\nabla_\theta R(\theta_o)]'\}^{-1})
\end{align*}
从而二次型
$$n\tilde{\lambda}'\{-[\nabla_\theta R(\theta_o)]\mathbold{H}_o^{-1}[\nabla_\theta R(\theta_o)]'\}\tilde{\lambda}\xrightarrow{d}\chi_J^2$$
最后根据Slutsky定理得到LM检验统计量
$$LM=-n\tilde{\lambda}'[\nabla_\theta R(\tilde{\theta})][\hat{H}^{-1}(\tilde{\theta})][\nabla_\theta R(\tilde{\theta})]'\tilde{\lambda}\xrightarrow{d}\chi_J^2$$

简单总结一下, Wald检验仅使用无约束信息, LM检验仅使用有约束信息, LR检验则同时利用了这两种信息, 以上三类检验在大样本下是渐近等价的. Wald检验使用范围最广, 因为不对条件密度做具体假设, 但Wald检验不具有不变性; LR检验以及某些LM检验具有不变性, 但可能很难得到似然函数.
\section{模型设定检验}
由于MLE依赖于条件密度模型$f(y|X_i;\theta)$的正确设定, 如果模型出现误设, 则通常难以得到一致估计量, 即使使用后面介绍的QMLE也会造成精度下降. 因此有必要检验原假设$$\HH_0: \forall x\in\mathscr{X}, \, \exists\theta_o\in\Theta,\,\text{s.t. }f(\cdot|X;\theta_o)=p_o(\cdot|X)$$
White (1982)提出了通过检验信息矩阵等式
$$\E[s_i(\theta_o)s_i(\theta_o)']+\E[H_i(\theta_o)]=0$$
是否成立来检验$\HH_0$是否成立.

定义$Q\times 1$维矩样本均值
$$\hat{g}(\theta)=n^{-1}\sum_{i=1}^{n}g_i(\theta)$$
其中$Q=P(P+1)/2$, 并且
$$g_i(\theta)=\text{vech}\,[s_i(\theta)s_i(\theta)'+H_i(\theta)]$$
令$\hth$为ML估计量, 在正则条件下可以用UWLLN推出$$\hat{g}(\hth)\xrightarrow{p}\E[g_i(\theta_o)]$$如果信息矩阵等式成立, 那么$\E[g_i(\theta_o)]=0$, 此时$\hat{g}(\hth)$接近于零向量, 因此可以考虑推导$\sqrt{n}\hat{g}(\hth)$的渐近分布. White (1982)证明了, 一定情况下在$n\to\infty$时有
\begin{align*}
\sqrt{n}\hat{g}(\hth)&=n^{-\frac{1}{2}}\sum_{i=1}^{n}[g_i(\theta_o)-\mathbold{G}_o\mathbold{H}_o^{-1}s_i(\theta_o)]+\text{o}_p(1) \\
&\xrightarrow{d} N(0,\BS)
\end{align*}
其中$\mathbold{G}_o=\E[\nabla_\theta g_i(\theta_o)]$, 渐近协方差矩阵
$$\BS=\text{var}[g_i(\theta_o)-\mathbold{G}_o\mathbold{H}_o^{-1}s_i(\theta_o)]$$
从而在模型设定正确时, 信息矩阵统计量
$$IM=n[\hat{g}(\hth)]'\hat{\BS}^{-1}[\hat{g}(\hth)]\xrightarrow{d}\chi_Q^2$$
其中$\BS$的一致估计量为
$$\hat{\BS}=n^{-1}\sum_{i=1}^{n}\hat{M}_i\hat{M}_i'$$
并且$\hat{M}_i=g_i(\hth)-[\nabla_\theta \hat{g}(\hth)]\hat{H}^{-1}(\hth)s_i(\hth)$.

同样地, IM检验本质上检验的是信息矩阵等式是否成立. 由于信息矩阵等式成立只是模型设定正确的必要非充分条件, 因此IM检验在大样本下不能拒绝原假设$\HH_0$并不意味着模型设定正确, 只是说没有发现模型误设的证据.


\section{拟极大似然估计}
\subsection{一般误设}
当条件密度模型$f(y|X_i;\theta)$设定错误时, 对于任意的$\theta\in\Theta$, 总有$f(\cdot|X_i;\theta)\neq p_o(\cdot|X_i)$. 假设伪真值 (pseudo-true value) $\theta^\ast$是问题
$$\max_{\theta\in\Theta}\,\E[l_i(\theta)]$$
的唯一最优解, 并且$\hat{\theta}$是问题
$$\max_{\theta\in\Theta}\,n^{-1}\sum_{i=1}^{n}\log f(Y_i|X_i;\theta)$$
的解, 那么称$\hth$为拟极大似然估计量 (quasi-maximum likelihood estimator). 在类似\ref{pro:pro9.2}的正则条件下, 可以通过极值估计量的一致性引理得到
$$\hat{\theta}=\theta^\ast+\text{o}_p(1)$$
但此时不能将$\theta^\ast$解释为真实参数, 事实上它最小化了$f(\cdot|X_i;\theta)$和$p_o(\cdot|X_i)$之间的距离. 其渐近分布为
$$\sqrt{n}(\hat{\theta}-\theta)\xrightarrow{d} N(0,\V_\theta)$$
这里的渐近协方差矩阵为
\begin{align*}
\V_\theta&=\mathbold{H}_\ast^{-1}\BO_\ast\mathbold{H}_\ast^{-1} \\
&=\E[H_i(\theta^\ast)]^{-1}\text{avar}[\sqrt{n}\hat{s}(\theta^\ast)]\E[H_i(\theta^\ast)]^{-1}
\end{align*}
由于条件信息矩阵等式不成立, 因此$\V_\theta$不能化简, 并且它一般也没有达到Cramér-Rao下界, 因此比MLE的有效性更低. 此时QML估计量的协方差矩阵估计量
\begin{equation}\label{eq9.23}
  \hat{\V}_{\text{QML}}=\left[\sum_{i=1}^{n}H_i(\hth)\right]^{-1}\left[\sum_{i=1}^{n}s_i(\hth)s_i(\hth)'\right]\left[\sum_{i=1}^{n}H_i(\hth)\right]^{-1}
\end{equation}
在原假设$\HH_0:R(\theta^\ast)=r$下, 还可以根据(\ref{eq9.23})实施稳健Wald检验与稳健LM检验, 但是无法构造LR检验统计量, 因为条件信息矩阵等式不再成立.

在绝大多数应用中, 我们都无法正确设定条件密度函数, 因此使用的都是GMM或者QMLE来估计参数. 如果误设程度越低, 也即$f(\cdot|X_i;\theta)$与$p_o(\cdot|X_i)$越接近, 则QMLE的有效性越好.
\subsection{模型选择检验}
有时候可能存在多个相互竞争的误设模型, 我们需要在其中选择一个更有吸引力的. 在这方面, Vuong (1989)基于LR检验做出了许多成果.

假设$f_1(y|X;\theta_1)$和$f_2(y|X;\theta_2)$是条件分布$D(Y_i|X_i)$的密度的候选模型, $\hat{\theta}_1$和$\hat{\theta}_2$分别是收敛于$\theta_1^\ast$和$\theta_2^\ast$的QMLE. 首先定义条件密度模型
\begin{align*}
\mathscr{F}_1&=\{f_1(y|X;\theta_1): \theta_1\in\Theta_1\} \\
\mathscr{F}_2&=\{f_2(y|X;\theta_2): \theta_2\in\Theta_2\}
\end{align*}
如果: (i) $\mathscr{F}_1\cap \mathscr{F}_2=\emptyset$, 那么称$\mathscr{F}_1$和$\mathscr{F}_2$是严格非嵌套的; (ii) $\mathscr{F}_2\subset\mathscr{F}_1$, 那么称$\mathscr{F}_2$嵌套于$\mathscr{F}_1$; (iii) $\mathscr{F}_1\cap \mathscr{F}_2\neq\emptyset$, $\mathscr{F}_1\subset\mathscr{F}_2$, 并且$\mathscr{F}_2\subset\mathscr{F}_1$, 则称$\mathscr{F}_1$与$\mathscr{F}_2$重叠.

对于严格非嵌套模型, 定义
$$l_{m}=\sum_{i=1}^{n}l_{im}(\hat{\theta}_m),\quad m=1,2$$
为在对应的估计量处的拟对数似然. 在正则条件下有
\begin{align*}
n^{-\frac{1}{2}}(l_1-l_2)&=n^{-\frac{1}{2}}\sum_{i=1}^{n}[l_{i1}(\hth_1)-l_{i2}(\hth_2)] \\
&=n^{-\frac{1}{2}}\sum_{i=1}^{n}[l_{i1}(\theta_1^\ast)-l_{i2}(\theta_2^\ast)]+\text{o}_p(1)
\end{align*}
在原假设$\HH_0: \E[l_{1i}(\theta_1^\ast)]=\E[l_{i2}(\theta_2^\ast)]$下, 可以证明
$$n^{-\frac{1}{2}}\sum_{i=1}^{n}[l_{i1}(\theta_1^\ast)-l_{i2}(\theta_2^\ast)]\xrightarrow{d} N(0,\eta^2)$$
其中$\eta^2=\text{var}[l_{1i}(\theta_1^\ast)-l_{i2}(\theta_2^\ast)]$. 渐近方差$\eta^2$的一致估计量为
$$\hat{\eta}^2=n^{-1}\sum_{i=1}^{n}[l_{1i}(\hth_1)-l_{i2}(\hth_2)]^2$$
当原假设$\HH_0$成立时, 可以得到VMS检验统计量
$$n^{-\frac{1}{2}}(l_1-l_2)/\hat{\eta}\xrightarrow{d}N(0,1)$$
如果VMS统计量显著大于0, 则说明模型1拟合得更好, 显著小于0的情况也可以类似地解释为模型2拟合得更好, 但这同样无法说明模型1和模型2是否是正确设定的. 最后, 对于模型嵌套和重叠的情形, 具体见Vuong (1989).
\subsection{线性指数族的QMLE}
之前讨论的内容允许模型$f(y|X;\theta)$没有任何东西是正确设定的, 但在模型设定正确与完全误设之间存在一个中间地带, 允许条件密度的某些特征是正确设定的.

\begin{example}
假设$\{(Y_i,X_i)\}_{i=1}^n$为i.i.d.随机样本, 非线性回归模型为
$$Y_i=g(X_i,\beta_o)+e_i$$
并且还满足$\E[e_i|X_i]=0$, 然而我们并不知道$e_i|X_i$的条件密度. 为了估计真实参数$\beta_o$, 尽管这么做可能不正确, 但仍假设$e_i|X_i\sim \text{i.i.d.}\,N(0,\sigma^2)$, 此时$Y_i$的拟条件概率密度为
$$f(y|X_i;\theta)=\frac{1}{\sqrt{2\uppi\sigma^2}}\exp\left\{-\frac{[y-g(X_i,\beta)]^2}{2\sigma^2}\right\}$$
其中$\theta=[\beta',\sigma^2]'$. 由此可以定义QML估计量
$$\hat{\theta}=[\hat{\beta}',\hat{\sigma}^2]'=\arg\max_{\beta,\sigma^2}\sum_{i=1}^{n}\log f(Y_i|X_i;\theta)$$
则在一般的正则条件下, $\hat{\beta}$为$\beta_o$的一致估计量.
\end{example}

事实上, 上述例子的结论之所以能成立, 是因为正态分布是线性指数族 (Linear Exponential Family, LEF)中的一员, 除了正态分布外, LEF中还包括Bernoulli分布, Poisson分布, Gamma分布等. 为简单起见, 我们仅考虑响应变量为标量的情形.

Gourieroux et al. (1984)的结果表明, LEF中的对数似然可以写为均值的一个函数
\begin{equation}\label{eq9.20}
  \log f(y|\mu)=a(\mu)+b(y)+yc(\mu)
\end{equation}
其中$\mu$是随机变量$Y_i$均值的一个待选值, 并且$\mu_o=\E[Y_i]$为真实期望. 例如$Y$服从正态分布$N(\mu,\sigma^2)$, 则
$$\log f(y|\mu)=-\frac{1}{2}\log(2\uppi\sigma^2)-\frac{1}{2\sigma^2}(y-\mu)^2$$
由此可知(\ref{eq9.20})中有$c(\mu)=\mu/\sigma^2$. 而对于Bernoulii分布, 有
$$\log f(y|\mu)=(1-y)\log(1-\mu)+y\log\mu,\quad 0<\mu<1$$
因此, $a(\mu)=\log(1-\mu)$, $b(y)=0$, $c(\mu)=\log[\mu/(1-\mu)]$. 事实上, 估计Probit模型的正是Bernoulli QMLE.

现在将$\mu$参数化为$m(X,\theta)$, 那么条件拟对数似然变为
$$\log f(y|m(X,\theta))=a(m(X,\theta))+b(y)+yc(m(X,\theta))$$
假设条件均值是正确设定的, 那么我们可以假定存在$\theta_o\in\Theta$, 使得$\E[Y_i|X_i]=m(X_i,\theta_o)$. 加上其它技术性条件, $\hat{\theta}_\text{QML}$是真实参数$\theta_o$的一致估计量.

在上面的例子中, 由于$\E[Y_i|X_i]=g(X_i,\beta_o)$, 并且$Y$服从正态分布, 因此可以通过QMLE一致估计出$\beta_o$. 不仅如此, 考虑$Y_i$为一个非负连续随机变量, 如果条件均值是正确设定的, 那么我们使用Poisson QMLE同样也可以得出一致估计量, 唯一的限制是$\E[Y_i|X_i=x]$候选值的范围应该与从LEF中选取的密度函数所允许的范围相同\footnote{举例而言, 如果$\E[Y_i|X_i=x]$的可能值为负, 那么就不能将$Y_i|X_i$设置为服从Poisson分布, 因为Poisson分布的均值不为负.}.

%\chapter{因果推断}
%本章基于Rubin (1974)建立的Rubin因果模型 (Rubin Causal Model, RCM), 讨论关于二值解释变量对响应变量的因果效应分析. 例如, 取得学位是否有助于提高工资? 成立工会对企业债券具有怎样的影响? 它们本质上都可以归结为$X$对于$Y$是否有影响? 如果有, 那么影响有多大?
%\section{处理效应}
%如果个体$i$接受了某种处理$D_i$ (例如服药), 它的响应变量$Y_i$为$Y_i(1)$, 而没有接受处理的响应变量为$Y_i(0)$, 那么我们称这两种结果为潜在结果, $D_i=1$的个体归入处理组 (treatment group), $D_i=0$的个体则归入控制组 (control group).

%进一步, 定义处理行为$D_i$对个体$i$的处理效应 (treatment effect)是个体$i$接受处理的潜在结果与没有接受处理的潜在结果之差, 也即
%\begin{equation}\label{eq10.1}
%  \gamma_i=Y_i(1)-Y_i(0)
%\end{equation}
%显然, 对于不同的个体$i$, 处理效应$\gamma_i$的大小很可能也是不同的, 因此在衡量处理效应时, 通常用平均处理效应 (Average Treatment Effect, ATE)来描述处理效应的平均结果
%$$\tau_\text{ATE}=\E[Y_i(1)-Y_i(0)]$$
%当然, 我们更感兴趣的可能是处理行为对处理组的影响, 它可以用参与者平均处理效应 (Average Treatment Effect on the Treated, ATT)进行衡量
%$$\tau_\text{ATT}=\E[Y_i(1)-Y_i(0)|D_i=1]$$
%在实践中要估计出ATE和ATT是极为困难的一件事, 因为对任何一个个体$i$, 我们不可能同时观测到接受和不接受处理的两种潜在结果, 因此无法计算效用$Y_i(1)-Y_i(0)$, 这称为因果推断的根本难点.

%然而在某些特殊假设下, 仍有可能估计出ATE和ATT. 将观测结果$Y_i$记为
%\begin{equation}\label{eq10.4}
%  Y_i=Y_i(0)+[Y_i(1)-Y_i(0)]\times\, D_i
%\end{equation}
%如果处理行为$D_i$是完全随机分配的, 那么个体的潜在结果和处理行为在统计上独立, 表示为
%\begin{equation}\label{eq10.2}
%  \{Y_i(1),Y_i(0)\}\,\perp D_i
%\end{equation}
%也即潜在结果的好坏不会影响到个体是否接受处理. 此时
%\begin{align*}
%\E[Y_i|D_i=1]&=\E[Y_i(1)|D_i=1]=\E[Y_i(1)] \\
%\E[Y_i|D_i=0]&=\E[Y_i(0)|D_i=0]=\E[Y_i(0)]
%\end{align*}
%故而
%$$\tau_\text{ATE}=\tau_\text{ATT}=\E[Y_i(1)|D_i=1]-\E[Y_i(0)|D_i=0]$$
%因此可以直接使用处理组的样本均值减去控制组的样本均值来估计ATE和ATT, 随机化处理保证了出自基本统计量的均值之差估计量是无偏, 一致和渐近正态的. 实际上, 条件(\ref{eq10.2})可以放松为更弱的条件均值独立假设
%\begin{equation}\label{eq10.3}
%  \E[Y_i(0)|D_i]=\E[Y_i(0)] \wedge \E[Y_i(1)|D_i]=\E[Y_i(1)]
%\end{equation}
%然而随机分配的成本通常很高, 实施起来可能过于困难, 因此在实践中难以实现. 此外, 实验本身也可能对结果产生影响, 例如参与就业培训的人一定知道自己参与了培训, 这点与药物双盲实验不同.

%\begin{remark}
%倘若没有条件均值独立假设(\ref{eq10.3})或更强的假设, 那么直接使用处理组的均值减去控制组的均值将会产生偏误, 此时
%$$\E[Y_i(1)|D_i=1]-\E[Y_i(0)|D_i=0]=\tau_\text{ATT}+\E[Y_i(0)|D_i=1]-\E[Y_i(0)|D_i=0]$$
%由于最后两项相减不为0, 因而无法正确估计ATE和ATT. 从上式还能看出, 如果我们只关心ATT的话, 那么假设(\ref{eq10.3})可以减弱为
%$$\E[Y_i(0)|D_i=1]=\E[Y_i(0)|D_i=0]=\E[Y_i(0)]$$
%尽管它仍是一个很强的假设.
%\end{remark}


%\section{处理的可忽略性}
%\subsection{假设条件}
%在绝大多数情况下, 一些个体至少部分地决定他们是否接受处理, 他们的决策与潜在结果的大小有关, 这就是所谓的自选择 (self-selection)问题. 如果潜在结果的差异是由于个体的可观测特征造成的, 那么通过消除这些特征差异就能消去自选择偏差, 这可以由倾向得分匹配 (Propensity Score Matching, PSM)做到.

%继续延用上一节的框架, 用$X$表示可观测回归元的向量, 并且用$[Y(0),Y(1),D,X]$描述总体, 我们可以观测到$Y_i$, $D_i$和$X_i$时, 而$Y_i$由方程(\ref{eq10.4})给定. 进一步, 以下假设条件成立.
%\begin{proposition}[可忽略性]\label{pro:pro10.1}
%$\{Y_i(1), Y_i(0)\}\perp D_i|X_i$
%\end{proposition}
%以上假设表明对于任意给定的$X_i=x_i$, 潜在结果和处理行为无关, 此时可观测特征相同的处理组和控制组的不可观测特征分布相同, 但假设\ref{pro:pro10.1}并不允许$D$取决于不可观测变量. 如果只关注ATT, 那么只需要下面这一更弱的假设即可.
%\begin{proposition}[均值可忽略性]\label{pro:pro10.2}

%$\E[Y_i(0)|X_i, D_i]=\E[Y_i(0)|X_i]\wedge \E[Y_i(1)|X_i, D_i]=\E[Y_i(1)|X_i].$

%\end{proposition}
%显然, 假设\ref{pro:pro10.1}可以直接推出假设\ref{pro:pro10.2}. 假设\ref{pro:pro10.2}意味着, 倘若我们可以在$X$中观测到足够决定是否处理的信息, 那么$[Y(0),Y(1)]$可能以$X$为条件均值独立于$D$.

%进一步定义
%$$\mu_0(X_i)=\E[Y_i(0)|X_i],\quad \mu_1(X_i)=\E[Y_i(1)|X_i]$$
%于是在假设\ref{pro:pro10.2}下, 以$X$为条件的ATE和ATT是相同的. 为了看到这一点, 定义
%\begin{align*}
%\tau_\text{ATE}(X_i)&=\E[Y_i(1)-Y_i(0)|X_i]=\mu_1(X_i)-\mu_0(X_i) \\
%\tau_\text{ATT}(X_i)&=\E[Y_i(1)-Y_i(0)|X_i,D_i=1]
%\end{align*}
%假设\ref{pro:pro10.2}立即意味着$\tau_\text{ATE}(X_i)=\tau_\text{ATT}(X_i)$. 直观来看, 控制变量$X$更丰富时, 可忽略性越有可能成立, 但是不要把本身可能受到处理行为影响的变量纳入其中, 否则可忽略性一般不太可能成立 (Wooldridge, 2010).

%\begin{remark}
%可忽略性这一假设在根本上是无法检验的, 正如在回归中由于无法观测到随机扰动项, 就无法从根本上检验内生性一样.
%\end{remark}

%\begin{proposition}[共同支撑]\label{pro:pro10.3}
%设$\mathscr{X}$是控制变量的支集, 对所有的$x\in\mathscr{X}$, 总有
%\begin{equation}\label{eq10.5}
%  0<\PP[D_i|X_i=x]<1
%\end{equation}
%\end{proposition}
%共同支撑条件又称重叠条件, 表明对于任意给定的观测特征$X_i=x$, 一部分个体接受处理, 另一部分没有接受处理, 也即同时存在处理组和控制组. 我们称
%$$p(X_i=x)=\PP[D_i=1|X_i=x],\quad x\in\mathscr{X}$$
%为倾向得分 (propensity score), 表示可观测特征为$X_i=x$时个体接受处理的概率. 可忽略性和共同支撑称为强可忽略性, 这是在所有估计$\tau_\text{ATE}$的方法中的一个关键假设.

%\subsection{识别}
%可以证明, 在均值可忽略性与共同支撑假设下, $\tau_{\text{ATE}}$与$\tau_\text{ATT}$可识别, 并且识别$\tau_\text{ATT}$的条件更弱. 注意到$Y_i=Y_i(0)+[Y_i(1)-Y_i(0)]\times D_i$, 根据均值可忽略性假设可得
%\begin{align*}
%\E[Y_i|X_i,D_i]&=\E[Y_i(0)|X_i,D_i]+\{\E[Y_i(1)|X_i,D_i]-\E[Y_i(0)|X_i,D_i]\}\times D_i \\
%&=\E[Y_i(0)|X_i]+\{\E[Y_i(1)|X_i]-\E[Y_i(0)|X_i]\}\times D_i \\
%&=\mu_0(X_i)+[\mu_1(X_i)-\mu_0(X_i)]\times D_i
%\end{align*}
%其中最后一个等号是通过均值可忽略性得到的
%\begin{align}
%\begin{split}
%\E[Y_i|X_i,D_i=0]&=\mu_0(X_i) \\
%\E[Y_i|X_i,D_i=1]&=\mu_1(X_i)
%\end{split}
%\label{eq10.6}
%\end{align}
%另一方面, 在共同支撑假设下, 一旦我们观测到$[Y,X,D]$, 那就可以在很一般地估计
%\begin{align*}
%\E[Y_i|X_i,D_i=0]&=m_0(X_i) \\
%\E[Y_i|X_i,D_i=1]&=m_1(X_i)
%\end{align*}
%而不论可忽略性是否成立, 而它一旦成立, 那么根据(\ref{eq10.6})可知
%\begin{equation}\label{eq10.7}
%  \tau_\text{ATE}(X_i)=m_1(X_i)-m_0(X_i)
%\end{equation}
%由此
%\begin{equation}\label{eq10.8}
%  \tau_\text{ATE}=\E[m_1(X_i)-m_0(X_i)]
%\end{equation}
%其中对于任意的$X_i\in\mathscr{X}$, $m_g(\cdot), g=0, 1$均是可识别的, 这是出自共同支撑假设的结果. 类似地, 无条件ATT可以由
%\begin{equation}\label{eq10.9}
 % \tau_\text{ATT}=\E[m_1(X_i)-m_0(X_i)|D_i=1]
%\end{equation}
%来获得. 当然, $\tau_\text{ATT}$的识别可以在更弱的假设条件下进行: $\E[Y_i(0)|X_i,D_i]=\E[Y_i(0)|X_i]$以及$\PP[D_i|X_i=x]<1$.

%简要总结一下, 在均值可忽略性假设下, 式(\ref{eq10.7})成立, 而如果添加共同支撑假设, 那么可以得到方程(\ref{eq10.8})那样的$\tau_\text{ATE}$. 类似地, 在更弱的条件下也能得到方程(\ref{eq10.9})那样的$\tau_\text{ATT}$.

%建立识别的另一种方式是倾向得分的倒数加权, 首先维持均值可忽略性假设\ref{pro:pro10.2}, 注意到$D_iY_i=D_iY_i(1)$, 利用LIE可得
%\begin{align*}
%\E\left[\left.\frac{D_iY_i}{p(X_i)}\right|X_i\right]&=\E\left[\left.\frac{D_iY_i(1)}{p(X_i)}\right|X_i\right]=\E\left[\left.\E\left[\left.\frac{D_iY_i(1)}{p(X_i)}\right|X_i,D_i\right]\right|X_i\right] %\\
%&=\E\left[\left.\frac{D_i\E[Y_i(1)|X_i,D_i]}{p(X_i)}\right|X_i\right]=\E\left[\left.\frac{D_i\E[Y_i(1)|X_i]}{p(X_i)}\right|X_i\right] \\
%&=\E\left[\left.\frac{D_i}{p(X_i)}\right|X_i\right]\mu_1(X_i)=\mu_1(X_i)
%\end{align*}
%同理可得
%$$\E\left[\left.\frac{(1-D_i)Y_i}{1-p(X_i)}\right|X_i\right]=\mu_0(X_i)$$
%结合这两种结果并使用一些简单的代数运算即可得出
%$$\E\left[\left.\frac{[D_i-p(X_i)]Y_i}{p(X_i)[1-p(X_i)]}\right|X_i\right]=\mu_1(X_i)-\mu_0(X_i)=\tau_{\text{ATE}}(X_i)$$
%如果共同支撑假设\ref{pro:pro10.3}成立并且期望存在, 那么可以根据LIE写出无条件ATE
%$$\tau_\text{ATE}=\E\left[\frac{[D_i-p(X_i)]Y_i}{p(X_i)[1-p(X_i)]}\right]$$
%显然, 上式使用了倾向得分而非回归函数建立了对$\tau_{\text{ATE}}$的识别.

%\subsection{回归调整}
%根据上面的讨论, $\tau_\text{ATE}$和$\tau_\text{ATT}$在均值可忽略性假设与共同支撑假设下可以识别出来, 这一识别策略基于回%归函数$m_1(X_i)=\E[Y_i|X_i,D_i=1]$以及$m_0(X_i)=\E[Y_i|X_i,D_i=0]$.

%假设我们从总体$[Y,X,D]$中得到了一个样本容量为$n$的随机样本, 并且已知$m_1(X_i)$和$m_0(X_i)$, 那么通过某种方法可以获取一致估计量$\hat{m}_1(X_i)$与$\hat{m}_0(X_i)$, 从而在相当弱的条件下可以得到ATE的一致估计量
%\begin{equation}\label{eq10.11}
%  \hat{\tau}_\text{ATE, REG}=n^{-1}\sum_{i=1}^{n}[\hat{m}_1(X_i)-\hat{m}_0(X_i)]
%\end{equation}
%以及ATT的一致估计量
%\begin{equation}\label{eq10.12}
 % \hat{\tau}_\text{ATT, %  REG}=\left(\sum_{i=1}^{n}D_i\right)^{-1}\left\{\sum_{i=1}^{n}D_i[\hat{m}_1(X_i)-\hat{m}_0(X_i)]\right\}
%\end{equation}
%分别称(\ref{eq10.11})和(\ref{eq10.12})为$\tau_\text{ATE}$和$\tau_\text{ATT}$的回归调整估计量.

%现在的问题是如何获得$\hat{m}_1(\cdot)$与$\hat{m}_0(\cdot)$, 为了尽可能的灵活, 人们可以使用非参数估计量, 例如核估计量 (kernal estimator)与级数估计量 (series estimator), 这部分内容具体可参考 Li and Racine (2007).

%另一方面, 人们也可以使用参数估计量. 设$m_0(X_i,\delta_0)$和$m_1(X_i,\delta_1)$为参数函数, 其中$m_0(X,\delta_0)$用$D_i=0$的观测值估计, 而$m_1(X_i,\delta_1)$则用$D_i=1$的观测值估计. 给定$\delta_0$和$\delta_1$的渐近正态一致估计量$\hat{\delta}_0$与$\hat{\delta}_1$, 那么
%$$\hat{\tau}_\text{ATE, REG}=n^{-1}\sum_{i=1}^{n}[m_1(X_i,\hat{\delta}_1)-m_0(X_i,\hat{\delta}_0)]$$
%可以证明, 在一定正则条件下有
%\begin{align*}
%\text{avar}[\sqrt{n}(\hat{\tau}_\text{ATE, REG}-\tau_\text{ATE})]&=\E[\{m_1(X_i,\delta_1)-m_0(X_i,\delta_0)-\tau_\text{ATE}\}^2] \\
%&\quad +\E[\nabla_{\delta_0}m_0(X_i,\delta_0)]\BS_0\E[\nabla_{\delta_0}m_0(X_i,\delta_0)]' \\
%&\quad +\E[\nabla_{\delta_1}m_1(X_i,\delta_1)]\BS_1\E[\nabla_{\delta_1}m_1(X_i,\delta_1)]'
%\end{align*}
%其中$\BS_0$与$\BS_1$分别是$\sqrt{n}(\hat{\delta}_0-\delta_0)$与$\sqrt{n}(\hat{\delta}_1-\delta_1)$的渐近方差. 于是一个简单的协方差矩阵估计量为
%\begin{align*}
%n\cdot\hat{\V}_{\text{ATE, REG}}&=n^{-1}\sum_{i=1}^{n}[m_1(X_i,\hat{\delta}_1)-m_0(X_i,\hat{\delta}_0)-\hat{\tau}_{\text{ATE, REG}}]^2 \\
%&\quad+ \left[n^{-1}\sum_{i=1}^{n}\nabla_{\delta_0}m_0(X_i,\hat{\delta}_0)\right]\hat{\BS}_0\left[n^{-1}\sum_{i=1}^{n}\nabla_{\delta_0}m_0(X_i,\hat{\delta}_0)\right]' \\
%&\quad +\left[n^{-1}\sum_{i=1}^{n}\nabla_{\delta_1}m_1(X_i,\hat{\delta}_1)\right]\hat{\BS}_1\left[n^{-1}\sum_{i=1}^{n}\nabla_{\delta_1}m_1(X_i,\hat{\delta}_1)\right]'
%\end{align*}

%线性回归仍是最流行的一种回归调整, 假设$m_0(X_i,\delta_0)=\alpha_0+X_i'\beta_0$及$m_1(X_i,\delta_1)=\alpha_1+X_i'\beta_1$, 那么$[\hat{\alpha}_0,\hat{\beta}_0]$出自$D_i=0$时$Y_i$对$1, X_i$的回归, 而$[\hat{\alpha}_1,\hat{\beta}_1]$可以类似得到. 于是
%\begin{align*}
%\hat{\tau}_{\text{ATE, REG}}(X_i)&=(\hat{\alpha}_1-\hat{\alpha}_0)+X_i'(\hb_1-\hb_0) \\
%\hat{\tau}_\text{ATE, REG}&=(\hat{\alpha}_1-\hat{\alpha}_0)+\overbar{X}'(\hb_1-\hb_0) \\
%\hat{\tau}_\text{ATT, REG}&=(\hat{\alpha}_1-\hat{\alpha}_0)+\overbar{X}_1'(\hb_1-\hb_0)
%\end{align*}
%其中$\overbar{X}=n^{-1}\sum_{i=1}^{n}X_i$为全样本均值, $\overbar{X}_1=n_1^{-1}\sum_{i=1}^{n}D_iX_i$为处理组子样本均值.

%如果$Y$是一个限值因变量, 例如二值变量, 那么可以利用Probit函数以及Bernoulli QMLE进行估计. 更广泛地说, 对于函数$0<G(\cdot)<1$, 设$[\hat{\alpha}_g,\hat{\beta}_g]$是两个子样本的QML估计量, 那么
%$$\hat{\tau}_\text{ATE, REG}=n^{-1}\sum_{i=1}^{n}[G(\hat{\alpha}_1+X_i'\hb_1)-G(\hat{\alpha}_0+X_i'\hb_0)]$$
%为$\tau_\text{ATE}$的估计量. 由于Bernoulli分布属于LEF, 故只要条件均值是正确设定的, 在其它一些正则条件下, $\hat{\tau}_\text{ATE, REG}$还是一致估计量.

%\subsection{匹配方法}
%既然处理行为仅受到可观测特征的影响, 那么一个自然的想法是找到具有相同可观测特征的未接受处理的个体, 比较控制组和处理组在观测结果上的差异. 这一点类似于回归方法中纳入足够的控制变量的行为.

%显然, 如果可观测特征太多, 直接使用$X$进行匹配可能遇到数据稀疏问题, 甚至找不到足够相似的$X_h$和$X_i$进行匹配. 为此需要将高维数据降维, 例如使用某个实值函数$f$, 将高维的$X_i$压缩为一维的$f(X_i)$, 进而根据$f(X_i)$进行匹配.

%首先考虑使用向量范数, 定义Mahalanobis距离
%$$d(h,i)=(X_h-X_i)'\hat{\BS}^{-1}_X(X_h-X_i)$$
%其中$\hat{\BS}_X$是协变量的协方差矩阵. 根据$d(h,i)$进行匹配的方法称为Mahalanobis匹配, 事实上它是协变量匹配中的一种, 我们还可以选取其它距离函数来度量距离, 协变量匹配的渐近性质已由Abadie and Imbens (2006)给出.

%Mahalanobis匹配的缺点是, 如果协变量个数过多或样本容量太少, 则难以找到好的匹配. Rosenbaum and Rubin (1983)提出了目前最为流行的倾向得分匹配 (Propensity Score Matching, PSM), 也即使用倾向得分来度量距离, 并且一旦知道了倾向得分, 那么可以直接利用Abadie and Imbens (2006)的结果, 得到的匹配估计量在合适的正则条件下是一致且渐近正态的.

%然而在仅有的观测数据中, 倾向得分是未知的, 我们仅能对其进行估计, 而估计得到的倾向得分会使得匹配估计量的统计性质非常复杂, bootstrap法在某些情况下不再适用. PSM的具体步骤见邱嘉平 (2020).

\chapter{限值因变量模型}
上一章提到的Probit模型是限值因变量 (Limited Dependent Variable, LDV)模型中的特例, 它的因变量取值只有0和1. 本章使用极大似然估计法, 简要讨论一些常见的LDV模型, 包括二值响应模型, 归并回归模型, 断尾回归模型以及样本选择模型等.
\section{二值响应模型}
\subsection{Probit与Logit}
在二值响应模型中, 因变量取值范围为$\{0,1\}$, 我们感兴趣的为响应概率
$$P(x)=\PP[Y=1|X=x]$$
以及回归模型
\begin{align}
Y&=P(X)+e \label{eq10.1} \\
\E[e|X]&=0 \nonumber
\end{align}
如果有$P(X)=X'\beta$, 那么(\ref{eq10.1})为线性概率模型 (Linear Probability Model, LPM)
$$Y=X'\beta+e$$
此时OLS估计量一定不是一致估计量, 因为$e$服从两点分布, 要么为$1-X'\beta$, 要么为$-X'\beta$, 故而$X_i$与$e_i$必定相关. 不仅如此, OLS的预测值还会超出$[0,1]$这一范围.

为了克服LPM的局限性, 选取某个值域严格位于$(0,1)$的实值函数$G$, 此时响应概率为
\begin{equation}\label{eq10.5}
  \PP[Y=1|X]=G(X'\beta)
\end{equation}
非线性函数$G$可以选取为
\begin{equation}\label{eq10.2}
  G(z)=\text{e}^z/(1+\text{e}^z)=\Lambda(z)
\end{equation}
它是标准Logistic随机变量的累积分布函数, (\ref{eq10.1})为Logit模型. 而如果函数$G$为
\begin{equation}\label{eq10.3}
  G(z)=\int_{-\infty}^{z}\phi(v)\,\text{d}v=\Phi(z)
\end{equation}
则(\ref{eq10.1})为之前提到的Probit模型, 其中$\phi$为标准正态随机变量的概率密度. 显然, Probit函数与Logit函数都是严格递增的, 并且当$z\to-\infty$时, $G(z)\to 0$, 而当$z\to\infty$时, $G(z)\to1$.

如同上一章开头的那样, Probit模型和Logit模型可以由潜变量模型得到, 也即
\begin{equation}\label{eq10.4}
  Y^\ast=X'\beta+e,\quad Y=\mathbbm{1}[Y^\ast>0]
\end{equation}
其中$Y^\ast$是不可观测的, 仅能观测到指示$Y^\ast$符号的二值变量. 此外, 假设$e$独立于$X$, 并且要么服从标准Logistic分布, 要么服从标准正态分布\footnote{显然这已经排除了潜变量模型中$e$存在异方差的情况, 与通常的线性回归或非线性回归不同, 潜变量模型的异方差性会导致MLE不一致. 这部分内容具体见 Wooldridge (2010).}. 无论哪种情况, 对于任意实数$z$总有$G(z)=1-G(-z)$. 于是对于模型(\ref{eq10.4}), 响应概率为
\begin{align*}
\PP[Y=1|X]&=\PP[Y^\ast>0|X]=\PP[e>-X'\beta|X] \\
&=1-G(-X'\beta)=G(X'\beta)
\end{align*}
这就得到了式(\ref{eq10.5})的情形.

对于二值响应模型, 重要的是解释回归元$X_j$对响应概率$\PP[Y=1|X]$的影响. 注意到
$$\E[Y|X]=\PP[Y=1|X]=G(X'\beta)$$
因此$\beta$不能像线性概率模型那样直接解释. 如果$X_j$是连续型的, 则它在$p(X)=\PP[Y=1|X]$上的回归导数为
$$\frac{\partial p(X)}{\partial X_j}=g(X'\beta)\beta_j,\quad g(z)=\frac{\text{d}G(z)}{\text{d}z}$$
这里的$g$为$G$的导数. 而如果$X_j$是二值的, 以$X_1$为例, 那么$X_1$对$p(X)$的影响为
$$G(\beta_0+\beta_1+\beta_2x_2+\cdots+\beta_kx_k)-G(\beta_0+\beta_2x_2+\cdots+\beta_kx_k)$$
利用随机样本, 我们可以对偏效应进行估计. 假设$\hb$为ML估计量, 那么当连续型变量$X_j$变动很小时, 它对$p(X)$的平均边际效应 (Average Marginal Effect, AME)为
$$n^{-1}\sum_{i=1}^{n}g(X_i'\hb)\hb_j$$
对于二元变量$X_1$也可以类似得到
$$n^{-1}\sum_{i=1}^{n}[G(\hb_0+\hb_1+\cdots+X_{ki}\hb_k)-G(\hb_0+X_{2i}\hb_2+\cdots+X_{ki}\hb_k)]$$
为了利用CMLE, 类似于上一章的做法, Probit模型和Logit模型在观测值$i$上的对数似然为
$$l_i(\beta)=Y_i\log G(X_i'\beta)+(1-Y_i)\log[1-G(X_i'\beta)]$$
根据定义, ML估计量$\hb$是问题
$$\max_{\beta\in\Theta}\,n^{-1}\sum_{i=1}^{n}\log l_i(\beta)$$
的唯一解.

可以证明, 在模型正确设定的情况下, 如果$\E[X_iX_i']$正定且有限, 则这Probit模型和Logit模型的ML估计量都是有效的渐近正态一致估计量, 也即
\begin{align*}
&\sqrt{n}(\hb_{\text{P}}-\beta_\text{P})\xrightarrow{d}N(0,\V_\text{P}) \\
&\sqrt{n}(\hb_{\text{L}}-\beta_\text{L})\xrightarrow{d}N(0,\V_\text{L})
\end{align*}
其中
\begin{align*}
\V_\text{P}&=\Q_\text{P}^{-1}=\E[\lambda(X_i'\beta_\text{P})\lambda(-X_i'\beta_{\text{P}})X_iX_i']^{-1} \\
\V_\text{L}&=\Q_\text{L}^{-1}=\E[\Lambda(X_i'\beta_\text{L})(1-\Lambda(X_i'\beta_\text{L}))X_iX_i']^{-1}
\end{align*}
这里的$\lambda(\cdot)=\phi(\cdot)/\Phi(\cdot)$为IMR. 此时$\hb_\text{P}$和$\hb_\text{L}$的协方差矩阵估计量为
\begin{align*}
\hat{\V}_\text{P}^0&=\left[\sum_{i=1}^{n}\lambda(X_i'\hb_\text{P})\lambda(-X_i'\hb_\text{P})X_iX_i'\right]^{-1} \\
\hat{\V}_\text{L}^0&=\left\{\sum_{i=1}^{n}\Lambda(X_i'\hb_\text{L})[1-\Lambda(X_i'\hb_\text{L})]X_iX_i'\right\}^{-1}
\end{align*}

而如果只有条件均值设定正确, 由于两点分布属于LEF, 于是在正则条件下的QML估计量$\hb_\text{P}$和$\hb_\text{L}$仍是一致的, 但渐近方差$\V_\text{P}$, $\V_\text{L}$及其它们的估计量会更为复杂.


\subsection{内生性问题}
考虑如下潜变量结构模型
\begin{align}
Y^\ast&=X_1'\beta_1+\beta_2X_2+e_1 \label{eq10.6} \\
X_2&=X_1'\gamma_1+Z'\gamma_2+e_2 \label{eq10.7} \\
Y&=\mathbbm{1}[Y^\ast>0]
\end{align}
其中$X_2$是一个内生的连续型解释变量\footnote{由于$X_2$在假设下服从正态分布, 因此必须为连续型随机变量, 而不能是离散的.}, 而$X_1$中的解释变量均是外生的, $Z$包含了不在$X_1$中的排他性工具, 并且还假定$[e_1,e_2]$和$[X_1,X_2]$独立.

为了应用CMLE, 还需要假定扰动项服从联合正态分布, 也即
$$\left.\begin{bmatrix}
    e_1 \\
    e_2
  \end{bmatrix}\right|[X_1,Z]\sim N\left(\begin{bmatrix}
                                           0 \\
                                           0
                                         \end{bmatrix},\begin{bmatrix}
                                                         1 & \sigma_{12} \\
                                                         \sigma_{21} & \sigma_2^2
                                                       \end{bmatrix}\right)$$
此时
\begin{align*}
Y^\ast&=\mu(\theta)+\varepsilon \\
\mu(\theta)&=X_1'\beta_1+\beta_2X_2+\delta(X_2-X_1'\gamma_1-Z'\gamma_2) \\
e_1&=\delta e_2+\varepsilon \\
\delta&=\sigma_{12}/\sigma_2^2 \\
\varepsilon&\sim N(0,\sigma_\varepsilon^2) \\
\sigma_\varepsilon^2&=1-\sigma_{12}^2/\sigma_2^2
\end{align*}
并且随机误差项$\sigma_\varepsilon$独立于$e_2$, 因此也独立于$X_2$. 根据以上论述, 可以得到$[Y,X_2]$的条件联合概率密度
$$f(y,x_2|X_1,Z)=\Phi\left(\frac{\mu(\theta)}{\sigma_\varepsilon}\right)^{y}\left[1-\Phi\left(\frac{\mu(\theta)}{\sigma_\varepsilon}\right)\right]^{1-y}\frac{1}{\sigma_2}\phi\left(\frac{x_2-X_1'\gamma_1-Z'\gamma_2}{\sigma_2}\right)$$
于是ML估计量是以下最大化问题的唯一解
\begin{align*}
\max_{\theta\in\Theta}\,&\sum_{i=1}^{n}\left\{Y_i\log\Phi\left(\frac{\mu_i(\theta)}{\sigma_\varepsilon}\right)+(1-Y_i)\log\left[1-\Phi\left(\frac{\mu_i(\theta)}{\sigma_\varepsilon}\right)\right]\right\} \\
&-\frac{n}{2}\log2\uppi\sigma_2^2-\frac{1}{2\sigma_2^2}\sum_{i=1}^{n}(X_{2i}-X_{1i}'\gamma_1-Z_i'\gamma_2)^2
\end{align*}
以上方法称为IV Probit.

由于MLE在数值计算上可能不易收敛 (尤其是$e_1$和$e_2$高度相关, 或者含有多个内生变量的时候), 因此 Rivers and Vuong (1988)提出了两步法估计, 它也是CF方法的应用. 设$\rho$为$e_1$与$e_2$的相关系数, 于是$\sigma_{21}=\rho\sigma_2$, $\delta=\rho/\sigma_2$, 以及
$$\sigma_\varepsilon^2=1-\delta\sigma_{12}=1-\rho^2$$
根据$e_1=\delta e_2+\varepsilon$可知
\begin{align*}
Y^\ast&=X_1'\beta_1+\beta_2X_2+\delta e_2+\varepsilon \\
\varepsilon|X_1, X_2, e_2&\sim N(0,1-\rho^2)
\end{align*}
可以证明
$$\PP[Y=1|X_1,X_2,e_2]=\Phi[(X_1'\beta+\beta_2X_2+\delta e_2)/(1-\rho^2)^{\frac{1}{2}}]$$
一旦能够观察到$e_2$, 那么$Y$对$X_1$, $X_2$与$e_2$的Probit会产生
\begin{equation}\label{eq10.8}
  \left[\frac{\beta_1}{\sqrt{1-\rho^2}},\frac{\beta_2}{\sqrt{1-\rho^2}},\frac{\delta}{\sqrt{1-\rho^2}}\right]
\end{equation}
的一致估计量. 然而$e_2$是不可观测的, 此时可以使用OLS估计方程(\ref{eq10.7})并取得残差$\hat{e}_2$ (一阶段), 然后用它将$e_2$替换掉, 再实施$Y$对$X_1$, $X_2$与$\hat{e}_2$的Probit (二阶段), 这样得到的CF估计量的渐近性质已由 Rivers and Vuong (1988)给出. 值得注意的是, $[e_1,e_2]$的联合正态对于CF估计不是必要的, 但对于MLE则必不可少.

相较于直接使用MLE, 两步法的优点之一是计算简单, 由于一阶段回归的误差被代入到了二阶段, 因此CF估计量不如ML估计量有效率, 也不易获得合适的标准误. 而一旦潜在的分布假设成立, MLE就可以取得一致和有效的估计量, 标准误也更容易计算.

此外, 两步法IV Probit模型估计的是(\ref{eq10.8})而非原本未经缩放的系数, 因此CF估计量的绝对值比ML估计量的更大, 因此需要通过软件报告出的$\rho$值进行调整. 此外, Rivers and Vuong (1988)还证明了在模型恰好识别的情况下, ML估计量和CF估计量是相同的.

\section{断尾回归模型}
考虑经典的线性回归模型
\begin{align*}
Y&=X'\beta+e \\
e|X&\sim N(0,\sigma^2)
\end{align*}
假设$c$为某个已知常数, 我们只能观测到$Y_i\geq c$的数据, 而$Y_i<c$时的数据是缺失的, 此时称随机变量$Y$是断尾的 (truncated). 此时$Y$的条件概率密度为
\begin{equation}\label{eq10.18}
  f(y|Y>c)=\frac{f(y)}{\PP[Y>c]}
\end{equation}
其中$f(y)$是$Y$不存在断尾时的概率密度. 进一步有
\begin{align}
\E[Y|X, Y>c]&=X'\beta+\sigma\E\left[\left.\frac{e}{\sigma}\right|\frac{e}{\sigma}>\frac{c-X'\beta}{\sigma}\right] \nonumber \\
&=X'\beta+\sigma\lambda\left(-\frac{c-X'\beta}{\sigma}\right) \label{eq10.17}
\end{align}
由于$Y_i\geq c$是样本可观测的条件, 因此式(\ref{eq10.17})表明直接用OLS估计$Y_i=X_i'\beta+e_i$是不一致的. 此外, OLS预测值可能出现$\hat{Y}\leq c$的不可能情形. 为了克服这种困难, 我们可以使用CMLE.

首先注意到$Y|X$在断尾前的分布为$N(X'\beta,\sigma^2)$, 其概率密度函数为
$$f(y|X_i)=\frac{1}{\sqrt{2\uppi\sigma^2}}\text{exp}\left[-\frac{1}{2}\left(\frac{y-X_i'\beta}{\sigma}\right)^2\right]=\frac{1}{\sigma}\phi\left(\frac{y-X_i'\beta}{\sigma}\right)$$
另一方面有
\begin{align*}
\PP[Y>c|X]&=1-\PP\left[\left.\frac{Y-X'\beta}{\sigma}\leq\frac{c-X'\beta}{\sigma}\right|X\right] \\
&=1-\Phi\left(\frac{c-X'\beta}{\sigma}\right)
\end{align*}
根据(\ref{eq10.18})可知$Y$断尾后的密度为
$$f(y|X_i,Y>c)=\frac{\sigma^{-1}\phi[(y-X_i'\beta)/\sigma]}{1-\Phi[(c-X_i'\beta)/\sigma]}$$
由此可以得到观测值$i$的条件对数似然
$$l_i(\theta)=-\frac{1}{2}\log2\uppi\sigma^2-\frac{1}{2}\left(\frac{Y_i-X_i'\beta}{\sigma}\right)^2-\log\left[1-\Phi\left(\frac{c-X_i'\beta}{\sigma}\right)\right]$$
由此可以得到ML估计量, 并且在$\E[X_iX_i']$非奇异和其它正则条件下, 断尾回归的ML估计量是一致和渐近正态的.
\section{归并回归模型}
\subsection{Tobit模型}
假设$Y$是一个在正值上连续的随机变量, 但它的值取0的概率为正, 此时$Y$的分布是混合型的. 如果直接使用OLS, 那么得到的预测值同样可能为负, 并且OLS估计量不是一致的.

为了看到这一点, 考虑如下归并回归 (censored regression)模型
\begin{align}
Y^\ast&=X'\beta+e \label{eq10.9} \\
Y&=\max\{0,Y^\ast\} \nonumber
\end{align}
其中回归方程(\ref{eq10.9})满足经典的线性模型假设, 并且随机扰动项服从条件正态分布$e|X\sim N(0,\sigma^2)$. 这里的数据归并意味着小于0的$Y$都被压缩到了一个点上, 上述归并回归模型又称为第 I类Tobit模型, 因为它是由Tobin (1958)提出的.

首先求$Y>0$的子样本的条件期望
\begin{align}
\E[Y|X,Y>0]&=\E[Y^\ast|X,Y>0]=\E[X'\beta+e|X,Y>0]\nonumber \\
&=X'\beta+\E[e|X,e>-X'\beta]=X'\beta+\sigma\lambda(X'\beta/\sigma) \label{eq10.10}
\end{align}
其中最后一个等号是通过标准正态的断尾公式\footnote{设$Y\sim N(0,1)$, $c$为任意常数, 那么$\E[Y|Y>c]=\lambda(-c)=\displaystyle\frac{\phi(c)}{1-\Phi(c)}$.}得到的. 进一步求全样本的条件期望
\begin{align*}
\E[Y|X]&=0\cdot \PP[Y=0|X]+\E[Y|X,Y>0]\cdot\PP[Y>0|X] \\
&=\E[Y|X,Y>0]\cdot\PP[Y>0|X]
\end{align*}
注意到
\begin{equation}
\PP[Y>0|X]=\PP[Y^\ast>0|X]=\PP[e>-X'\beta|X]=\Phi(X'\beta/\sigma) \label{eq10.15}
\end{equation}
因此
\begin{align}
  \E[Y|X]&=\Phi(X'\beta/\sigma)[X'\beta+\sigma\lambda(X'\beta/\sigma)] \nonumber \\
  &=X'\beta\Phi(X'\beta/\sigma)+\sigma\phi(X'\beta/\sigma) \label{eq10.11}
\end{align}
根据(\ref{eq10.10})和(\ref{eq10.11})可知, 无论是全样本还是子样本, 对方程(\ref{eq10.9})的OLS回归都不会产生一致估计量.

然而, 我们可以使用MLE对Tobit模型进行估计. 类似可以写出
\begin{equation}\label{eq10.12}
  \PP[Y=0|X]=\PP[Y^\ast<0|X]=\PP[e<-X'\beta|X]=\Phi(-X'\beta/\sigma)
\end{equation}
此时可以写出$Y$的条件概率密度为
$$f(y|X_i)=\left[\Phi\left(-\frac{X_i'\beta}{\sigma}\right)\right]^{\mathbbm{1}[y=0]}\left[\frac{1}{\sigma}\phi\left(\frac{y-X_i'\beta}{\sigma}\right)\right]^{\mathbbm{1}[y>0]}$$
于是ML估计量为
$$\hat{\theta}_\text{ML}=\arg\max_{\theta\in\Theta}\,n^{-1}\sum_{i=1}^{n}\log f(Y_i|X_i)$$
其中$\theta=[\beta',\sigma]'$.

Tobit估计量的渐近性质由Amemiya (1973)给出, 就$\{X_i\}$取固定常数序列的情况而言, 如果$X_i$是有界的并且$\lim n^{-1}\sum_{i=1}^{n}X_iX_i'$非奇异, 则Tobit估计量是一致的和渐近正态的\footnote{从这个角度来看, 对于$X_i$随机的情况, 一致性和渐近正态性的一个充分条件是$\E[X_iX_i']$的非奇异性.}. 一旦通过MLE获得了估计量和渐近标准误, 三大检验方法都可以根据需要来轻易使用.

现在来看Tobit模型的解释, 它和上一节的二值选择模型一样无法直接用回归系数来解释边际效应. 假设$X_j$为连续型随机变量, McDonald and Moffitt (1980)给出了一个特别有用的分解式
\begin{align}
\begin{split}
  \frac{\partial \E[Y|X]}{\partial X_j}&=\frac{\partial \PP[Y>0|X]}{\partial X_j}\cdot\E[Y|X,Y>0]\\
  &\quad +\PP[Y>0|X]\cdot\frac{\partial \E[Y|X,Y>0]}{\partial X_j}
\end{split}
\label{eq10.13}
\end{align}
对IMR求导可得
$$\lambda'(c)=-\lambda(c)[c+\lambda(c)]$$
从而
\begin{align}
\frac{\partial \E[Y|X,Y>0]}{\partial X_j}&=\beta_j+\beta_j\lambda'(X'\beta/\sigma) \nonumber \\
&=\beta_j\{1-\lambda(X'\beta/\sigma)[X'\beta/\sigma+\lambda(X'\beta/\sigma)]\} \label{eq10.14}
\end{align}
根据(\ref{eq10.15})又可得到
\begin{equation}\label{eq10.16}
  \frac{\partial\PP[Y>0|X]}{\partial X_j}=(\beta_j/\sigma)\phi(X'\beta/\sigma)
\end{equation}
将(\ref{eq10.14})和(\ref{eq10.16})代入到(\ref{eq10.13})有
$$\frac{\partial\E[Y|X]}{\partial X_j}=\Phi(X'\beta/\sigma)\beta_j$$
于是AME可以通过下式估计而来
$$\left[n^{-1}\sum_{i=1}^{n}\Phi(X_i'\hb/\hat{\sigma})\right]\hat{\beta}_j$$
其中$\hb$和$\hat{\sigma}$都是ML估计量. 而对于二元变量$X_j$, 根据(\ref{eq10.14})可知AME可估计为
\begin{align*}
&n^{-1}\sum_{i=1}^{n}\{[\Phi(\hat{w}_{1i}/\hat{\sigma})\hat{w}_{1i}+\hat{\sigma}\phi(\hat{w}_{1i}/\hat{\sigma})] -[\Phi(\hat{w}_{0i}/\hat{\sigma})\hat{w}_{0i}+\hat{\sigma}\phi(\hat{w}_{0i}/\hat{\sigma})]\}
\end{align*}
其中$\hat{w}_{1i}=X_{i(-j)}'\hb_{-j}+\hb_j$, $\hat{w}_{1i}=X_{i(-j)}'\hb_{-j}$, 而下标$-j$表示剔除了变量$X_j$.

Tobit潜变量模型中$e$的非正态性和异方差性同样会导致MLE不一致, Powell (1984)提出了归并最小绝对偏差 (Censored Least Absolute Deviation, CLAD)估计量来解决这一问题. 考虑如下归并中值回归
\begin{align*}
Y^\ast&=X'\beta+e \\
\text{Med}\,[e|X]&=0 \\
Y&=\max\,\{0,Y^\ast\}
\end{align*}
这里的$Y^\ast$为潜变量并且满足$\text{Med}\,[Y^\ast|X]=X'\beta$, 限值因变量$Y$归并到0. 可以证明, 归并中值回归意味着
$$\text{Med}\,[Y|X]=\max\,\{0,X'\beta\}$$
Powell (1984)提出了通过求解
$$\max_\beta n^{-1}\sum_{i=1}^{n}|Y_i-\max\,\{0,X_i'\beta\}|$$
来估计$\beta$. 由于$|Y-\max\,\{0,X'\beta\}|$是关于$\beta$的连续函数, 因此CLAD估计量在一定正则条件下是一致的. 然而, 由于目标函数并非二阶连续可微, 因此要建立CLAD估计量的渐近正态性就十分困难. 当Tobit模型设定正确时, CLAD估计结果应该和Tobit估计结果差不多.
\subsection{栅栏模型}
根据之前的讨论, 在第I类Tobit模型中, 一个解释变量对$\PP[Y>0|X]$和$\E[Y|X,Y>0]$的边际效应必须有相同的符号, 但我们可能并不想施加这样的约束. 为此, Cragg (1971)提出了第I类Tobit模型的扩展, 也即栅栏模型 (hurdle model), 该模型允许分别使用单独的机制来确定参与决策 (即$Y>0$还是$Y=0$)与数量决策 ($Y>0$时它的大小).

定义选择指示符$s$, 它是一个二值变量, 决定了$Y=0$还是严格为正, 再定义一个具有连续分布的非负潜变量$W^\ast$, 假设$Y$由下式生成
$$Y=s\cdot W^\ast$$
选择指示符$s$在观测上等价于$\mathbbm{1}[Y>0]$, 而$W^\ast$只有在$s=1$时才能被观测到, 此时$Y=W^\ast$. 进一步假设
\begin{equation}\label{eq10.27}
  \text{D}[W^\ast|s,X]=\text{D}[W^\ast|X]
\end{equation}
它表明$s$和$W^\ast$以$X$为条件相互独立. 换言之, 以可观测的协变量$X$为条件, 决定$s$和$W^\ast$的机制是相互独立的. 此时
$$\E[Y|X,s]=s\E[W^\ast|X,s]=s\E[W^\ast|X]$$
而当$s=1$时有
$$\E[Y|X,Y>0]=\E[W^\ast|X]$$
进而
$$\E[Y|X]=\PP[s=1|X]\E[W^\ast|X]$$

在Cragg模型中, 假定条件独立性假设(\ref{eq10.27})成立, 并且二值变量$s$服从Probit模型
$$\PP[s=1|X]=\Phi(X'\gamma)$$
并且还假定潜变量$W^\ast$有一个断尾正态分布. 定义$W^\ast=X'\beta+e$, 其中给定$X$时$e$服从一个带有断尾点$-X'\beta$, 方差为$\sigma^2$的正态分布. 因为当$Y>0$时有$Y=W^\ast$, 根据(\ref{eq10.18})可知条件概率密度为
$$f(y|X,Y>0)=\sigma^{-1}[\Phi(X'\beta/\sigma)]^{-1}\phi[(y-X'\beta)/\sigma]$$
进而有
$$f(y|X)=[1-\Phi(X'\gamma)]^{\mathbbm{1}[y=0]}\{\sigma^{-1}\Phi(X{'}\beta)[\Phi(X{'}\gamma/\beta)]^{-1}\phi[(y-X{'}\beta)/\sigma]\}^{\mathbbm{1}[y>0]}$$
上式清晰地表明了在没有引入$s$和$W^\ast$的情况下是如何设定模型的, 并且当$\gamma=\beta/\sigma$时, 断尾正态栅栏 (Truncated Normal Hurdle, TNH)模型就简约为第I类Tobit模型.

一旦给定i.i.d.随机样本$\{(X_i,Y_i)\}$, 则观测值$i$的条件对数自然为
\begin{align*}
l_i(\theta)&=\mathbbm{1}[Y_i=0]\log[1-\Phi(X_i'\gamma)]+\mathbbm{1}[Y_i>0]\log\Phi(X_i'\gamma) \\
&\quad +\mathbbm{1}[Y_i>0]\{-\log (X_i'\beta/\sigma)+\log\phi[(Y_i-X_i'\beta)/\sigma]-\log\sigma\}
\end{align*}
由于参数$\theta=[\gamma',\beta',\sigma]'$允许自由变动, 容易看出ML估计量$\hat{\gamma}$正好是$s_i=\mathbbm{1}[Y_i>0]$对$X_i$的Probit估计量.

由于条件分布$\text{D}[Y|X,Y>0]$在第I类Tobit模型和TNH模型中是相同的, 故而
$$\E[Y|X,Y>0]=X'\beta+\sigma\lambda(X'\beta/\sigma)$$
所不同的是TNH模型允许$\PP[Y>0|X]$服从无约束的Probit模型, 因此在TNH模型中有
$$\E[Y|X]=\Phi(X'\gamma)[X'\beta+\sigma\lambda(X'\beta/\sigma)]$$
于是解释变量$X_j$对$\E[Y|X]$的边际效应为
$$\frac{\partial \E[Y|X]}{X_j}=\gamma_j\phi(X'\gamma)[X'\beta+\sigma\lambda(X'\beta/\sigma)]+\beta_j\Phi(X'\gamma)\theta(X'\beta/\sigma)$$
其中$\theta(z)=1-\lambda(z)[z-\lambda(z)]$. 通过MLE得到估计量后, 就可以根据上式得出估计AME的表达式.

\section{样本选择问题}
所谓的选择性样本通常用来描述一个不是从总体中随机抽取得到的样本, 上述提到的数据断尾也是样本选择 (sample selection)问题的一个特例, 而另一个样本选择问题则是从属断尾 (incidental truncation). 当存在样本选择问题时, 除非在特定类型下, 否则样本无法代表总体, 使用OLS就会存在偏差.
\subsection{一致性OLS估计}
对于总体线性回归模型
\begin{align}
Y&=X'\beta+e \nonumber \\
\E[e|X]&=0 \label{eq10.23}
\end{align}
如果数据是完全随机缺失的 (Missing Completely At Random, MCAR), 也即数据缺失的原因在统计上独立于影响$Y$的可观测因素与不可观测因素, 那么缺失数据不会在统计上造成任何影响. 从实际上来看, 我们仍可以假定数据是从总体中随机抽样得到的.

另一方面, 如果样本仅由外生解释变量决定, 那么此时就出现外生样本选择问题, 此时也很容易得到使得OLS为一致 (甚至无偏)估计的条件. 在总体中抽取随机样本可得
\begin{equation}\label{eq10.19}
  Y_i=X_i'\beta+e_i
\end{equation}
显然, 如果对每个个体$i$都能观测到$Y_i$和$X_{ji}$, 那么就可以使用OLS. 然而出于某些原因, 某个个体$i$的$Y_i$或自变量无法观测到, 但至少可以观测到某些个体$i$的变量集的全部信息.

对于每一个个体$i$, 定义一个选择指标$s_i$, 如果观测到了$[Y_i,X_i]$的全部信息, 则$s_i=1$, 否则$s_i=0$. 从定义上看, $s_i=1$表示我们将用到这个观测, 而$s_i=0$就表示用不到这一观测. 我们感兴趣的是, OLS估计量在选择性样本 (即使用$s_i=1$的观测)上的统计性质. 考虑估计以下方程
\begin{equation}\label{eq10.20}
  s_iY_i=s_iX_i'\beta+se_i
\end{equation}
当$s_i=1$时, 上式就变为(\ref{eq10.19}); 当$s_i=0$时, 上式两端为0, 这显然没有告诉我们关于$\beta$的任何信息. 将$s_iY_i$对$s_iX_i$回归, 等同于利用$s_i=1$的观测将$Y_i$对$X_i$回归. 因此, 可以通过研究一个随机样本来研究(\ref{eq10.20})以了解$\hb$的统计性质. 此时有
$$\hb=\beta+\left(n^{-1}\sum_{i=1}^{n}s_iX_iX_i'\right)^{-1}\left(n^{-1}\sum_{i=1}^{n}s_iX_ie_i\right)$$
一旦我们假定
\begin{equation}\label{eq10.21}
  \E[s_iX_ie_i]=0
\end{equation}
以及相关矩条件成立, 则OLS估计量是一致的. 关于条件(\ref{eq10.21})的一个充分条件是
\begin{equation}\label{eq10.22}
  \E[e_i|X_i,s_i]=0
\end{equation}
它允许选择指示符$s$与可观测变量$X$相关, 而与随机误差项$e$无关, 这称为外生样本选择 (exogenous sample selection).

一旦条件(\ref{eq10.23})成立, 且$s$是$X$的一个确定性函数, 那么条件(\ref{eq10.22})自然也成立. 换言之, 如果存在非随机函数$h$使得$s=h(X)$, 就产生了外生性抽样并排除了$s$受不可观测效应影响的机制. 在外生样本选择的情况下, 条件(\ref{eq10.22})意味着
$$\E[Y|X,s]=\E[Y|X]=X'\beta$$
这就表明无论是$Y$还是$X$发生了数据缺失, 在选择性样本上的OLS仍是一致的. 关于OLS估计量一致性的结论还可以推广到2SLS上, 也即当$\E[e_i|Z_i,s_i]=0$时, 2SLS通常也是一致的.
\subsection{从属断尾}
正如之前提到的那样, 样本选择问题的主要形式为从属断尾. 对于总体线性回归模型
\begin{align*}
Y&=X'\beta+e \\
\E[e|X]&=0
\end{align*}
假设总能观测到解释变量$X$, 但只能观测到总体$Y$的一个子集, 并且是否能观测到$Y$不取决于$Y$的结果. 此时因变量$Y$的断尾就是从属的, 它取决于另一个变量\footnote{例如在工资方程中, 如果一个人有工作, 那么确实可以观测到工资, 然而对于失业人群而言, 工资就是缺失值.}. 事实上, 前面提到的TNH模型已应用在了缺失数据问题, 但它要求参与决策和数量决策过程条件独立, 并且$Y$同样是大于等于0的, 而非直接缺失.

利用选择指示符$s_i$, 当且仅当$s_i=1$时可以完全观测到$[Y_i,X_i]$, 而当$s_i=0$时$Y_i$缺失, 于是$Y$在受选择样本中的条件均值为
\begin{equation}\label{eq10.24}
  \E[Y|X,s=1]=X'\beta+\E[e|X,s=1]
\end{equation}
由于$s$取决于其它变量影响, 故而可以设\footnote{为了避免高度共线性, 通常要求$Z$至少包含一个不在$X$中的排他性约束变量, 该变量只影响选择方程而不影响结果方程.}
\begin{equation}\label{eq10.26}
  s=\mathbbm{1}[Z'\gamma+u]
\end{equation}
进一步有
\begin{align*}
\E[e|X,s=1]&=\E[e|u>-Z'\gamma]
\end{align*}
假设$e$独立于$X$, 并且$e$在$u$上的线性投影为$e=\rho u+\varepsilon$, 并且$u$和$\varepsilon$独立. 因此方程(\ref{eq10.24})变为
$$\E[Y|X,s=1]=X'\beta+\rho\E[u|u>-Z'\gamma]=X'\beta+\rho g(Z'\gamma)$$
如果$u\sim N(0,1)$, 那么$g(Z'\gamma)=\lambda(Z'\gamma)$, 从而
\begin{equation}\label{eq10.25}
  \E[Y|X,s=1]=X'\beta+\rho\lambda(Z'\gamma)
\end{equation}
因此只要$\rho\neq0$, 根据受选择样本获得的OLS估计量就不是一致的. 由此可见, 从属断尾算是遗漏变量问题的一个特例.

由于$\gamma$是未知的, 我们无法对每个$i$计算$\lambda(Z_i'\gamma)$, 但根据(\ref{eq10.26})和$u\sim N(0,1)$可知
$$\PP[s=1|Z]=\Phi(Z'\gamma)$$
因此可以通过$s$对$Z$的Probit模型来估计$\gamma$, 然后再来估计$\beta$, 这称为Heckit方法 (Heckman, 1976, 1979), 样本选择模型又称为第II类Tobit模型.

具体而言, Heckit方法也是两步法估计:
\begin{itemize}
  \item 利用$n$个观测值, 实施$s_i$对$Z_i$的Probit模型并得到$\hat{\gamma}$, 然后对每个$i$计算IMR $\hat{\lambda}_i=\lambda(Z_i'\hat{\gamma})$.
  \item 利用受选择样本做$Y_i$对$X_i$和$\lambda_i$的OLS回归: $Y_i=X_i'\beta+\rho\hat{\lambda}_i+e_i$.
\end{itemize}
由此得到的OLS估计量在标准条件下是一致和渐近正态的, 根据生成回归元的内容, 基于通常的$T$检验统计量就能检验$\HH_0:\rho=0$. Heckit方法本质上也是CF方法, 因此当$\rho\neq0$时, Heckit方法得到的OLS估计量的标准误存在误差, 此时应该使用Bootstrap法对其调整.

为了应用CMLE, 还需要假定$[e_1,u]$服从联合正态分布
$$\begin{bmatrix}
    e \\
    u
  \end{bmatrix}\sim N\left(\begin{bmatrix}
                                    0 \\
                                    0
                                  \end{bmatrix},\begin{bmatrix}
                                                  \sigma^2 & \sigma_{21} \\
                                                  \sigma_{12} & 1
                                                \end{bmatrix}\right)$$
可以证明, 此时$(s,Y)$的联合概率密度为
$$f(s,y|X_i,Z_i)=[1-\Phi(Z_i'\gamma)]^{1-s}\left\{\Phi\left[\frac{\sigma^2Z_i'\gamma+\sigma_{21}(y-X_i'\beta)}{\sigma\sqrt{\sigma^2-\sigma_{21}}}\right]\frac{1}{\sigma}\phi\left(\frac{y-X_i'\beta}{\sigma}\right) \right\}^s$$
由此可求得对数似然以及ML估计量, 在一定正则条件下, 它是渐近有效的一致估计量.

如果在样本选择背景下还存在内生性问题, 假设此时的回归模型为
\begin{equation}\label{eq10.28}
  Y=X_1'\beta_1+\beta_2X_2+e
\end{equation}
其中$Y$只能在$s=1$时才能观测到, $X_1$为外生解释变量, $X_2$为内生解释变量, 此时仍可以考虑CF方法:
\begin{itemize}
  \item 实施$s_i$对外生解释变量、影响选择方程的排他性约束、以及$X_2$的工具变量的Probit回归, 计算得到IMR后代入到方程(\ref{eq10.28}).
  \item 对更新后的方程实施标准的2SLS回归.
\end{itemize}

\begin{remark}
为了使结果令人信服, 至少应该有两个排他性约束, 一个用于选择方程, 而另一个用于内生解释变量.
\end{remark}




\nocite{amemiya1973regression}
\nocite{mcdonald1980uses}
\nocite{tobin1958estimation}
\nocite{nickell1981biases}
\nocite{arellano1995another}
\nocite{blundell1998initial}
\nocite{anderson1982formulation}
\nocite{amemiya1985advanced}
\nocite{amemiya1986instrumental}
\nocite{arellano1987computing}
\nocite{arellano1991some}
%\nocite{andrews1991heteroskedasticity}
\nocite{angrist2009mostly}
\nocite{athreya2006measure}
%\nocite{billingsley1961lindeberg}
\nocite{davidson2020stochastic}
\nocite{duflo2011peer}
\nocite{durrett2019probability}
\nocite{fatas2001government}
\nocite{greene2017econometric}
\nocite{hansen2022modern}
\nocite{hansen2022econometrics}
\nocite{hansen2022probability}
\nocite{hansen1982large}
\nocite{hausman1978specification}
\nocite{hayashi2011econometrics}
\nocite{hausman1981panel}
%\nocite{holland1986statistics}
\nocite{hong2017probability}
\nocite{hong2020foundations}
\nocite{jennrich1969asymptotic}
\nocite{kinal1980existence}
\nocite{kleibergen2006generalized}
\nocite{klenke2013probability}
\nocite{cragg1971some}
%\nocite{newey1987simple}
%\nocite{newey1994large}
\nocite{pagan1984econometric}
\nocite{portnoy2022linearity}
%\nocite{rubin1974estimating}
\nocite{rudin1976principles}
\nocite{sargan1958estimation}
\nocite{staiger1994instrumental}
\nocite{stock2008heteroskedasticity}
\nocite{16284}
\nocite{white1980heteroskedasticity}
%\nocite{white1994estimation}
\nocite{wooldridge1995score}
\nocite{wooldridge2010econometric}
\nocite{wooldridge2015control}
\nocite{wooldridge2019introductory}
\nocite{zellner1962efficient}
\nocite{dube2020queens}
\nocite{white1982maximum}
\nocite{vuong1989likelihood}
\nocite{gourieroux1984pseudo}
%\nocite{gourieroux1995statistics}
\nocite{zehna1966invariance}
\nocite{newey1990semiparametric}
\nocite{newey1994large}
%\nocite{imbens1994identification}
\nocite{rivers1988limited}
\nocite{powell1984least}
\nocite{heckman1976common}
\nocite{heckman1979sample}
\printbibliography[heading=bibintoc, title=\ebibname]

\end{document}
